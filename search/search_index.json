{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u5173\u4e8e\u672c\u9879\u76ee \u672c\u9879\u76ee\u4e3b\u8981\u8bb0\u5f55hardware\u76f8\u5173\u77e5\u8bc6\uff0c\u4e3b\u8981\u5305\u62ec\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406\u76f8\u5173\u7684\u77e5\u8bc6\u3001CPU\u76f8\u5173\u7684\u77e5\u8bc6\u3002 \u7ae0\u8282 Computer-architecture \u4ecb\u7ecd\u7ecf\u5178\u7684\u3001\u7406\u8bba\u7684\u8ba1\u7b97\u673aarchitecture\uff0c\u6784\u5efa\u5b8c\u6574\u7684\u89c6\u89d2\u3002 \u7ae0\u8282 Memory \u4ecb\u7ecdmemory\u3002 \u7ae0\u8282 Processor(computing) \u4ecb\u7ecd\u5904\u7406\u5668\u7684\u6982\u5ff5\u3002 \u7ae0\u8282 CPU \u4ecb\u7ecd\u7ecf\u5178\u7684CPU\u3002 \u7ae0\u8282 Modern-CPU \u4ecb\u7ecd\u5f53\u4ee3CPU\u3002 \u7ae0\u8282 CPU-memory-access \u4ecb\u7ecdCPU access memory\u3002 \u7ae0\u8282 Assembly-language \u4ecb\u7ecd\u6c47\u7f16\u7684\u4e00\u4e9b\u5185\u5bb9\u3002","title":"Home"},{"location":"#_1","text":"\u672c\u9879\u76ee\u4e3b\u8981\u8bb0\u5f55hardware\u76f8\u5173\u77e5\u8bc6\uff0c\u4e3b\u8981\u5305\u62ec\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406\u76f8\u5173\u7684\u77e5\u8bc6\u3001CPU\u76f8\u5173\u7684\u77e5\u8bc6\u3002 \u7ae0\u8282 Computer-architecture \u4ecb\u7ecd\u7ecf\u5178\u7684\u3001\u7406\u8bba\u7684\u8ba1\u7b97\u673aarchitecture\uff0c\u6784\u5efa\u5b8c\u6574\u7684\u89c6\u89d2\u3002 \u7ae0\u8282 Memory \u4ecb\u7ecdmemory\u3002 \u7ae0\u8282 Processor(computing) \u4ecb\u7ecd\u5904\u7406\u5668\u7684\u6982\u5ff5\u3002 \u7ae0\u8282 CPU \u4ecb\u7ecd\u7ecf\u5178\u7684CPU\u3002 \u7ae0\u8282 Modern-CPU \u4ecb\u7ecd\u5f53\u4ee3CPU\u3002 \u7ae0\u8282 CPU-memory-access \u4ecb\u7ecdCPU access memory\u3002 \u7ae0\u8282 Assembly-language \u4ecb\u7ecd\u6c47\u7f16\u7684\u4e00\u4e9b\u5185\u5bb9\u3002","title":"\u5173\u4e8e\u672c\u9879\u76ee"},{"location":"Processor%28computing%29/","text":"Processor \"processor\"\u662fcomputer\u4e2d\u91cd\u8981\u7684\u7ec4\u6210\u90e8\u5206\u3002 wikipedia Processor (computing) In computing , a processor or processing unit is an electronic circuit which performs operations on some external data source, usually memory or some other data stream. The term is frequently used to refer to the central processor ( central processing unit ) in a system, but typical computer systems (especially SoCs ) combine a number of specialised \"processors\". Examples processor link \u6ce8\u89e3 CPU \u2013 central processing unit - If designed conforming to the von Neumann architecture , it contains at least a control unit (CU), arithmetic logic unit (ALU) and processor registers . - In some contexts, the ALU and registers are called the processing unit . \u5728 CPU \u7ae0\u8282\u8fdb\u884c\u4ecb\u7ecd GPU graphics processing unit \u65b0\u4e00\u4ee3\u7684 VPU vision processing unit TPU tensor processing unit NPU neural processing unit PPU physics processing unit DSP digital signal processor ISP image signal processor SPU or SPE synergistic processing element in the Cell microprocessor FPGA field-programmable gate array sound chip \u4e5f\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u58f0\u5361","title":"Processor(computing)"},{"location":"Processor%28computing%29/#processor","text":"\"processor\"\u662fcomputer\u4e2d\u91cd\u8981\u7684\u7ec4\u6210\u90e8\u5206\u3002","title":"Processor"},{"location":"Processor%28computing%29/#wikipedia#processor#computing","text":"In computing , a processor or processing unit is an electronic circuit which performs operations on some external data source, usually memory or some other data stream. The term is frequently used to refer to the central processor ( central processing unit ) in a system, but typical computer systems (especially SoCs ) combine a number of specialised \"processors\".","title":"wikipedia Processor (computing)"},{"location":"Processor%28computing%29/#examples","text":"processor link \u6ce8\u89e3 CPU \u2013 central processing unit - If designed conforming to the von Neumann architecture , it contains at least a control unit (CU), arithmetic logic unit (ALU) and processor registers . - In some contexts, the ALU and registers are called the processing unit . \u5728 CPU \u7ae0\u8282\u8fdb\u884c\u4ecb\u7ecd GPU graphics processing unit \u65b0\u4e00\u4ee3\u7684 VPU vision processing unit TPU tensor processing unit NPU neural processing unit PPU physics processing unit DSP digital signal processor ISP image signal processor SPU or SPE synergistic processing element in the Cell microprocessor FPGA field-programmable gate array sound chip \u4e5f\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u7684\u58f0\u5361","title":"Examples"},{"location":"Assembly-language/","text":"Assembly language \u9762\u5411hardware\u7684\u8bed\u8a00\u3002 wikipedia Assembly language","title":"Assembly-language"},{"location":"Assembly-language/#assembly#language","text":"\u9762\u5411hardware\u7684\u8bed\u8a00\u3002","title":"Assembly language"},{"location":"Assembly-language/#wikipedia#assembly#language","text":"","title":"wikipedia Assembly language"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/","text":"1.2 \u8ba1\u7b97\u673a\u53d1\u5c55\u7b80\u53f2 \u53c2\u89c1\uff1a Computer 1.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807 \u541e\u5410\u91cf \u54cd\u5e94\u65f6\u95f4 \u5229\u7528\u7387 \u5904\u7406\u673a\u5b57\u957f \u6307\u5904\u7406\u673a**\u8fd0\u7b97\u5668**\uff08ALU\uff09\u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684**\u4f4d\u6570**\uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b NOTE : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f \u603b\u7ebf\u5bbd\u5ea6 \u4e00\u822c\u6307CPU\u4e2d**\u8fd0\u7b97\u5668**\u4e0e**\u5b58\u50a8\u5668 **\u4e4b\u95f4\u8fdb\u884c\u4e92\u8054\u7684\u5185\u90e8\u603b\u7ebf\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002 \u4e3b\u9891/\u65f6\u949f\u5468\u671f CPU\u7684\u5de5\u4f5c\u8282\u62cd\u53d7**\u4e3b\u65f6\u949f**\u7684\u63a7\u5236\uff0c \u4e3b\u65f6\u949f**\u4e0d\u65ad\u4ea7\u751f\u56fa\u5b9a\u9891\u7387\u7684\u65f6\u949f\uff0c**\u4e3b\u65f6\u949f\u9891\u7387 \uff08 f \uff09\u53eb**CPU\u4e3b\u9891**\u3002\u5ea6\u91cf\u5355\u4f4d\u662f MHz , GHz NOTE : \u4e3b\u9891\u7684\u542b\u4e49\u5c31\u662fCPU\u4e3b\u65f6\u949f\u7684\u9891\u7387\uff1b \u9891\u7387\u7684\u542b\u4e49\u662f\u4e00\u79d2\u949f\u6267\u884c\u591a\u5c11\u6b21\uff1b \u4e3b\u9891\u7684\u5012\u6570\u79f0\u4e3a**CPU\u65f6\u949f\u5468\u671f**\uff08 T \uff09\uff0c T=1/f \uff0c\u5ea6\u91cf\u5355\u4f4d\u662f us \uff0c ns \u3002 NOTE : **CPU\u65f6\u949f\u5468\u671f**\u5728\u300a5.2.1 \u6307\u4ee4\u5468\u671f\u7684\u57fa\u672c\u6982\u5ff5\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b\u4ece\u4e0a\u8ff0\u5173\u7cfb\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u4e3b\u9891\u548cCPU\u65f6\u949f\u5468\u671f\u5bc6\u5207\u76f8\u5173\uff1b\u4e3b\u65f6\u949f\u5219\u5728\u300a5.3.2 \u65f6\u5e8f\u4fe1\u53f7\u4ea7\u751f\u5668\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b CPU\u6267\u884c\u65f6\u95f4 \u8868\u793aCPU\u6267\u884c\u4e00\u822c\u7a0b\u5e8f\u6240\u5360\u7528\u7684CPU\u65f6\u95f4\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u516c\u5f0f\u8ba1\u7b97\uff1a CPU\u6267\u884c\u65f6\u95f4 = CPU\u65f6\u949f\u5468\u671f\u6570 * CPU\u65f6\u949f\u5468\u671f CPI \u8868\u793a\u6bcf\u6761\u6307\u4ee4\u5468\u671f\u6570\uff0c\u5373\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u7684\u5e73\u5747\u65f6\u949f\u5468\u671f\u6570\uff0c\u7528\u4e0b\u9762\u7684\u516c\u5f0f\u8ba1\u7b97\uff1a CPI = \u6267\u884c\u67d0\u6bb5\u7a0b\u5e8f\u6240\u9700\u7684CPU\u65f6\u949f\u5468\u671f\u6570 / \u7a0b\u5e8f\u5305\u542b\u7684\u6307\u4ee4\u6761\u6570 FLOPS \u8868\u793a\u6bcf\u79d2\u6267\u884c\u6d6e\u70b9\u64cd\u4f5c\u7684\u6b21\u6570\uff0c\u7528\u6765\u8861\u91cf\u673a\u5668\u6d6e\u70b9\u64cd\u4f5c\u7684\u6027\u80fd\u3002\u7528\u4e0b\u5f0f\u8ba1\u7b97\uff1a FLOPS = \u7a0b\u5e8f\u4e2d\u6d6e\u70b9\u64cd\u4f5c\u6b21\u6570 / \u7a0b\u5e8f\u6267\u884c\u65f6\u95f4","title":"1.2-\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\u7b80\u53f2"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#12","text":"\u53c2\u89c1\uff1a Computer","title":"1.2 \u8ba1\u7b97\u673a\u53d1\u5c55\u7b80\u53f2"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#124","text":"","title":"1.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#_1","text":"","title":"\u541e\u5410\u91cf"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#_2","text":"","title":"\u54cd\u5e94\u65f6\u95f4"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#_3","text":"","title":"\u5229\u7528\u7387"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#_4","text":"\u6307\u5904\u7406\u673a**\u8fd0\u7b97\u5668**\uff08ALU\uff09\u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684**\u4f4d\u6570**\uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b NOTE : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f","title":"\u5904\u7406\u673a\u5b57\u957f"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#_5","text":"\u4e00\u822c\u6307CPU\u4e2d**\u8fd0\u7b97\u5668**\u4e0e**\u5b58\u50a8\u5668 **\u4e4b\u95f4\u8fdb\u884c\u4e92\u8054\u7684\u5185\u90e8\u603b\u7ebf\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002","title":"\u603b\u7ebf\u5bbd\u5ea6"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#_6","text":"CPU\u7684\u5de5\u4f5c\u8282\u62cd\u53d7**\u4e3b\u65f6\u949f**\u7684\u63a7\u5236\uff0c \u4e3b\u65f6\u949f**\u4e0d\u65ad\u4ea7\u751f\u56fa\u5b9a\u9891\u7387\u7684\u65f6\u949f\uff0c**\u4e3b\u65f6\u949f\u9891\u7387 \uff08 f \uff09\u53eb**CPU\u4e3b\u9891**\u3002\u5ea6\u91cf\u5355\u4f4d\u662f MHz , GHz NOTE : \u4e3b\u9891\u7684\u542b\u4e49\u5c31\u662fCPU\u4e3b\u65f6\u949f\u7684\u9891\u7387\uff1b \u9891\u7387\u7684\u542b\u4e49\u662f\u4e00\u79d2\u949f\u6267\u884c\u591a\u5c11\u6b21\uff1b \u4e3b\u9891\u7684\u5012\u6570\u79f0\u4e3a**CPU\u65f6\u949f\u5468\u671f**\uff08 T \uff09\uff0c T=1/f \uff0c\u5ea6\u91cf\u5355\u4f4d\u662f us \uff0c ns \u3002 NOTE : **CPU\u65f6\u949f\u5468\u671f**\u5728\u300a5.2.1 \u6307\u4ee4\u5468\u671f\u7684\u57fa\u672c\u6982\u5ff5\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b\u4ece\u4e0a\u8ff0\u5173\u7cfb\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u4e3b\u9891\u548cCPU\u65f6\u949f\u5468\u671f\u5bc6\u5207\u76f8\u5173\uff1b\u4e3b\u65f6\u949f\u5219\u5728\u300a5.3.2 \u65f6\u5e8f\u4fe1\u53f7\u4ea7\u751f\u5668\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b","title":"\u4e3b\u9891/\u65f6\u949f\u5468\u671f"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#cpu","text":"\u8868\u793aCPU\u6267\u884c\u4e00\u822c\u7a0b\u5e8f\u6240\u5360\u7528\u7684CPU\u65f6\u95f4\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u516c\u5f0f\u8ba1\u7b97\uff1a CPU\u6267\u884c\u65f6\u95f4 = CPU\u65f6\u949f\u5468\u671f\u6570 * CPU\u65f6\u949f\u5468\u671f","title":"CPU\u6267\u884c\u65f6\u95f4"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#cpi","text":"\u8868\u793a\u6bcf\u6761\u6307\u4ee4\u5468\u671f\u6570\uff0c\u5373\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u7684\u5e73\u5747\u65f6\u949f\u5468\u671f\u6570\uff0c\u7528\u4e0b\u9762\u7684\u516c\u5f0f\u8ba1\u7b97\uff1a CPI = \u6267\u884c\u67d0\u6bb5\u7a0b\u5e8f\u6240\u9700\u7684CPU\u65f6\u949f\u5468\u671f\u6570 / \u7a0b\u5e8f\u5305\u542b\u7684\u6307\u4ee4\u6761\u6570","title":"CPI"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2/#flops","text":"\u8868\u793a\u6bcf\u79d2\u6267\u884c\u6d6e\u70b9\u64cd\u4f5c\u7684\u6b21\u6570\uff0c\u7528\u6765\u8861\u91cf\u673a\u5668\u6d6e\u70b9\u64cd\u4f5c\u7684\u6027\u80fd\u3002\u7528\u4e0b\u5f0f\u8ba1\u7b97\uff1a FLOPS = \u7a0b\u5e8f\u4e2d\u6d6e\u70b9\u64cd\u4f5c\u6b21\u6570 / \u7a0b\u5e8f\u6267\u884c\u65f6\u95f4","title":"FLOPS"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/","text":"1.2 \u8ba1\u7b97\u673a\u53d1\u5c55\u7b80\u53f2 \u53c2\u89c1\uff1a Computer 1.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807 \u541e\u5410\u91cf \u54cd\u5e94\u65f6\u95f4 \u5229\u7528\u7387 \u5904\u7406\u673a\u5b57\u957f \u6307\u5904\u7406\u673a**\u8fd0\u7b97\u5668**\u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684**\u4f4d\u6570**\uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b SUMMARY : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right SUMMARY : \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f \u603b\u7ebf\u5bbd\u5ea6 \u4e00\u822c\u6307CPU\u4e2d**\u8fd0\u7b97\u5668**\u4e0e**\u5b58\u50a8\u5668 **\u4e4b\u95f4\u8fdb\u884c\u4e92\u8054\u7684\u5185\u90e8\u603b\u7ebf\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002 \u4e3b\u9891/\u65f6\u949f\u5468\u671f CPU\u7684\u5de5\u4f5c\u8282\u62cd\u53d7**\u4e3b\u65f6\u949f**\u7684\u63a7\u5236\uff0c \u4e3b\u65f6\u949f**\u4e0d\u65ad\u4ea7\u751f\u56fa\u5b9a\u9891\u7387\u7684\u65f6\u949f\uff0c**\u4e3b\u65f6\u949f\u9891\u7387 \uff08 f \uff09\u53eb**CPU\u4e3b\u9891**\u3002\u5ea6\u91cf\u5355\u4f4d\u662f MHz , GHz SUMMARY : \u4e3b\u9891\u7684\u542b\u4e49\u5c31\u662fCPU\u4e3b\u65f6\u949f\u7684\u9891\u7387\uff1b SUMMARY : \u9891\u7387\u7684\u542b\u4e49\u662f\u4e00\u79d2\u949f\u6267\u884c\u591a\u5c11\u6b21\uff1b \u4e3b\u9891\u7684\u5012\u6570\u79f0\u4e3a**CPU\u65f6\u949f\u5468\u671f**\uff08 T \uff09\uff0c T=1/f \uff0c\u5ea6\u91cf\u5355\u4f4d\u662f us \uff0c ns \u3002 SUMMARY : **CPU\u65f6\u949f\u5468\u671f**\u5728\u300a5.2.1 \u6307\u4ee4\u5468\u671f\u7684\u57fa\u672c\u6982\u5ff5\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b\u4ece\u4e0a\u8ff0\u5173\u7cfb\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u4e3b\u9891\u548cCPU\u65f6\u949f\u5468\u671f\u5bc6\u5207\u76f8\u5173\uff1b\u4e3b\u65f6\u949f\u5219\u5728\u300a5.3.2 \u65f6\u5e8f\u4fe1\u53f7\u4ea7\u751f\u5668\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b SUMMARY : \u53c2\u89c1 Memory Alignment CPU\u6267\u884c\u65f6\u95f4 \u8868\u793aCPU\u6267\u884c\u4e00\u822c\u7a0b\u5e8f\u6240\u5360\u7528\u7684CPU\u65f6\u95f4\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u516c\u5f0f\u8ba1\u7b97\uff1a CPU\u6267\u884c\u65f6\u95f4 = CPU\u65f6\u949f\u5468\u671f\u6570 * CPU\u65f6\u949f\u5468\u671f CPI \u8868\u793a\u6bcf\u6761\u6307\u4ee4\u5468\u671f\u6570\uff0c\u5373\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u7684\u5e73\u5747\u65f6\u949f\u5468\u671f\u6570\uff0c\u7528\u4e0b\u9762\u7684\u516c\u5f0f\u8ba1\u7b97\uff1a CPI = \u6267\u884c\u67d0\u6bb5\u7a0b\u5e8f\u6240\u9700\u7684CPU\u65f6\u949f\u5468\u671f\u6570 / \u7a0b\u5e8f\u5305\u542b\u7684\u6307\u4ee4\u6761\u6570 FLOPS \u8868\u793a\u6bcf\u79d2\u6267\u884c\u6d6e\u70b9\u64cd\u4f5c\u7684\u6b21\u6570\uff0c\u7528\u6765\u8861\u91cf\u673a\u5668\u6d6e\u70b9\u64cd\u4f5c\u7684\u6027\u80fd\u3002\u7528\u4e0b\u5f0f\u8ba1\u7b97\uff1a FLOPS = \u7a0b\u5e8f\u4e2d\u6d6e\u70b9\u64cd\u4f5c\u6b21\u6570 / \u7a0b\u5e8f\u6267\u884c\u65f6\u95f4","title":"1.2-\u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#12","text":"\u53c2\u89c1\uff1a Computer","title":"1.2 \u8ba1\u7b97\u673a\u53d1\u5c55\u7b80\u53f2"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#124","text":"","title":"1.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#_1","text":"","title":"\u541e\u5410\u91cf"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#_2","text":"","title":"\u54cd\u5e94\u65f6\u95f4"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#_3","text":"","title":"\u5229\u7528\u7387"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#_4","text":"\u6307\u5904\u7406\u673a**\u8fd0\u7b97\u5668**\u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684**\u4f4d\u6570**\uff0c\u598232\u4f4d\uff0c64\u4f4d\uff1b SUMMARY : \u8fd9\u5e94\u8be5\u5c31\u662f\u6211\u4eec\u5e73\u65f6\u6240\u8bf4\u768432\u4f4d\uff0c\u621664\u4f4d\uff1b\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u7684\u8fd0\u7b97\uff0c\u5176\u5b9e\u8574\u542b\u4e2d\uff0cCPU\u4e00\u6b21CPU\u4e00\u6b21\u6027\u80fd\u8bfb\u53d6\u6570\u636e\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002\u53c2\u89c1 Redis\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u77f3zmallc.c\u6e90\u7801\u89e3\u8bfb\uff08\u4e00\uff09 \uff1b Data alignment: Straighten up and fly right SUMMARY : \u4e0a\u8ff0\u4e00\u6b21\u7684\u542b\u4e49\u662f\u4ec0\u4e48\uff1f\u662f\u6307\u4e00\u4e2a\u6307\u4ee4\u5468\u671f\uff1f","title":"\u5904\u7406\u673a\u5b57\u957f"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#_5","text":"\u4e00\u822c\u6307CPU\u4e2d**\u8fd0\u7b97\u5668**\u4e0e**\u5b58\u50a8\u5668 **\u4e4b\u95f4\u8fdb\u884c\u4e92\u8054\u7684\u5185\u90e8\u603b\u7ebf\u4e8c\u8fdb\u5236\u4f4d\u6570\u3002","title":"\u603b\u7ebf\u5bbd\u5ea6"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#_6","text":"CPU\u7684\u5de5\u4f5c\u8282\u62cd\u53d7**\u4e3b\u65f6\u949f**\u7684\u63a7\u5236\uff0c \u4e3b\u65f6\u949f**\u4e0d\u65ad\u4ea7\u751f\u56fa\u5b9a\u9891\u7387\u7684\u65f6\u949f\uff0c**\u4e3b\u65f6\u949f\u9891\u7387 \uff08 f \uff09\u53eb**CPU\u4e3b\u9891**\u3002\u5ea6\u91cf\u5355\u4f4d\u662f MHz , GHz SUMMARY : \u4e3b\u9891\u7684\u542b\u4e49\u5c31\u662fCPU\u4e3b\u65f6\u949f\u7684\u9891\u7387\uff1b SUMMARY : \u9891\u7387\u7684\u542b\u4e49\u662f\u4e00\u79d2\u949f\u6267\u884c\u591a\u5c11\u6b21\uff1b \u4e3b\u9891\u7684\u5012\u6570\u79f0\u4e3a**CPU\u65f6\u949f\u5468\u671f**\uff08 T \uff09\uff0c T=1/f \uff0c\u5ea6\u91cf\u5355\u4f4d\u662f us \uff0c ns \u3002 SUMMARY : **CPU\u65f6\u949f\u5468\u671f**\u5728\u300a5.2.1 \u6307\u4ee4\u5468\u671f\u7684\u57fa\u672c\u6982\u5ff5\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b\u4ece\u4e0a\u8ff0\u5173\u7cfb\u53ef\u4ee5\u770b\u51fa\uff0cCPU\u4e3b\u9891\u548cCPU\u65f6\u949f\u5468\u671f\u5bc6\u5207\u76f8\u5173\uff1b\u4e3b\u65f6\u949f\u5219\u5728\u300a5.3.2 \u65f6\u5e8f\u4fe1\u53f7\u4ea7\u751f\u5668\u300b\u4e2d\u6709\u4ecb\u7ecd\uff1b SUMMARY : \u53c2\u89c1 Memory Alignment","title":"\u4e3b\u9891/\u65f6\u949f\u5468\u671f"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#cpu","text":"\u8868\u793aCPU\u6267\u884c\u4e00\u822c\u7a0b\u5e8f\u6240\u5360\u7528\u7684CPU\u65f6\u95f4\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u516c\u5f0f\u8ba1\u7b97\uff1a CPU\u6267\u884c\u65f6\u95f4 = CPU\u65f6\u949f\u5468\u671f\u6570 * CPU\u65f6\u949f\u5468\u671f","title":"CPU\u6267\u884c\u65f6\u95f4"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#cpi","text":"\u8868\u793a\u6bcf\u6761\u6307\u4ee4\u5468\u671f\u6570\uff0c\u5373\u6267\u884c\u4e00\u6761\u6307\u4ee4\u6240\u9700\u7684\u5e73\u5747\u65f6\u949f\u5468\u671f\u6570\uff0c\u7528\u4e0b\u9762\u7684\u516c\u5f0f\u8ba1\u7b97\uff1a CPI = \u6267\u884c\u67d0\u6bb5\u7a0b\u5e8f\u6240\u9700\u7684CPU\u65f6\u949f\u5468\u671f\u6570 / \u7a0b\u5e8f\u5305\u542b\u7684\u6307\u4ee4\u6761\u6570","title":"CPI"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.2-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/#flops","text":"\u8868\u793a\u6bcf\u79d2\u6267\u884c\u6d6e\u70b9\u64cd\u4f5c\u7684\u6b21\u6570\uff0c\u7528\u6765\u8861\u91cf\u673a\u5668\u6d6e\u70b9\u64cd\u4f5c\u7684\u6027\u80fd\u3002\u7528\u4e0b\u5f0f\u8ba1\u7b97\uff1a FLOPS = \u7a0b\u5e8f\u4e2d\u6d6e\u70b9\u64cd\u4f5c\u6b21\u6570 / \u7a0b\u5e8f\u6267\u884c\u65f6\u95f4","title":"FLOPS"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/","text":"1.3\u8ba1\u7b97\u673a\u7684\u786c\u4ef6 1.3.1 \u786c\u4ef6\u7ec4\u6210\u8981\u7d20 \u4f7f\u7528\u6253\u7b97\u76d8\u6765\u8bf4\u660e\u8ba1\u7b97\u673a\u7684\u4e3b\u8981\u7ec4\u6210\u548c\u5de5\u4f5c\u539f\u7406\u3002\u7ed9\u5b9a\u4e00\u4e2a\u7b97\u76d8\u3001\u4e00\u5f20\u5e26\u6709\u6a2a\u683c\u7684\u7eb8\u3001\u4e00\u652f\u7b14\uff0c\u8981\u6c42\u8ba1\u7b97y=ax+b-c\u3002\u4e0b\u9762\u5c06\u89e3\u9898\u6b65\u9aa4\u8bb0\u5f55\u5728\u5e26\u6709\u6a2a\u683c\u7684\u7eb8\u5e26\u4e0a\uff1a \u88681.3\u89e3\u9898\u6b65\u9aa4\u548c\u6570\u636e\u8bb0\u5f55\u5728\u6a2a\u683c\u7eb8\u4e0a \u884c\u6570 \u89e3\u9898\u6b65\u9aa4\u548c\u6570\u636e \u8bf4\u660e 1 \u53d6\u6570 \uff089\uff09->\u7b97\u76d8 \uff089\uff09\u8868\u793a\u7b2c9\u884c\u7684\u6570a\uff0c\u4e0b\u540c 2 \u4e58\u6cd5 \uff0812\uff09->\u7b97\u76d8 \u5b8c\u6210 a*x \uff0c\u7ed3\u679c\u5728\u7b97\u76d8\u4e0a 3 \u52a0\u6cd5 \uff0810\uff09->\u7b97\u76d8 \u5b8c\u6210 a*x+b \uff0c\u7ed3\u679c\u5728\u7b97\u76d8\u4e0a 4 \u51cf\u6cd5 \uff0811\uff09->\u7b97\u76d8 \u5b8c\u6210 y=a*x+b-c \uff0c\u7ed3\u679c\u5728\u7b97\u76d8\u4e0a 5 \u5b58\u6570 y->13 \u7b97\u76d8\u4e0a\u7684y\u503c\u8bb0\u523013\u884c 6 \u8f93\u51fa \u5c06\u7b97\u76d8\u4e0a\u7684y\u503c\u5199\u51fa\u6765\u7ed9\u4eba\u770b 7 \u505c\u6b62 8 9 a \u6570\u636e 10 b \u6570\u636e 11 c \u6570\u636e 12 x \u6570\u636e 13 y \u6570\u636e \u5728\u7535\u5b50\u8ba1\u7b97\u673a\u4e2d\uff1a **\u8fd0\u7b97\u5668**\u76f8\u5f53\u4e8e\u7b97\u76d8 **\u5b58\u50a8\u5668**\u76f8\u5f53\u4e8e\u7eb8 **\u63a7\u5236\u5668**\u76f8\u5f53\u4e8e\u4eba\u7684\u5927\u8111\uff08\u63a7\u5236\u7740\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\uff09 |<--> \u5b58\u50a8\u5668 | \u7cfb |<--> \u8fd0\u7b97\u5668 \u7edf | \u603b |<--> \u63a7\u5236\u5668 \u7ebf | |<--> \u9002\u914d\u5668 | | | | \u8f93\u5165\u8bbe\u5907 \u8f93\u51fa\u8bbe\u5907 CPU = \u8fd0\u7b97\u5668 + \u63a7\u5236\u5668 1.3.2 \u8fd0\u7b97\u5668 \u53c2\u89c1\uff1a ALU \u5b57\u957f\uff1a\u8fd0\u7b97\u5668\u7684\u957f\u5ea6 1.3.3 \u5b58\u50a8\u5668 \u53c2\u89c1\uff1a Computer memory 1.3.4 \u63a7\u5236\u5668 \u53c2\u89c1\uff1a Control unit \u63a7\u5236\u5668\u7684\u4efb\u52a1\u662f\u4ece\u5185\u5b58\u4e2d\u53d6\u51fa\u89e3\u9898\u6b65\u9aa4\u52a0\u4ee5\u5206\u6790\uff0c\u7136\u540e\u6267\u884c\u67d0\u79cd\u64cd\u4f5c\uff1b NOTE: \u8bfb\u53d6\u6307\u4ee4\u548c\u8bfb\u53d6\u6570\u636e\u90fd\u662f\u7531\u63a7\u5236\u5668\u6765\u5b8c\u6210\u7684\uff1b\u6240\u4ee5\u63a7\u5236\u5668\u9700\u8981\u4e86\u89e3\u6307\u4ee4\u7684\u683c\u5f0f 1. \u8ba1\u7b97\u7a0b\u5e8f \u88681.4\u8ba1\u7b97y=ax+b-c\u7684\u7a0b\u5e8f \u6307\u4ee4\u5730\u5740 \u64cd\u4f5c\u7801 \u5730\u5740\u7801 \u6307\u4ee4\u64cd\u4f5c\u5185\u5bb9 \u8bf4\u660e 1 \u53d6\u6570 9 (9) -> A \u5c06\u5b58\u50a8\u56689\u53f7\u5730\u5740\u7684\u6570\u8bfb\u5165\u5230\u8fd0\u7b97\u5668A 2 \u4e58\u6cd5 12 (A) * (12) -> A \u5b8c\u6210 a*x \uff0c\u7ed3\u679c\u4fdd\u7559\u7740\u8fd0\u7b97\u5668A 3 \u52a0\u6cd5 10 (A) + (10) -> A \u5b8c\u6210 a*x+b \uff0c\u7ed3\u679c\u4fdd\u7559\u7740\u8fd0\u7b97\u5668A 4 \u51cf\u6cd5 11 (A) - (11) -> A \u5b8c\u6210 a*x+b-c \uff0c\u7ed3\u679c\u4fdd\u7559\u7740\u8fd0\u7b97\u5668A 5 \u5b58\u6570 13 A->13 \u8fd0\u7b97\u5668A\u4e2d\u7684\u7ed3\u679cy\u9001\u5165\u5230\u5b58\u50a8\u566813\u53f7\u5730\u5740 6 \u6253\u5370 7 \u505c\u6b62 8 \u6570\u636e\u5730\u5740 \u6570\u636e \u8bf4\u660e 9 a 10 b 11 c 12 x 13 y \u88681.4\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u4f8b\u5b50\uff0c\u8fd9\u4e2a\u8868\u4e2d\u5c55\u793a\u4e86**\u6307\u4ee4**\uff0c \u6570\u636e \uff0cCPU\u7684\u8fd0\u7b97\u903b\u8f91\u7b49\u5173\u952e\u5185\u5bb9\uff1b NOTE : \u4ece\u88681.4\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u8fd0\u7b97\u5668\u7684\u64cd\u4f5c\u6570\u9700\u8981\u901a\u8fc7**\u53d6\u6570\u6307\u4ee4**\u4ece\u5185\u5b58\u4e2d\u52a0\u8f7d\u5230**\u8fd0\u7b97\u5668**\u4e2d\uff1b\u88681.4\u8fd8\u5c55\u793a\u5904\u7406\uff0c\u8fd0\u7b97\u5668\u5728\u6267\u884c\u8fd0\u7b97\u6307\u4ee4\uff0c\u5982**\u4e58\u6cd5\u6307\u4ee4**\uff0c**\u52a0\u6cd5\u6307\u4ee4**\u7b49\u7684\u65f6\u5019\uff0c\u4ea7\u751f\u7684\u8fd0\u7b97\u7ed3\u679c\u662f\u4fdd\u7559\u5728\u8fd0\u7b97\u5668\u4e2d\uff0c\u9700\u8981\u6267\u884c**\u5b58\u6570\u6307\u4ee4**\u624d\u80fd\u591f\u5c06**\u8fd0\u7b97\u5668**\u4e2d\u7684\u6570\u636e\u5199\u5165\u5230\u5185\u5b58\u4e2d\uff1b\u8fd9\u5176\u5b9e\u975e\u5e38\u662f\u7b26\u5408 Load\u2013store architecture \u7684\u3002 2. \u6307\u4ee4\u7684\u5f62\u5f0f \u6bcf\u6761\u6307\u4ee4\u9700\u8981\u660e\u786e\u5730\u544a\u8bc9\u63a7\u5236\u5668\uff0c\u4ece\u5b58\u50a8\u5668\u7684\u54ea\u4e2a\u5355\u5143\u53d6\u6570\uff0c\u5e76\u8fdb\u884c\u4f55\u79cd\u64cd\u4f5c\uff1b\u8fd9\u6837\u4e00\u6765\uff0c\u53ef\u77e5\u6307\u4ee4\u7684\u5185\u5bb9\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1a\u64cd\u4f5c\u7684\u6027\u8d28\u548c\u64cd\u4f5c\u6570\u7684\u5730\u5740\uff1b\u524d\u8005\u4e3a**\u64cd\u4f5c\u7801**\uff0c\u540e\u8005\u4e3a**\u5730\u5740\u7801**\uff1b \u64cd\u4f5c\u7801|\u5730\u5740\u7801 \u6307\u4ee4\u7684**\u64cd\u4f5c\u7801**\u548c**\u5730\u5740\u7801**\u90fd\u53ef\u4ee5\u4f7f\u7528**\u4e8c\u8fdb\u5236\u4ee3\u7801**\u6765\u8fdb\u884c\u8868\u793a\uff0c\u8fd9\u5c31\u662f**\u6307\u4ee4\u7684\u6570\u7801\u5316**\uff1b\u6307\u4ee4**\u6570\u7801\u5316**\u4ee5\u540e\uff0c\u5c31\u770b\u53ef\u4ee5\u548c\u6570\u636e\u4e00\u6837\u5b58\u5165\u5230**\u5b58\u50a8\u5668**\u4e2d( Stored-program computer )\uff1b\u5b58\u50a8\u5668\u7684\u4efb\u4f55\u4f4d\u7f6e\u65e2\u53ef\u4ee5\u5b58\u653e\u6570\u636e\u4e5f\u53ef\u4ee5\u5b58\u653e\u6307\u4ee4\uff0c\u4e0d\u8fc7\u4e00\u822c\u5c06\u6307\u4ee4\u548c\u6570\u636e\u5206\u5f00\u5b58\u653e\u3002 \u63a7\u5236\u5668**\u4f9d\u636e\u5b58\u50a8\u7684\u7a0b\u5e8f\u6765\u63a7\u5236\u5168\u673a\u534f\u8c03\u5730\u5b8c\u6210\u8ba1\u7b97\u4efb\u52a1\u53eb\u505a**\u7a0b\u5e8f\u63a7\u5236 \u3002 \u6309\u7167 Von Neumann architecture \u7684\u601d\u60f3\uff0c\u6307\u4ee4\u548c\u6570\u636e\uff08\u5982\u679c\u4e0d\u6e05\u695a\u4e24\u8005\u7684\u5dee\u522b\uff0c\u53bb\u770b\u88681-4\uff09\u5b58\u653e\u5728\u540c\u4e00\u4e2a\u5b58\u50a8\u5668\u4e2d\uff0c**\u63a7\u5236\u5668**\u6309\u7167stored program\u7684**\u5730\u5740\u987a\u5e8f**\u8fdb\u884c\u6267\u884c\uff1b \u6309\u7167 Harvard architecture \u7684\u601d\u60f3\uff0c\u5c06\u6307\u4ee4\u548c\u6570\u636e\u5206\u522b\u5b58\u653e\u5728\u4e24\u4e2a\u5b58\u50a8\u5668\u4e2d\uff1b \u4e00\u53f0\u8ba1\u7b97\u673a\u901a\u5e38\u6709\u51e0\u5341\u79cd\u57fa\u672c**\u6307\u4ee4**\uff0c\u4ece\u800c\u6784\u6210\u8be5\u8ba1\u7b97\u673a\u7684**\u6307\u4ee4\u7cfb\u7edf**\u3002**\u6307\u4ee4\u7cfb\u7edf**\u4e0d\u4ec5\u662f**\u786c\u4ef6\u8bbe\u8ba1**\u7684\u4f9d\u636e\uff0c\u800c\u4e14\u662f**\u8f6f\u4ef6\u8bbe\u8ba1**\u7684\u57fa\u7840\u3002\u56e0\u6b64**\u6307\u4ee4\u7cfb\u7edf**\u662f\u8861\u91cf**\u8ba1\u7b97\u673a\u6027\u80fd**\u7684\u4e00\u4e2a\u91cd\u8981\u6307\u6807\u3002 3. \u63a7\u5236\u5668\u7684\u57fa\u672c\u4efb\u52a1 \u7531\u88681-4\u53ef\u77e5\uff0c\u8ba1\u7b97\u673a\u5728\u8fdb\u884c\u8ba1\u7b97\u7684\u65f6\u5019\uff0c**\u6307\u4ee4**\u5fc5\u987b\u662f\u6309\u7167\u4e00\u5b9a\u7684**\u987a\u5e8f**\u4e00\u6761\u63a5\u7740\u4e00\u6761\u7684\u8fdb\u884c\uff1b \u63a7\u5236\u5668**\u7684\u57fa\u672c\u4efb\u52a1\u5c31\u662f\u6309\u7167\u8ba1\u7b97\u673a\u7a0b\u5e8f\u6240\u6392\u7684\u6307\u4ee4\u5e8f\u5217\uff0c\u5148\u4ece**\u5b58\u50a8\u5668**\u4e2d\u53d6\u51fa\u4e00\u6761\u6307\u4ee4\u653e\u5230**\u63a7\u5236\u5668**\u4e2d\uff0c\u5bf9\u8be5**\u6307\u4ee4**\u7684**\u64cd\u4f5c\u7801**\u7531**\u8bd1\u7801\u5668**\u8fdb\u884c\u5206\u6790\u5224\u65ad\uff0c\u7136\u540e\u6839\u636e**\u6307\u4ee4\u6027\u8d28 \uff08\u5373\u8be5\u6307\u4ee4\u8981\u505a\u4ec0\u4e48\u8fd0\u7b97\uff09\uff0c\u6267\u884c\u8fd9\u6761\u6307\u4ee4\uff1b\u63a5\u7740\u4ece\u5b58\u50a8\u5668\u4e2d\u53d6\u51fa\u7b2c\u4e8c\u6761\u6307\u4ee4\uff0c\u518d\u6267\u884c\u7b2c\u4e8c\u6761\u6307\u4ee4\u3002\u4ee5\u6b64\u7c7b\u63a8\u3002 \u901a\u5e38\uff0c\u5c06\u53d6\u6307\u4ee4\u7684\u8fd9\u6bb5\u65f6\u95f4\u53eb\u505a**\u53d6\u6307\u5468\u671f**\uff0c\u800c\u628a\u6267\u884c\u6307\u4ee4\u7684\u4e00\u6bb5\u65f6\u95f4\u53eb\u505a**\u6267\u884c\u5468\u671f**\u3002\u56e0\u6b64\u63a7\u5236\u5668\u53cd\u590d\u4ea4\u66ff\u5730\u5904\u4e8e\u53d6\u6307\u5468\u671f\u548c\u6267\u884c\u5468\u671f\u4e4b\u4e2d\uff1b \u6bcf\u53d6\u51fa\u4e00\u6761\u6307\u4ee4\uff0c\u63a7\u5236\u5668\u4e2d\u7684**\u6307\u4ee4\u8ba1\u6570\u5668**\u5c31\u52a01\uff0c\u4ece\u800c\u4e3a\u4e0b\u4e00\u6761\u6307\u4ee4\u505a\u597d\u51c6\u5907\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u6307\u4ee4\u5728\u5b58\u50a8\u5668\u4e2d\u662f\u987a\u5e8f\u5b58\u653e\u7684\u539f\u56e0\uff1b 4. \u6307\u4ee4\u6d41\u548c\u6570\u636e\u6d41 \u6211\u4eec\u4f7f\u7528bit\u6765\u4f5c\u4e3a\u8ba1\u7b97\u673a\u7684\u6700\u5c0f\u4fe1\u606f\u5355\u4f4d\u3002 \u5f53CPU\u5411\u5b58\u50a8\u5668\u9001\u5165\u6216\u8005\u4ece\u5b58\u50a8\u5668\u4e2d\u53d6\u51fa\u4fe1\u606f\u65f6\uff0c\u5f80\u5f80\u662f\u5b58\u53d6byte\uff08\u5b57\u8282\uff09\u548cWord\uff08\u5b57\uff09\u7b49\u8f83\u5927\u4fe1\u606f\u5355\u4f4d\uff0c\u800c\u4e0d\u662fbit\uff1b\u901a\u5e38\u5c06\u7ec4\u6210\u4e00\u4e2aWord\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u53eb\u505a**\u5b57\u957f**\uff1b NOTE: \u8fd9\u5e94\u8be5\u662f\u6211\u4eec\u9700\u8981\u8fdb\u884c\u5730\u5740\u5bf9\u9f50\u7684\u539f\u56e0\u6240\u5728\uff1b NOTE: \u8fd9\u90e8\u5206\u5185\u5bb9\u57284.2.3 **\u6307\u4ee4\u5b57\u957f\u5ea6**\u7ae0\u8282\u4e5f\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b \u7531\u4e8e\u8ba1\u7b97\u673a\u4f7f\u7528\u7684\u4fe1\u606f\u65e2\u6709\u6307\u4ee4\u53c8\u6709\u6570\u636e\uff0c\u6240\u4ee5**\u8ba1\u7b97\u673a\u5b57**\u65e2\u53ef\u4ee5\u4ee3\u8868**\u6307\u4ee4**\uff0c\u4e5f\u53ef\u4ee5\u4ee3\u8868**\u6570\u636e**\uff1b\u5982\u679c\u67d0\u5b57\u4ee3\u8868\u7684\u662f\u8981\u5904\u7406\u7684\u6570\u636e\uff0c\u5219\u79f0\u4e3a**\u6570\u636e\u5b57**\uff1b\u5982\u679c\u67d0\u5b57\u4e3a\u4e00\u6761\u6307\u4ee4\uff0c\u5219\u79f0\u4e3a**\u6307\u4ee4\u5b57**\uff1b \u6211\u4eec\u770b\u5230\uff0c\u6307\u4ee4\u548c\u6570\u636e\u7edf\u7edf\u90fd\u5b58\u653e\u5728\u5185\u5b58\u4e2d\uff0c\u4ece\u5f62\u5f0f\u4e0a\u6765\u770b\uff0c\u5b83\u4eec\u90fd\u662f\u4e8c\u8fdb\u5236\u6570\u7801\uff0c\u4f3c\u4e4e\u5f88\u96be\u5206\u6e05\u695a\u54ea\u4e9b\u662f**\u6307\u4ee4\u5b57**\uff0c\u54ea\u4e9b\u662f**\u6570\u636e\u5b57**\uff1b\u90a3\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u63a7\u5236\u5668\u662f\u5982\u4f55\u533a\u5206\u5f00\u54ea\u4e9b\u662f**\u6307\u4ee4\u5b57**\uff0c\u54ea\u4e9b\u662f**\u6570\u636e\u5b57**\u7684\uff1f\u7b54\u6848\u4e3a\uff1a \u53d6\u6307\u5468\u671f**\u4e2d\uff0c\u4ece**\u5185\u5b58**\u8bfb\u51fa\u7684**\u4fe1\u606f\u6d41**\u662f**\u6307\u4ee4\u6d41 \uff0c\u5b83\u6d41\u5411**\u63a7\u5236\u5668**\uff1b \u6267\u884c\u5468\u671f**\u4e2d\u4ece\u5185\u5b58\u4e2d\u8bfb\u51fa\u7684**\u4fe1\u606f\u6d41**\u4e3a**\u6570\u636e\u6d41 \uff0c\u5b83\u7531\u5185\u5b58\u6d41\u5411**\u8fd0\u7b97\u5668**\uff1b\u663e\u7136\uff0c\u67d0\u4e9b\u6307\u4ee4\u8fdb\u884c\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u4e24\u6b21\u8bbf\u95ee\u5185\u5b58\uff0c\u4e00\u6b21\u662f\u53d6\u6307\u4ee4\uff0c\u53e6\u4e00\u6b21\u662f\u53d6\u6570\u636e\uff0c\u5982\u88681.4\u4e2d\u53d6\u6570\u3001\u4e58\u6570\u3001\u52a0\u6cd5\u3001\u51cf\u6cd5\u3001\u5b58\u6570\u6307\u4ee4\u5c31\u662f\u5982\u6b64\uff1b 1.3.5 \u9002\u914d\u5668\u4e0e\u8f93\u5165\u8f93\u51fa\u8bbe\u5907 \u8ba1\u7b97\u673a\u7cfb\u7edf\u4e2d\u5fc5\u987b\u6709**\u603b\u7ebf**\uff0c \u7cfb\u7edf\u603b\u7ebf**\u662f\u6784\u6210\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u9aa8\u67b6\uff0c\u662f\u591a\u4e2a\u7cfb\u7edf\u90e8\u4ef6\u4e4b\u95f4\u8fdb\u884c\u6570\u636e\u4f20\u8f93\u7684\u516c\u5171\u901a\u8def\uff1b\u501f\u52a9**\u7cfb\u7edf\u603b\u7ebf \uff0c\u8ba1\u7b97\u673a\u5728\u7cfb\u7edf\u90e8\u4ef6\u4e4b\u95f4\u5b9e\u73b0\u4f20\u9001\u5730\u5740\u3001\u6570\u636e\u548c\u63a7\u5236\u4fe1\u606f\uff1b","title":"1.3-\u8ba1\u7b97\u673a\u786c\u4ef6"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#13","text":"","title":"1.3\u8ba1\u7b97\u673a\u7684\u786c\u4ef6"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#131","text":"\u4f7f\u7528\u6253\u7b97\u76d8\u6765\u8bf4\u660e\u8ba1\u7b97\u673a\u7684\u4e3b\u8981\u7ec4\u6210\u548c\u5de5\u4f5c\u539f\u7406\u3002\u7ed9\u5b9a\u4e00\u4e2a\u7b97\u76d8\u3001\u4e00\u5f20\u5e26\u6709\u6a2a\u683c\u7684\u7eb8\u3001\u4e00\u652f\u7b14\uff0c\u8981\u6c42\u8ba1\u7b97y=ax+b-c\u3002\u4e0b\u9762\u5c06\u89e3\u9898\u6b65\u9aa4\u8bb0\u5f55\u5728\u5e26\u6709\u6a2a\u683c\u7684\u7eb8\u5e26\u4e0a\uff1a \u88681.3\u89e3\u9898\u6b65\u9aa4\u548c\u6570\u636e\u8bb0\u5f55\u5728\u6a2a\u683c\u7eb8\u4e0a \u884c\u6570 \u89e3\u9898\u6b65\u9aa4\u548c\u6570\u636e \u8bf4\u660e 1 \u53d6\u6570 \uff089\uff09->\u7b97\u76d8 \uff089\uff09\u8868\u793a\u7b2c9\u884c\u7684\u6570a\uff0c\u4e0b\u540c 2 \u4e58\u6cd5 \uff0812\uff09->\u7b97\u76d8 \u5b8c\u6210 a*x \uff0c\u7ed3\u679c\u5728\u7b97\u76d8\u4e0a 3 \u52a0\u6cd5 \uff0810\uff09->\u7b97\u76d8 \u5b8c\u6210 a*x+b \uff0c\u7ed3\u679c\u5728\u7b97\u76d8\u4e0a 4 \u51cf\u6cd5 \uff0811\uff09->\u7b97\u76d8 \u5b8c\u6210 y=a*x+b-c \uff0c\u7ed3\u679c\u5728\u7b97\u76d8\u4e0a 5 \u5b58\u6570 y->13 \u7b97\u76d8\u4e0a\u7684y\u503c\u8bb0\u523013\u884c 6 \u8f93\u51fa \u5c06\u7b97\u76d8\u4e0a\u7684y\u503c\u5199\u51fa\u6765\u7ed9\u4eba\u770b 7 \u505c\u6b62 8 9 a \u6570\u636e 10 b \u6570\u636e 11 c \u6570\u636e 12 x \u6570\u636e 13 y \u6570\u636e \u5728\u7535\u5b50\u8ba1\u7b97\u673a\u4e2d\uff1a **\u8fd0\u7b97\u5668**\u76f8\u5f53\u4e8e\u7b97\u76d8 **\u5b58\u50a8\u5668**\u76f8\u5f53\u4e8e\u7eb8 **\u63a7\u5236\u5668**\u76f8\u5f53\u4e8e\u4eba\u7684\u5927\u8111\uff08\u63a7\u5236\u7740\u6574\u4e2a\u8ba1\u7b97\u8fc7\u7a0b\uff09 |<--> \u5b58\u50a8\u5668 | \u7cfb |<--> \u8fd0\u7b97\u5668 \u7edf | \u603b |<--> \u63a7\u5236\u5668 \u7ebf | |<--> \u9002\u914d\u5668 | | | | \u8f93\u5165\u8bbe\u5907 \u8f93\u51fa\u8bbe\u5907 CPU = \u8fd0\u7b97\u5668 + \u63a7\u5236\u5668","title":"1.3.1 \u786c\u4ef6\u7ec4\u6210\u8981\u7d20"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#132","text":"\u53c2\u89c1\uff1a ALU \u5b57\u957f\uff1a\u8fd0\u7b97\u5668\u7684\u957f\u5ea6","title":"1.3.2 \u8fd0\u7b97\u5668"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#133","text":"\u53c2\u89c1\uff1a Computer memory","title":"1.3.3 \u5b58\u50a8\u5668"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#134","text":"\u53c2\u89c1\uff1a Control unit \u63a7\u5236\u5668\u7684\u4efb\u52a1\u662f\u4ece\u5185\u5b58\u4e2d\u53d6\u51fa\u89e3\u9898\u6b65\u9aa4\u52a0\u4ee5\u5206\u6790\uff0c\u7136\u540e\u6267\u884c\u67d0\u79cd\u64cd\u4f5c\uff1b NOTE: \u8bfb\u53d6\u6307\u4ee4\u548c\u8bfb\u53d6\u6570\u636e\u90fd\u662f\u7531\u63a7\u5236\u5668\u6765\u5b8c\u6210\u7684\uff1b\u6240\u4ee5\u63a7\u5236\u5668\u9700\u8981\u4e86\u89e3\u6307\u4ee4\u7684\u683c\u5f0f","title":"1.3.4 \u63a7\u5236\u5668"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#1","text":"\u88681.4\u8ba1\u7b97y=ax+b-c\u7684\u7a0b\u5e8f \u6307\u4ee4\u5730\u5740 \u64cd\u4f5c\u7801 \u5730\u5740\u7801 \u6307\u4ee4\u64cd\u4f5c\u5185\u5bb9 \u8bf4\u660e 1 \u53d6\u6570 9 (9) -> A \u5c06\u5b58\u50a8\u56689\u53f7\u5730\u5740\u7684\u6570\u8bfb\u5165\u5230\u8fd0\u7b97\u5668A 2 \u4e58\u6cd5 12 (A) * (12) -> A \u5b8c\u6210 a*x \uff0c\u7ed3\u679c\u4fdd\u7559\u7740\u8fd0\u7b97\u5668A 3 \u52a0\u6cd5 10 (A) + (10) -> A \u5b8c\u6210 a*x+b \uff0c\u7ed3\u679c\u4fdd\u7559\u7740\u8fd0\u7b97\u5668A 4 \u51cf\u6cd5 11 (A) - (11) -> A \u5b8c\u6210 a*x+b-c \uff0c\u7ed3\u679c\u4fdd\u7559\u7740\u8fd0\u7b97\u5668A 5 \u5b58\u6570 13 A->13 \u8fd0\u7b97\u5668A\u4e2d\u7684\u7ed3\u679cy\u9001\u5165\u5230\u5b58\u50a8\u566813\u53f7\u5730\u5740 6 \u6253\u5370 7 \u505c\u6b62 8 \u6570\u636e\u5730\u5740 \u6570\u636e \u8bf4\u660e 9 a 10 b 11 c 12 x 13 y \u88681.4\u662f\u4e00\u4e2a\u975e\u5e38\u597d\u7684\u4f8b\u5b50\uff0c\u8fd9\u4e2a\u8868\u4e2d\u5c55\u793a\u4e86**\u6307\u4ee4**\uff0c \u6570\u636e \uff0cCPU\u7684\u8fd0\u7b97\u903b\u8f91\u7b49\u5173\u952e\u5185\u5bb9\uff1b NOTE : \u4ece\u88681.4\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u8fd0\u7b97\u5668\u7684\u64cd\u4f5c\u6570\u9700\u8981\u901a\u8fc7**\u53d6\u6570\u6307\u4ee4**\u4ece\u5185\u5b58\u4e2d\u52a0\u8f7d\u5230**\u8fd0\u7b97\u5668**\u4e2d\uff1b\u88681.4\u8fd8\u5c55\u793a\u5904\u7406\uff0c\u8fd0\u7b97\u5668\u5728\u6267\u884c\u8fd0\u7b97\u6307\u4ee4\uff0c\u5982**\u4e58\u6cd5\u6307\u4ee4**\uff0c**\u52a0\u6cd5\u6307\u4ee4**\u7b49\u7684\u65f6\u5019\uff0c\u4ea7\u751f\u7684\u8fd0\u7b97\u7ed3\u679c\u662f\u4fdd\u7559\u5728\u8fd0\u7b97\u5668\u4e2d\uff0c\u9700\u8981\u6267\u884c**\u5b58\u6570\u6307\u4ee4**\u624d\u80fd\u591f\u5c06**\u8fd0\u7b97\u5668**\u4e2d\u7684\u6570\u636e\u5199\u5165\u5230\u5185\u5b58\u4e2d\uff1b\u8fd9\u5176\u5b9e\u975e\u5e38\u662f\u7b26\u5408 Load\u2013store architecture \u7684\u3002","title":"1. \u8ba1\u7b97\u7a0b\u5e8f"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#2","text":"\u6bcf\u6761\u6307\u4ee4\u9700\u8981\u660e\u786e\u5730\u544a\u8bc9\u63a7\u5236\u5668\uff0c\u4ece\u5b58\u50a8\u5668\u7684\u54ea\u4e2a\u5355\u5143\u53d6\u6570\uff0c\u5e76\u8fdb\u884c\u4f55\u79cd\u64cd\u4f5c\uff1b\u8fd9\u6837\u4e00\u6765\uff0c\u53ef\u77e5\u6307\u4ee4\u7684\u5185\u5bb9\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff1a\u64cd\u4f5c\u7684\u6027\u8d28\u548c\u64cd\u4f5c\u6570\u7684\u5730\u5740\uff1b\u524d\u8005\u4e3a**\u64cd\u4f5c\u7801**\uff0c\u540e\u8005\u4e3a**\u5730\u5740\u7801**\uff1b \u64cd\u4f5c\u7801|\u5730\u5740\u7801 \u6307\u4ee4\u7684**\u64cd\u4f5c\u7801**\u548c**\u5730\u5740\u7801**\u90fd\u53ef\u4ee5\u4f7f\u7528**\u4e8c\u8fdb\u5236\u4ee3\u7801**\u6765\u8fdb\u884c\u8868\u793a\uff0c\u8fd9\u5c31\u662f**\u6307\u4ee4\u7684\u6570\u7801\u5316**\uff1b\u6307\u4ee4**\u6570\u7801\u5316**\u4ee5\u540e\uff0c\u5c31\u770b\u53ef\u4ee5\u548c\u6570\u636e\u4e00\u6837\u5b58\u5165\u5230**\u5b58\u50a8\u5668**\u4e2d( Stored-program computer )\uff1b\u5b58\u50a8\u5668\u7684\u4efb\u4f55\u4f4d\u7f6e\u65e2\u53ef\u4ee5\u5b58\u653e\u6570\u636e\u4e5f\u53ef\u4ee5\u5b58\u653e\u6307\u4ee4\uff0c\u4e0d\u8fc7\u4e00\u822c\u5c06\u6307\u4ee4\u548c\u6570\u636e\u5206\u5f00\u5b58\u653e\u3002 \u63a7\u5236\u5668**\u4f9d\u636e\u5b58\u50a8\u7684\u7a0b\u5e8f\u6765\u63a7\u5236\u5168\u673a\u534f\u8c03\u5730\u5b8c\u6210\u8ba1\u7b97\u4efb\u52a1\u53eb\u505a**\u7a0b\u5e8f\u63a7\u5236 \u3002 \u6309\u7167 Von Neumann architecture \u7684\u601d\u60f3\uff0c\u6307\u4ee4\u548c\u6570\u636e\uff08\u5982\u679c\u4e0d\u6e05\u695a\u4e24\u8005\u7684\u5dee\u522b\uff0c\u53bb\u770b\u88681-4\uff09\u5b58\u653e\u5728\u540c\u4e00\u4e2a\u5b58\u50a8\u5668\u4e2d\uff0c**\u63a7\u5236\u5668**\u6309\u7167stored program\u7684**\u5730\u5740\u987a\u5e8f**\u8fdb\u884c\u6267\u884c\uff1b \u6309\u7167 Harvard architecture \u7684\u601d\u60f3\uff0c\u5c06\u6307\u4ee4\u548c\u6570\u636e\u5206\u522b\u5b58\u653e\u5728\u4e24\u4e2a\u5b58\u50a8\u5668\u4e2d\uff1b \u4e00\u53f0\u8ba1\u7b97\u673a\u901a\u5e38\u6709\u51e0\u5341\u79cd\u57fa\u672c**\u6307\u4ee4**\uff0c\u4ece\u800c\u6784\u6210\u8be5\u8ba1\u7b97\u673a\u7684**\u6307\u4ee4\u7cfb\u7edf**\u3002**\u6307\u4ee4\u7cfb\u7edf**\u4e0d\u4ec5\u662f**\u786c\u4ef6\u8bbe\u8ba1**\u7684\u4f9d\u636e\uff0c\u800c\u4e14\u662f**\u8f6f\u4ef6\u8bbe\u8ba1**\u7684\u57fa\u7840\u3002\u56e0\u6b64**\u6307\u4ee4\u7cfb\u7edf**\u662f\u8861\u91cf**\u8ba1\u7b97\u673a\u6027\u80fd**\u7684\u4e00\u4e2a\u91cd\u8981\u6307\u6807\u3002","title":"2. \u6307\u4ee4\u7684\u5f62\u5f0f"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#3","text":"\u7531\u88681-4\u53ef\u77e5\uff0c\u8ba1\u7b97\u673a\u5728\u8fdb\u884c\u8ba1\u7b97\u7684\u65f6\u5019\uff0c**\u6307\u4ee4**\u5fc5\u987b\u662f\u6309\u7167\u4e00\u5b9a\u7684**\u987a\u5e8f**\u4e00\u6761\u63a5\u7740\u4e00\u6761\u7684\u8fdb\u884c\uff1b \u63a7\u5236\u5668**\u7684\u57fa\u672c\u4efb\u52a1\u5c31\u662f\u6309\u7167\u8ba1\u7b97\u673a\u7a0b\u5e8f\u6240\u6392\u7684\u6307\u4ee4\u5e8f\u5217\uff0c\u5148\u4ece**\u5b58\u50a8\u5668**\u4e2d\u53d6\u51fa\u4e00\u6761\u6307\u4ee4\u653e\u5230**\u63a7\u5236\u5668**\u4e2d\uff0c\u5bf9\u8be5**\u6307\u4ee4**\u7684**\u64cd\u4f5c\u7801**\u7531**\u8bd1\u7801\u5668**\u8fdb\u884c\u5206\u6790\u5224\u65ad\uff0c\u7136\u540e\u6839\u636e**\u6307\u4ee4\u6027\u8d28 \uff08\u5373\u8be5\u6307\u4ee4\u8981\u505a\u4ec0\u4e48\u8fd0\u7b97\uff09\uff0c\u6267\u884c\u8fd9\u6761\u6307\u4ee4\uff1b\u63a5\u7740\u4ece\u5b58\u50a8\u5668\u4e2d\u53d6\u51fa\u7b2c\u4e8c\u6761\u6307\u4ee4\uff0c\u518d\u6267\u884c\u7b2c\u4e8c\u6761\u6307\u4ee4\u3002\u4ee5\u6b64\u7c7b\u63a8\u3002 \u901a\u5e38\uff0c\u5c06\u53d6\u6307\u4ee4\u7684\u8fd9\u6bb5\u65f6\u95f4\u53eb\u505a**\u53d6\u6307\u5468\u671f**\uff0c\u800c\u628a\u6267\u884c\u6307\u4ee4\u7684\u4e00\u6bb5\u65f6\u95f4\u53eb\u505a**\u6267\u884c\u5468\u671f**\u3002\u56e0\u6b64\u63a7\u5236\u5668\u53cd\u590d\u4ea4\u66ff\u5730\u5904\u4e8e\u53d6\u6307\u5468\u671f\u548c\u6267\u884c\u5468\u671f\u4e4b\u4e2d\uff1b \u6bcf\u53d6\u51fa\u4e00\u6761\u6307\u4ee4\uff0c\u63a7\u5236\u5668\u4e2d\u7684**\u6307\u4ee4\u8ba1\u6570\u5668**\u5c31\u52a01\uff0c\u4ece\u800c\u4e3a\u4e0b\u4e00\u6761\u6307\u4ee4\u505a\u597d\u51c6\u5907\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\u6307\u4ee4\u5728\u5b58\u50a8\u5668\u4e2d\u662f\u987a\u5e8f\u5b58\u653e\u7684\u539f\u56e0\uff1b","title":"3. \u63a7\u5236\u5668\u7684\u57fa\u672c\u4efb\u52a1"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#4","text":"\u6211\u4eec\u4f7f\u7528bit\u6765\u4f5c\u4e3a\u8ba1\u7b97\u673a\u7684\u6700\u5c0f\u4fe1\u606f\u5355\u4f4d\u3002 \u5f53CPU\u5411\u5b58\u50a8\u5668\u9001\u5165\u6216\u8005\u4ece\u5b58\u50a8\u5668\u4e2d\u53d6\u51fa\u4fe1\u606f\u65f6\uff0c\u5f80\u5f80\u662f\u5b58\u53d6byte\uff08\u5b57\u8282\uff09\u548cWord\uff08\u5b57\uff09\u7b49\u8f83\u5927\u4fe1\u606f\u5355\u4f4d\uff0c\u800c\u4e0d\u662fbit\uff1b\u901a\u5e38\u5c06\u7ec4\u6210\u4e00\u4e2aWord\u7684\u4e8c\u8fdb\u5236\u4f4d\u6570\u53eb\u505a**\u5b57\u957f**\uff1b NOTE: \u8fd9\u5e94\u8be5\u662f\u6211\u4eec\u9700\u8981\u8fdb\u884c\u5730\u5740\u5bf9\u9f50\u7684\u539f\u56e0\u6240\u5728\uff1b NOTE: \u8fd9\u90e8\u5206\u5185\u5bb9\u57284.2.3 **\u6307\u4ee4\u5b57\u957f\u5ea6**\u7ae0\u8282\u4e5f\u8fdb\u884c\u4e86\u4ecb\u7ecd\uff1b \u7531\u4e8e\u8ba1\u7b97\u673a\u4f7f\u7528\u7684\u4fe1\u606f\u65e2\u6709\u6307\u4ee4\u53c8\u6709\u6570\u636e\uff0c\u6240\u4ee5**\u8ba1\u7b97\u673a\u5b57**\u65e2\u53ef\u4ee5\u4ee3\u8868**\u6307\u4ee4**\uff0c\u4e5f\u53ef\u4ee5\u4ee3\u8868**\u6570\u636e**\uff1b\u5982\u679c\u67d0\u5b57\u4ee3\u8868\u7684\u662f\u8981\u5904\u7406\u7684\u6570\u636e\uff0c\u5219\u79f0\u4e3a**\u6570\u636e\u5b57**\uff1b\u5982\u679c\u67d0\u5b57\u4e3a\u4e00\u6761\u6307\u4ee4\uff0c\u5219\u79f0\u4e3a**\u6307\u4ee4\u5b57**\uff1b \u6211\u4eec\u770b\u5230\uff0c\u6307\u4ee4\u548c\u6570\u636e\u7edf\u7edf\u90fd\u5b58\u653e\u5728\u5185\u5b58\u4e2d\uff0c\u4ece\u5f62\u5f0f\u4e0a\u6765\u770b\uff0c\u5b83\u4eec\u90fd\u662f\u4e8c\u8fdb\u5236\u6570\u7801\uff0c\u4f3c\u4e4e\u5f88\u96be\u5206\u6e05\u695a\u54ea\u4e9b\u662f**\u6307\u4ee4\u5b57**\uff0c\u54ea\u4e9b\u662f**\u6570\u636e\u5b57**\uff1b\u90a3\u8fd9\u5c31\u5f15\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u63a7\u5236\u5668\u662f\u5982\u4f55\u533a\u5206\u5f00\u54ea\u4e9b\u662f**\u6307\u4ee4\u5b57**\uff0c\u54ea\u4e9b\u662f**\u6570\u636e\u5b57**\u7684\uff1f\u7b54\u6848\u4e3a\uff1a \u53d6\u6307\u5468\u671f**\u4e2d\uff0c\u4ece**\u5185\u5b58**\u8bfb\u51fa\u7684**\u4fe1\u606f\u6d41**\u662f**\u6307\u4ee4\u6d41 \uff0c\u5b83\u6d41\u5411**\u63a7\u5236\u5668**\uff1b \u6267\u884c\u5468\u671f**\u4e2d\u4ece\u5185\u5b58\u4e2d\u8bfb\u51fa\u7684**\u4fe1\u606f\u6d41**\u4e3a**\u6570\u636e\u6d41 \uff0c\u5b83\u7531\u5185\u5b58\u6d41\u5411**\u8fd0\u7b97\u5668**\uff1b\u663e\u7136\uff0c\u67d0\u4e9b\u6307\u4ee4\u8fdb\u884c\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u4e24\u6b21\u8bbf\u95ee\u5185\u5b58\uff0c\u4e00\u6b21\u662f\u53d6\u6307\u4ee4\uff0c\u53e6\u4e00\u6b21\u662f\u53d6\u6570\u636e\uff0c\u5982\u88681.4\u4e2d\u53d6\u6570\u3001\u4e58\u6570\u3001\u52a0\u6cd5\u3001\u51cf\u6cd5\u3001\u5b58\u6570\u6307\u4ee4\u5c31\u662f\u5982\u6b64\uff1b","title":"4. \u6307\u4ee4\u6d41\u548c\u6570\u636e\u6d41"},{"location":"Book-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/1.3-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6/#135","text":"\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e2d\u5fc5\u987b\u6709**\u603b\u7ebf**\uff0c \u7cfb\u7edf\u603b\u7ebf**\u662f\u6784\u6210\u8ba1\u7b97\u673a\u7cfb\u7edf\u7684\u9aa8\u67b6\uff0c\u662f\u591a\u4e2a\u7cfb\u7edf\u90e8\u4ef6\u4e4b\u95f4\u8fdb\u884c\u6570\u636e\u4f20\u8f93\u7684\u516c\u5171\u901a\u8def\uff1b\u501f\u52a9**\u7cfb\u7edf\u603b\u7ebf \uff0c\u8ba1\u7b97\u673a\u5728\u7cfb\u7edf\u90e8\u4ef6\u4e4b\u95f4\u5b9e\u73b0\u4f20\u9001\u5730\u5740\u3001\u6570\u636e\u548c\u63a7\u5236\u4fe1\u606f\uff1b","title":"1.3.5 \u9002\u914d\u5668\u4e0e\u8f93\u5165\u8f93\u51fa\u8bbe\u5907"},{"location":"CPU/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbaCPU\uff0c\u91cd\u8981\u5185\u5bb9\u5982\u4e0b: \u7ae0\u8282 \u7b80\u4ecb CPU \u4ecb\u7ecdCPU Components \u63cf\u8ff0CPU\u7684\u7ec4\u6210 Instruction-set-architectures \u6307\u4ee4\u96c6 Memory-access \u63cf\u8ff0CPU\u548cmemory\u7684\u4ea4\u4e92 Execution-of-instruction \u63cf\u8ff0CPU\u6267\u884c\u6307\u4ee4\u7684\u8fc7\u7a0b Manufacturer \u63cf\u8ff0\u77e5\u540d\u7684CPU\u5382\u5546","title":"Introduction"},{"location":"CPU/#_1","text":"\u672c\u7ae0\u8ba8\u8bbaCPU\uff0c\u91cd\u8981\u5185\u5bb9\u5982\u4e0b: \u7ae0\u8282 \u7b80\u4ecb CPU \u4ecb\u7ecdCPU Components \u63cf\u8ff0CPU\u7684\u7ec4\u6210 Instruction-set-architectures \u6307\u4ee4\u96c6 Memory-access \u63cf\u8ff0CPU\u548cmemory\u7684\u4ea4\u4e92 Execution-of-instruction \u63cf\u8ff0CPU\u6267\u884c\u6307\u4ee4\u7684\u8fc7\u7a0b Manufacturer \u63cf\u8ff0\u77e5\u540d\u7684CPU\u5382\u5546","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/CPU/","text":"CPU wikipedia Central processing unit","title":"CPU"},{"location":"CPU/CPU/#cpu","text":"","title":"CPU"},{"location":"CPU/CPU/#wikipedia#central#processing#unit","text":"","title":"wikipedia Central processing unit"},{"location":"CPU/Components/Control-unit/","text":"Control unit","title":"Control-unit"},{"location":"CPU/Components/Control-unit/#control#unit","text":"","title":"Control unit"},{"location":"CPU/Components/Register/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0register\uff0c\u5b83\u662f\u7406\u89e3\u5f88\u591a\u5185\u5bb9\u7684\u57fa\u7840\uff1b\u5173\u4e8e\u5177\u4f53architecture\u4e2d\u7684register\uff0c\u53c2\u89c1: architecture \u7ae0\u8282 ARM CPU\\ARM Intel CPU\\Intel \u9664\u4e86\u672c\u5de5\u7a0b\uff0c\u5728\u4e0b\u9762\u7684\u5de5\u7a0b\u4e2d\u4e5f\u6d89\u53ca\u4e86register\uff1a \u5de5\u7a0b \u7ae0\u8282 \u5de5\u7a0bprogramming-language C-family-language\\C-and-C++\\From-source-code-to-exec\\ABI\\Call-convention \u7ae0\u8282\uff0c\u6d89\u53ca\u5230Stack-register \u5de5\u7a0bLinux-OS Shell-and-tools\\Tools\\Debug\\GDB\\Debugging-with-gdb\\10-Examining-Data\\10.13-Registers.md \u7ae0\u8282\uff0c\u5176\u4e2d\u63cf\u8ff0\u4e86\u5982\u4f55\u67e5\u770bregister\u7684\u503c Architecture\u2019s canonical mnemonics for registers. \u4e0d\u540c\u7684Architecture\u5bf9\u4e8eregister\u5f80\u5f80\u91c7\u7528\u4e0d\u540c\u7684canonical mnemonics\uff08\u52a9\u8bb0\u7b26\uff09\uff0c\u975e\u5e38\u5178\u578b\u7684\u5c31\u662fIntel\u548cARM\uff0c\u5173\u4e8e\u6b64\uff0c\u8be6\u89c1\u63cf\u8ff0\u5b83\u4eec\u7684\u7ae0\u8282\u3002","title":"Introduction"},{"location":"CPU/Components/Register/#_1","text":"\u672c\u7ae0\u63cf\u8ff0register\uff0c\u5b83\u662f\u7406\u89e3\u5f88\u591a\u5185\u5bb9\u7684\u57fa\u7840\uff1b\u5173\u4e8e\u5177\u4f53architecture\u4e2d\u7684register\uff0c\u53c2\u89c1: architecture \u7ae0\u8282 ARM CPU\\ARM Intel CPU\\Intel \u9664\u4e86\u672c\u5de5\u7a0b\uff0c\u5728\u4e0b\u9762\u7684\u5de5\u7a0b\u4e2d\u4e5f\u6d89\u53ca\u4e86register\uff1a \u5de5\u7a0b \u7ae0\u8282 \u5de5\u7a0bprogramming-language C-family-language\\C-and-C++\\From-source-code-to-exec\\ABI\\Call-convention \u7ae0\u8282\uff0c\u6d89\u53ca\u5230Stack-register \u5de5\u7a0bLinux-OS Shell-and-tools\\Tools\\Debug\\GDB\\Debugging-with-gdb\\10-Examining-Data\\10.13-Registers.md \u7ae0\u8282\uff0c\u5176\u4e2d\u63cf\u8ff0\u4e86\u5982\u4f55\u67e5\u770bregister\u7684\u503c","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Components/Register/#architectures#canonical#mnemonics#for#registers","text":"\u4e0d\u540c\u7684Architecture\u5bf9\u4e8eregister\u5f80\u5f80\u91c7\u7528\u4e0d\u540c\u7684canonical mnemonics\uff08\u52a9\u8bb0\u7b26\uff09\uff0c\u975e\u5e38\u5178\u578b\u7684\u5c31\u662fIntel\u548cARM\uff0c\u5173\u4e8e\u6b64\uff0c\u8be6\u89c1\u63cf\u8ff0\u5b83\u4eec\u7684\u7ae0\u8282\u3002","title":"Architecture\u2019s canonical mnemonics for registers."},{"location":"CPU/Components/Register/Program-counter/","text":"Program counter \u8fd9\u4e2a\u5bc4\u5b58\u5668\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u544a\u8bc9CPU\u53bb\u6267\u884c\u54ea\u4e00\u6761\u6307\u4ee4\u3002\u6240\u6709\u5bf9 control flow \u7684\u64cd\u4f5c\u7684\u6307\u4ee4\u6700\u7ec8\u90fd\u662f\u901a\u8fc7\u64cd\u4f5c\u8fd9\u4e2a\u5bc4\u5b58\u5668\u7684\u503c\u6765\u5b9e\u73b0\u7684\u3002 Program counter \u6240\u6307\u5411\u7684\u80af\u5b9a\u662fcode area\uff0c Program counter \u76f8\u5f53\u4e8enext pointer\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5b83\u662f\u81ea\u52a01\u7684\uff0c\u9664\u975e\u901a\u8fc7 JMP (x86 instruction) \u7b49\u6307\u4ee4\u6765\u663e\u793a\u66f4\u6539\u5b83\u7684\u503c\u3002","title":"Program-counter"},{"location":"CPU/Components/Register/Program-counter/#program#counter","text":"\u8fd9\u4e2a\u5bc4\u5b58\u5668\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u544a\u8bc9CPU\u53bb\u6267\u884c\u54ea\u4e00\u6761\u6307\u4ee4\u3002\u6240\u6709\u5bf9 control flow \u7684\u64cd\u4f5c\u7684\u6307\u4ee4\u6700\u7ec8\u90fd\u662f\u901a\u8fc7\u64cd\u4f5c\u8fd9\u4e2a\u5bc4\u5b58\u5668\u7684\u503c\u6765\u5b9e\u73b0\u7684\u3002 Program counter \u6240\u6307\u5411\u7684\u80af\u5b9a\u662fcode area\uff0c Program counter \u76f8\u5f53\u4e8enext pointer\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u5b83\u662f\u81ea\u52a01\u7684\uff0c\u9664\u975e\u901a\u8fc7 JMP (x86 instruction) \u7b49\u6307\u4ee4\u6765\u663e\u793a\u66f4\u6539\u5b83\u7684\u503c\u3002","title":"Program counter"},{"location":"CPU/Components/Register/Stack-register/","text":"Stack register A stack register is a computer central processor register whose purpose is to keep track of a call stack . On an accumulator-based architecture machine, this may be a dedicated register such as SP on an Intel x86 machine. Stack registers in x86 In 8086 , the main stack register is called stack pointer - SP. The stack segment register (SS) is usually used to store information about the memory segment that stores the call stack of currently executed program. SP points to current stack top . By default, the stack grows downward in memory, so newer values are placed at lower memory addresses. To push a value to the stack, the PUSH instruction is used. To pop a value from the stack, the POP instruction is used. NOTE: \u5bf9\u4e8e 8086 \uff0c SP\u548cSS\u9700\u8981\u4e00\u8d77\u624d\u80fd\u591f\u5de5\u4f5c\u3002 Example : Assuming that SS = 1000h and SP = 0xF820. This means that current stack top is the physical address 0x1F820 (this is due to memory segmentation in 8086 ). The next two machine instructions of the program are: PUSH AX PUSH BX These first instruction shall push the value stored in AX (16-bit register) to the stack. This is done by subtracting a value of 2 (2 bytes) from SP. The new value of SP becomes 0xF81E. The CPU then copies the value of AX to the memory word whose physical address is 0x1F81E. When \"PUSH BX\" is executed, SP is set to 0xF81C and BX is copied to 0x1F81C. NOTE: \u4e0a\u9762\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5c31\u76f8\u5f53\u4e8e\u51fd\u6570\u5728\u6267\u884c\u7684\u65f6\u5019\uff0c\u7ed9\u81ea\u52a8\u53d8\u91cf\u5206\u914d\u5185\u5b58\u7a7a\u95f4\u3002 This illustrates how PUSH works. Usually, the running program pushes registers to the stack to make use of the registers for other purposes, like to call a routine that may change the current values of registers. To restore the values stored at the stack, the program shall contain machine instructions like this: POP BX POP AX POP BX copies the word at 0x1F81C (which is the old value of BX) to BX, then increases SP by 2. SP now is 0xF81E. POP AX copies the word at 0x1F81E to AX, then sets SP to 0xF820. NOTE : The program above pops BX first, that's because it was pushed last. NOTE : In 8086, PUSH & POP instructions can only work with 16-bit elements.","title":"Stack-register"},{"location":"CPU/Components/Register/Stack-register/#stack#register","text":"A stack register is a computer central processor register whose purpose is to keep track of a call stack . On an accumulator-based architecture machine, this may be a dedicated register such as SP on an Intel x86 machine.","title":"Stack register"},{"location":"CPU/Components/Register/Stack-register/#stack#registers#in#x86","text":"In 8086 , the main stack register is called stack pointer - SP. The stack segment register (SS) is usually used to store information about the memory segment that stores the call stack of currently executed program. SP points to current stack top . By default, the stack grows downward in memory, so newer values are placed at lower memory addresses. To push a value to the stack, the PUSH instruction is used. To pop a value from the stack, the POP instruction is used. NOTE: \u5bf9\u4e8e 8086 \uff0c SP\u548cSS\u9700\u8981\u4e00\u8d77\u624d\u80fd\u591f\u5de5\u4f5c\u3002 Example : Assuming that SS = 1000h and SP = 0xF820. This means that current stack top is the physical address 0x1F820 (this is due to memory segmentation in 8086 ). The next two machine instructions of the program are: PUSH AX PUSH BX These first instruction shall push the value stored in AX (16-bit register) to the stack. This is done by subtracting a value of 2 (2 bytes) from SP. The new value of SP becomes 0xF81E. The CPU then copies the value of AX to the memory word whose physical address is 0x1F81E. When \"PUSH BX\" is executed, SP is set to 0xF81C and BX is copied to 0x1F81C. NOTE: \u4e0a\u9762\u63cf\u8ff0\u7684\u8fc7\u7a0b\u5c31\u76f8\u5f53\u4e8e\u51fd\u6570\u5728\u6267\u884c\u7684\u65f6\u5019\uff0c\u7ed9\u81ea\u52a8\u53d8\u91cf\u5206\u914d\u5185\u5b58\u7a7a\u95f4\u3002 This illustrates how PUSH works. Usually, the running program pushes registers to the stack to make use of the registers for other purposes, like to call a routine that may change the current values of registers. To restore the values stored at the stack, the program shall contain machine instructions like this: POP BX POP AX POP BX copies the word at 0x1F81C (which is the old value of BX) to BX, then increases SP by 2. SP now is 0xF81E. POP AX copies the word at 0x1F81E to AX, then sets SP to 0xF820. NOTE : The program above pops BX first, that's because it was pushed last. NOTE : In 8086, PUSH & POP instructions can only work with 16-bit elements.","title":"Stack registers in x86"},{"location":"CPU/Components/Register/Status-register/","text":"Status register","title":"Status-register"},{"location":"CPU/Components/Register/Status-register/#status#register","text":"","title":"Status register"},{"location":"CPU/Execution-of-instruction/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bba\u5bf9\u6307\u4ee4\u7684\u6267\u884c\u8fc7\u7a0b\u3002","title":"Introduction"},{"location":"CPU/Execution-of-instruction/#_1","text":"\u672c\u7ae0\u8ba8\u8bba\u5bf9\u6307\u4ee4\u7684\u6267\u884c\u8fc7\u7a0b\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Execution-of-instruction/CPU%E7%9A%84%E6%97%B6%E5%BA%8F/","text":"\u9996\u5148\uff0cCPU\u6709 Clock generator \uff0c\u5b83\u4ea7\u751f Clock signal \uff0c\u5b83\u4ea7\u751fsignal\u7684\u9891\u7387\u79f0\u4e3a Clock rate \uff0cCPU\u7684\u63a7\u5236\u5355\u5143\u6709 Instruction cycle \uff0c\u5b83\u7684\u6027\u80fd\u5ea6\u91cf\u4e3a Cycles per instruction \u3002","title":"CPU\u7684\u65f6\u5e8f"},{"location":"CPU/Execution-of-instruction/Instruction-cycle/","text":"Instruction cycle wikipedia Instruction cycle The instruction cycle (also known as the fetch\u2013decode\u2013execute cycle or simply the fetch-execute cycle ) is the cycle which the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is composed of three main stages: the fetch stage, the decode stage, and the execute stage. This is a simple diagram illustrating the individual stages of the fetch-decode-execute cycle. In simpler CPUs, the instruction cycle is executed sequentially, each instruction being processed before the next one is started. In most modern CPUs, the instruction cycles are instead executed concurrently , and often in parallel , through an instruction pipeline : the next instruction starts being processed before the previous instruction has finished, which is possible because the cycle is broken up into separate steps.[ 1]","title":"Instruction-cycle"},{"location":"CPU/Execution-of-instruction/Instruction-cycle/#instruction#cycle","text":"","title":"Instruction cycle"},{"location":"CPU/Execution-of-instruction/Instruction-cycle/#wikipedia#instruction#cycle","text":"The instruction cycle (also known as the fetch\u2013decode\u2013execute cycle or simply the fetch-execute cycle ) is the cycle which the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is composed of three main stages: the fetch stage, the decode stage, and the execute stage. This is a simple diagram illustrating the individual stages of the fetch-decode-execute cycle. In simpler CPUs, the instruction cycle is executed sequentially, each instruction being processed before the next one is started. In most modern CPUs, the instruction cycles are instead executed concurrently , and often in parallel , through an instruction pipeline : the next instruction starts being processed before the previous instruction has finished, which is possible because the cycle is broken up into separate steps.[ 1]","title":"wikipedia Instruction cycle"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/","text":"Out-of-order execution 1\u3001\u7b80\u5355\u800c\u8a00\uff0cOoOE\u662fCPU\u4e3a\u4e86performance\uff0c\u4e0d\u6309\u7167In-order execution\uff0c\u5373 instruction cycle \uff0c\u800c\u662f\u91c7\u53d6\u7279\u6b8a\u7684\u6267\u884c\u65b9\u5f0f\u3002 2\u3001\u6700\u6700\u5178\u578b\u7684Out-of-order execution\u662fmemory reordering\uff0c\u5728 CPU-memory-access\\Memory-ordering \u7ae0\u8282\u8fdb\u884c\u4e86\u603b\u7ed3\u3002 Wikipedia Out-of-order execution In computer engineering , out-of-order execution (or more formally dynamic execution ), is a paradigm used in most high-performance microprocessors to make use of instruction cycles that would otherwise be wasted by a certain type of costly delay. In this paradigm, a processor executes instructions in an order governed by the availability of input data, rather than by their original order in a program.[ 1] In doing so, the processor can avoid being idle while waiting for the preceding instruction to complete to retrieve data for the next instruction in a program, processing instead the next instructions which are able to run immediately and independently.[ 2] It can be viewed as a hardware based dynamic recompilation or just-in-time compilation (JIT) to improve instruction scheduling . NOTE: CPU\u7684out-of-order execution\u662f\u5904\u4e8eperformance\u8003\u8651\u7684 Basic concept In-order processors Main article: Instruction cycle Out-of-order processors NOTE: \u6bd4\u8f83\u590d\u6742 The key concept of OoOE processing is to allow the processor to avoid a class of stalls that occur when the data needed to perform an operation are unavailable. The benefit of OoOE processing grows as the instruction pipeline deepens and the speed difference between main memory (or cache memory ) and the processor widens. On modern machines, the processor runs many times faster than the memory, so during the time an in-order processor spends waiting for data to arrive, it could have processed a large number of instructions. \u7d20\u6750 zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A \u5565\uff1f...\u8fd8\u771f\u7684\u662f\u8fd9\u6837\u3002\u539f\u56e0\u5728\u4e8e\u5f53\u4ee3CPU\u5185\u90e8\u4e5f\u6709\u6307\u4ee4\u91cd\u6392\u3002\u4e5f\u5c31\u662f\u8bf4\uff0cCPU\u6267\u884c\u6307\u4ee4\u7684\u987a\u5e8f\uff0c\u4e5f\u4e0d\u89c1\u5f97\u662f\u5b8c\u5168\u4e25\u683c\u6309\u7167\u673a\u5668\u7801\u7684\u987a\u5e8f\u3002\u7279\u522b\u662f\uff0c\u5f53\u4ee3CPU\u7684IPC\uff08\u6bcf\u65f6\u949f\u6267\u884c\u6307\u4ee4\u6570\uff09\u4e00\u822c\u90fd\u8fdc\u5927\u4e8e1\uff0c\u4e5f\u5c31\u662f\u6240\u8c13\u7684**\u591a\u53d1\u5c04**\uff0c\u5f88\u591a\u547d\u4ee4\u90fd\u662f\u540c\u65f6\u6267\u884c\u7684\u3002\u6bd4\u5982\uff0c\u5f53\u4ee3CPU\u5f53\u4e2d\uff08\u4e00\u4e2a\u6838\u5fc3\uff09\u4e00\u822c\u4f1a\u67092\u5957\u4ee5\u4e0a\u7684\u6574\u6570ALU\uff08\u52a0\u6cd5\u5668\uff09\uff0c2\u5957\u4ee5\u4e0a\u7684\u6d6e\u70b9ALU\uff08\u52a0\u6cd5\u5668\uff09\uff0c\u5f80\u5f80\u8fd8\u6709\u72ec\u7acb\u7684\u4e58\u6cd5\u5668\uff0c\u4ee5\u53ca\uff0c\u72ec\u7acb\u7684Load\u548cStore\u6267\u884c\u5668\u3002Load\u548cStore\u6a21\u5757\u5f80\u5f80\u8fd8\u67098\u4e2a\u4ee5\u4e0a\u7684\u961f\u5217\uff0c\u4e5f\u5c31\u662f\u53ef\u4ee5\u540c\u65f6\u8fdb\u884c8\u4e2a\u4ee5\u4e0a\u5185\u5b58\u5730\u5740\uff08cache line\uff09\u7684\u8bfb\u5199\u4ea4\u6362\u3002 \u662f\u4e0d\u662f\u6709\u4e9b\u6655\uff1f\u7b80\u5355\u6765\u8bf4\uff0c\u4f60\u53ef\u4ee5\u7406\u89e3\u5f53\u4ee3CPU\u4e0d\u4ec5\u662f\u591a\u6838\u5fc3\uff0c\u800c\u4e14\u6bcf\u4e2a\u6838\u5fc3\u8fd8\u662f\u591a\u4efb\u52a1\uff08\u591a\u6307\u4ee4\uff09\u5e76\u884c\u7684\u3002\u8ba1\u7b97\u673a\u8bfe\u672c\u4e0a\u7684\u90a3\u79cd\u4e00\u4e2a\u65f6\u949f\u4e00\u6761\u6307\u4ee4\u7684\uff0c\u65e9\u5c31\u662f\u8001\u9ec4\u5386\u4e86 \uff08\u5f53\u7136\uff0c\u5b8f\u89c2\u6765\u770b\u57fa\u672c\u539f\u7406\u5e76\u6ca1\u6709\u6539\u53d8\uff09 Processor technologies # Out-of-order \u5176\u4e2d\u603b\u7ed3\u4e86\u4e00\u4e9bout of order\u7684\u5185\u5bb9\u3002 1\u3001 Wide-issue \"wide issue\"\u5e94\u8be5\u5c31\u662f\"multiple issue\"\uff0c\u5373\u591a\u53d1\u5c04\u3002 umd.edu Multiple Issue Processors I NOTE: 1\u3001\u4ecb\u7ecd\u5730\u6bd4\u8f83\u8be6\u7ec6 Types of Multiple Issue Processors: There are basically two variations in multiple issue processors \u2013 Superscalar processors and VLIW (Very Long Instruction Word) processors. There are two types of superscalar processors that issue varying numbers of instructions per clock. They are statically scheduled superscalars that use in-order execution dynamically scheduled superscalars that use out-of-order execution","title":"Introduction"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#out-of-order#execution","text":"1\u3001\u7b80\u5355\u800c\u8a00\uff0cOoOE\u662fCPU\u4e3a\u4e86performance\uff0c\u4e0d\u6309\u7167In-order execution\uff0c\u5373 instruction cycle \uff0c\u800c\u662f\u91c7\u53d6\u7279\u6b8a\u7684\u6267\u884c\u65b9\u5f0f\u3002 2\u3001\u6700\u6700\u5178\u578b\u7684Out-of-order execution\u662fmemory reordering\uff0c\u5728 CPU-memory-access\\Memory-ordering \u7ae0\u8282\u8fdb\u884c\u4e86\u603b\u7ed3\u3002","title":"Out-of-order execution"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#wikipedia#out-of-order#execution","text":"In computer engineering , out-of-order execution (or more formally dynamic execution ), is a paradigm used in most high-performance microprocessors to make use of instruction cycles that would otherwise be wasted by a certain type of costly delay. In this paradigm, a processor executes instructions in an order governed by the availability of input data, rather than by their original order in a program.[ 1] In doing so, the processor can avoid being idle while waiting for the preceding instruction to complete to retrieve data for the next instruction in a program, processing instead the next instructions which are able to run immediately and independently.[ 2] It can be viewed as a hardware based dynamic recompilation or just-in-time compilation (JIT) to improve instruction scheduling . NOTE: CPU\u7684out-of-order execution\u662f\u5904\u4e8eperformance\u8003\u8651\u7684","title":"Wikipedia Out-of-order execution"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#basic#concept","text":"","title":"Basic concept"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#in-order#processors","text":"Main article: Instruction cycle","title":"In-order processors"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#out-of-order#processors","text":"NOTE: \u6bd4\u8f83\u590d\u6742 The key concept of OoOE processing is to allow the processor to avoid a class of stalls that occur when the data needed to perform an operation are unavailable. The benefit of OoOE processing grows as the instruction pipeline deepens and the speed difference between main memory (or cache memory ) and the processor widens. On modern machines, the processor runs many times faster than the memory, so during the time an in-order processor spends waiting for data to arrive, it could have processed a large number of instructions.","title":"Out-of-order processors"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#_1","text":"","title":"\u7d20\u6750"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#zhihu#c11#memory#order#a","text":"\u5565\uff1f...\u8fd8\u771f\u7684\u662f\u8fd9\u6837\u3002\u539f\u56e0\u5728\u4e8e\u5f53\u4ee3CPU\u5185\u90e8\u4e5f\u6709\u6307\u4ee4\u91cd\u6392\u3002\u4e5f\u5c31\u662f\u8bf4\uff0cCPU\u6267\u884c\u6307\u4ee4\u7684\u987a\u5e8f\uff0c\u4e5f\u4e0d\u89c1\u5f97\u662f\u5b8c\u5168\u4e25\u683c\u6309\u7167\u673a\u5668\u7801\u7684\u987a\u5e8f\u3002\u7279\u522b\u662f\uff0c\u5f53\u4ee3CPU\u7684IPC\uff08\u6bcf\u65f6\u949f\u6267\u884c\u6307\u4ee4\u6570\uff09\u4e00\u822c\u90fd\u8fdc\u5927\u4e8e1\uff0c\u4e5f\u5c31\u662f\u6240\u8c13\u7684**\u591a\u53d1\u5c04**\uff0c\u5f88\u591a\u547d\u4ee4\u90fd\u662f\u540c\u65f6\u6267\u884c\u7684\u3002\u6bd4\u5982\uff0c\u5f53\u4ee3CPU\u5f53\u4e2d\uff08\u4e00\u4e2a\u6838\u5fc3\uff09\u4e00\u822c\u4f1a\u67092\u5957\u4ee5\u4e0a\u7684\u6574\u6570ALU\uff08\u52a0\u6cd5\u5668\uff09\uff0c2\u5957\u4ee5\u4e0a\u7684\u6d6e\u70b9ALU\uff08\u52a0\u6cd5\u5668\uff09\uff0c\u5f80\u5f80\u8fd8\u6709\u72ec\u7acb\u7684\u4e58\u6cd5\u5668\uff0c\u4ee5\u53ca\uff0c\u72ec\u7acb\u7684Load\u548cStore\u6267\u884c\u5668\u3002Load\u548cStore\u6a21\u5757\u5f80\u5f80\u8fd8\u67098\u4e2a\u4ee5\u4e0a\u7684\u961f\u5217\uff0c\u4e5f\u5c31\u662f\u53ef\u4ee5\u540c\u65f6\u8fdb\u884c8\u4e2a\u4ee5\u4e0a\u5185\u5b58\u5730\u5740\uff08cache line\uff09\u7684\u8bfb\u5199\u4ea4\u6362\u3002 \u662f\u4e0d\u662f\u6709\u4e9b\u6655\uff1f\u7b80\u5355\u6765\u8bf4\uff0c\u4f60\u53ef\u4ee5\u7406\u89e3\u5f53\u4ee3CPU\u4e0d\u4ec5\u662f\u591a\u6838\u5fc3\uff0c\u800c\u4e14\u6bcf\u4e2a\u6838\u5fc3\u8fd8\u662f\u591a\u4efb\u52a1\uff08\u591a\u6307\u4ee4\uff09\u5e76\u884c\u7684\u3002\u8ba1\u7b97\u673a\u8bfe\u672c\u4e0a\u7684\u90a3\u79cd\u4e00\u4e2a\u65f6\u949f\u4e00\u6761\u6307\u4ee4\u7684\uff0c\u65e9\u5c31\u662f\u8001\u9ec4\u5386\u4e86 \uff08\u5f53\u7136\uff0c\u5b8f\u89c2\u6765\u770b\u57fa\u672c\u539f\u7406\u5e76\u6ca1\u6709\u6539\u53d8\uff09","title":"zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#processor#technologies#out-of-order","text":"\u5176\u4e2d\u603b\u7ed3\u4e86\u4e00\u4e9bout of order\u7684\u5185\u5bb9\u3002 1\u3001 Wide-issue \"wide issue\"\u5e94\u8be5\u5c31\u662f\"multiple issue\"\uff0c\u5373\u591a\u53d1\u5c04\u3002","title":"Processor technologies # Out-of-order"},{"location":"CPU/Execution-of-instruction/Out-of-order-execution/#umdedu#multiple#issue#processors#i","text":"NOTE: 1\u3001\u4ecb\u7ecd\u5730\u6bd4\u8f83\u8be6\u7ec6 Types of Multiple Issue Processors: There are basically two variations in multiple issue processors \u2013 Superscalar processors and VLIW (Very Long Instruction Word) processors. There are two types of superscalar processors that issue varying numbers of instructions per clock. They are statically scheduled superscalars that use in-order execution dynamically scheduled superscalars that use out-of-order execution","title":"umd.edu Multiple Issue Processors I"},{"location":"CPU/Execution-of-instruction/Speculative/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbaCPU Speculative execution\uff0c\u6211\u662f\u5728\u5b66\u4e60C-family language\u7684branch prediction optimization technique\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u7684\u8fd9\u4e2a\u4e3b\u9898\u3002 wikipedia Speculative execution NOTE: \"speculative\" \u7684\u610f\u601d\u662f\"\u6295\u673a\u7684\u3001\u601d\u7d22\u7684\u3001\u9884\u6d4b\u7684\" Speculative execution is an optimization technique where a computer system performs some task that may not be needed. Work is done before it is known whether it is actually needed, so as to prevent a delay that would have to be incurred by doing the work after it is known that it is needed. If it turns out the work was not needed after all, most changes made by the work are reverted and the results are ignored. The objective is to provide more concurrency if extra resources are available. NOTE: \u6b64\u5904\u5bf9\u5b83\u7684\u76ee\u7684\u662f\u603b\u7ed3\u662f: provide more concurrency This approach is employed in a variety of areas, including: 1 branch prediction in pipelined processors , 2 value prediction for exploiting value locality,[ 1] 3 prefetching memory and files , and 4 optimistic concurrency control in database systems .[ 2] [ 3] [ 4] 5 Speculative multithreading is a special case of speculative execution. NOTE: Speculative execution\u662f\u4e00\u79cd\u975e\u5e38\u91cd\u8981\u7684optimization technique\uff0c\u5b83\u7684\u76ee\u7684\u662f: \"provide more concurrency\"(\u53c2\u89c1\u4e0a\u9762\u7684\u63cf\u8ff0)\u3002 \u5b83\u5728computer science\u7684\u5404\u4e2afield\u4e2d\u90fd\u6709\u7740\u5e94\u7528\u3002 \u8fd9\u4f53\u73b0\u4e86: \"a technique/thought can be used in a variety of areas\"\uff0c\u5373 \"\u540c\u4e00\u4e2a\u601d\u60f3\uff0c\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\"\uff0c\u4ece\u4e0b\u9762\u7684 \"Variants\" \u7ae0\u8282\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u4e0d\u540c\u9886\u57df\u5bf9\u8fd9\u4e2a\u601d\u60f3\u8fdb\u884c\u8fd0\u7528\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u4f1a\u8fdb\u884c\u4e00\u5b9a\u7684\u8c03\u6574\uff0c\u56e0\u6b64\u4f1a\u4ea7\u751f\u65b0\u7684concept\u3002 Overview NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\u662f Branch predictor Variants Speculative computation was a related earlier concept.[ 6] Eager execution See also: Eager evaluation Predictive execution See also: Pipeline (computing) Main article: Branch predictor Common forms of this include branch predictors and memory dependence prediction . A generalized form is sometimes referred to as value prediction.[ 1] [ 8] NOTE: \u5728\u540e\u9762\u7ae0\u8282\u4f1a\u8fdb\u884c\u63cf\u8ff0 Related concepts Lazy execution Main article: Lazy evaluation Lazy execution is the opposite of eager execution, and does not involve speculation. \u7ecf\u5178\u4f8b\u5b50 \u6700\u80fd\u591f\u4f53\u73b0CPU Speculative execution \u7684\u4f8b\u5b50\u5c31\u662f: stackoverflow Why is processing a sorted array faster than processing an unsorted array? \uff0c\u5b83\u88ab\u6536\u5f55\u4e8e Branch-predictor \u7ae0\u8282\u3002","title":"Introduction"},{"location":"CPU/Execution-of-instruction/Speculative/#_1","text":"\u672c\u7ae0\u8ba8\u8bbaCPU Speculative execution\uff0c\u6211\u662f\u5728\u5b66\u4e60C-family language\u7684branch prediction optimization technique\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u7684\u8fd9\u4e2a\u4e3b\u9898\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Execution-of-instruction/Speculative/#wikipedia#speculative#execution","text":"NOTE: \"speculative\" \u7684\u610f\u601d\u662f\"\u6295\u673a\u7684\u3001\u601d\u7d22\u7684\u3001\u9884\u6d4b\u7684\" Speculative execution is an optimization technique where a computer system performs some task that may not be needed. Work is done before it is known whether it is actually needed, so as to prevent a delay that would have to be incurred by doing the work after it is known that it is needed. If it turns out the work was not needed after all, most changes made by the work are reverted and the results are ignored. The objective is to provide more concurrency if extra resources are available. NOTE: \u6b64\u5904\u5bf9\u5b83\u7684\u76ee\u7684\u662f\u603b\u7ed3\u662f: provide more concurrency This approach is employed in a variety of areas, including: 1 branch prediction in pipelined processors , 2 value prediction for exploiting value locality,[ 1] 3 prefetching memory and files , and 4 optimistic concurrency control in database systems .[ 2] [ 3] [ 4] 5 Speculative multithreading is a special case of speculative execution. NOTE: Speculative execution\u662f\u4e00\u79cd\u975e\u5e38\u91cd\u8981\u7684optimization technique\uff0c\u5b83\u7684\u76ee\u7684\u662f: \"provide more concurrency\"(\u53c2\u89c1\u4e0a\u9762\u7684\u63cf\u8ff0)\u3002 \u5b83\u5728computer science\u7684\u5404\u4e2afield\u4e2d\u90fd\u6709\u7740\u5e94\u7528\u3002 \u8fd9\u4f53\u73b0\u4e86: \"a technique/thought can be used in a variety of areas\"\uff0c\u5373 \"\u540c\u4e00\u4e2a\u601d\u60f3\uff0c\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\"\uff0c\u4ece\u4e0b\u9762\u7684 \"Variants\" \u7ae0\u8282\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u4e0d\u540c\u9886\u57df\u5bf9\u8fd9\u4e2a\u601d\u60f3\u8fdb\u884c\u8fd0\u7528\u7684\u65f6\u5019\uff0c\u5f80\u5f80\u4f1a\u8fdb\u884c\u4e00\u5b9a\u7684\u8c03\u6574\uff0c\u56e0\u6b64\u4f1a\u4ea7\u751f\u65b0\u7684concept\u3002","title":"wikipedia Speculative execution"},{"location":"CPU/Execution-of-instruction/Speculative/#overview","text":"NOTE: \u539f\u6587\u8fd9\u4e00\u6bb5\u6240\u63cf\u8ff0\u7684\u662f Branch predictor","title":"Overview"},{"location":"CPU/Execution-of-instruction/Speculative/#variants","text":"Speculative computation was a related earlier concept.[ 6]","title":"Variants"},{"location":"CPU/Execution-of-instruction/Speculative/#eager#execution","text":"See also: Eager evaluation","title":"Eager execution"},{"location":"CPU/Execution-of-instruction/Speculative/#predictive#execution","text":"See also: Pipeline (computing) Main article: Branch predictor Common forms of this include branch predictors and memory dependence prediction . A generalized form is sometimes referred to as value prediction.[ 1] [ 8] NOTE: \u5728\u540e\u9762\u7ae0\u8282\u4f1a\u8fdb\u884c\u63cf\u8ff0","title":"Predictive execution"},{"location":"CPU/Execution-of-instruction/Speculative/#related#concepts","text":"","title":"Related concepts"},{"location":"CPU/Execution-of-instruction/Speculative/#lazy#execution","text":"Main article: Lazy evaluation Lazy execution is the opposite of eager execution, and does not involve speculation.","title":"Lazy execution"},{"location":"CPU/Execution-of-instruction/Speculative/#_2","text":"\u6700\u80fd\u591f\u4f53\u73b0CPU Speculative execution \u7684\u4f8b\u5b50\u5c31\u662f: stackoverflow Why is processing a sorted array faster than processing an unsorted array? \uff0c\u5b83\u88ab\u6536\u5f55\u4e8e Branch-predictor \u7ae0\u8282\u3002","title":"\u7ecf\u5178\u4f8b\u5b50"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/","text":"Branch predictor \u5728C family programming\uff0c\u53ef\u4ee5\u5f00\u542f\u8fd9\u4e2aoptimization\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C++\\Language-reference\\Attribute\\Likely-and-unlikely \u7ae0\u8282\u3002 computerhope Branch prediction NOTE: \u89e3\u91ca\u5f97\u6bd4\u8f83\u597d Branch prediction is a technique used in CPU design that attempts to guess the outcome of a conditional operation and prepare for the most likely result. A digital circuit that performs this operation is known as a branch predictor . It is an important component of modern CPU architectures , such as the x86 . How does it work? When a conditional operation such as an if\u2026else statement needs to be processed, the branch predictor \"speculates\" which condition most likely will be met. It then executes the operations required by the most likely result ahead of time. This way, they're already complete if and when the guess was correct. At runtime , if the guess turns out not to be correct, the CPU executes the other branch of operation, incurring a slight delay. But if the guess was correct, speed is significantly increased. The first time a conditional operation is seen, the branch predictor does not have much information to use as the basis of a guess. But the more frequently the same operation is used, the more accurate its guess can become. wikipedia Branch predictor In computer architecture , a branch predictor [ 1] [ 2] [ 3] [ 4] [ 5] is a digital circuit that tries to guess which way a branch (e.g., an if\u2013then\u2013else structure ) will go before this is known definitively. The purpose of the branch predictor is to improve the flow in the instruction pipeline . Branch predictors play a critical role in achieving high effective performance in many modern pipelined microprocessor architectures[ 6] such as x86 . stackoverflow Why is processing a sorted array faster than processing an unsorted array? Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: #include <algorithm> #include <ctime> #include <iostream> int main () { // Generate data const unsigned arraySize = 32768 ; int data [ arraySize ]; for ( unsigned c = 0 ; c < arraySize ; ++ c ) data [ c ] = std :: rand () % 256 ; // !!! With this, the next loop runs faster. std :: sort ( data , data + arraySize ); // Test clock_t start = clock (); long long sum = 0 ; for ( unsigned i = 0 ; i < 100000 ; ++ i ) { // Primary loop for ( unsigned c = 0 ; c < arraySize ; ++ c ) { if ( data [ c ] >= 128 ) sum += data [ c ]; } } double elapsedTime = static_cast < double > ( clock () - start ) / CLOCKS_PER_SEC ; std :: cout << elapsedTime << std :: endl ; std :: cout << \"sum = \" << sum << std :: endl ; } // g++ --std=c++11 test.cpp 1\u3001Without std::sort(data, data + arraySize); , the code runs in 11.54 seconds. 2\u3001With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: import java.util.Arrays ; import java.util.Random ; public class Main { public static void main ( String [] args ) { // Generate data int arraySize = 32768 ; int data [] = new int [ arraySize ] ; Random rnd = new Random ( 0 ); for ( int c = 0 ; c < arraySize ; ++ c ) data [ c ] = rnd . nextInt () % 256 ; // !!! With this, the next loop runs faster Arrays . sort ( data ); // Test long start = System . nanoTime (); long sum = 0 ; for ( int i = 0 ; i < 100000 ; ++ i ) { // Primary loop for ( int c = 0 ; c < arraySize ; ++ c ) { if ( data [ c ] >= 128 ) sum += data [ c ] ; } } System . out . println (( System . nanoTime () - start ) / 1000000000.0 ); System . out . println ( \"sum = \" + sum ); } } With a similar but less extreme result. My first thought was that sorting brings the data into the cache , but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter. A You are a victim of branch prediction fail. What is Branch Prediction? NOTE: \u9996\u5148\u4ee5train\u901a\u8fc7railroad junction\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e Consider a railroad junction(\u4ea4\u53c9\u70b9): Image by Mecanismo, via Wikimedia Commons. Used under the CC-By-SA 3.0 license. Now for the sake of argument, suppose this is back in the 1800s - before long distance or radio communication. You are the operator of a junction(\u8fde\u63a5) and you hear a train coming. You have no idea which way it is supposed to go. You stop the train to ask the driver which direction they want. And then you set the switch appropriately. Trains are heavy and have a lot of inertia. So they take forever to start up and slow down. Is there a better way? You guess which direction the train will go! If you guessed right, it continues on. If you guessed wrong, the captain(\u961f\u957f) will stop, back up, and yell at you to flip the switch. Then it can restart down the other path. If you guess right every time , the train will never have to stop. If you guess wrong too often , the train will spend a lot of time stopping, backing up, and restarting. Consider an if-statement: At the processor level, it is a branch instruction : You are a processor and you see a branch. You have no idea which way it will go. What do you do? You halt execution and wait until the previous instructions are complete. Then you continue down the correct path. Modern processors are complicated and have long pipelines. So they take forever to \"warm up\" and \"slow down\". Is there a better way? You guess which direction the branch will go! If you guessed right, you continue executing. If you guessed wrong, you need to flush the pipeline and roll back to the branch. Then you can restart down the other path. If you guess right every time , the execution will never have to stop. If you guess wrong too often , you spend a lot of time stalling, rolling back, and restarting. This is branch prediction . I admit it's not the best analogy since the train could just signal the direction with a flag. But in computers, the processor doesn't know which direction a branch will go until the last moment. So how would you strategically guess to minimize the number of times that the train must back up and go down the other path? You look at the past history! If the train goes left 99% of the time, then you guess left. If it alternates, then you alternate your guesses. If it goes one way every three times, you guess the same... In other words, you try to identify a pattern and follow it. This is more or less how branch predictors work. Most applications have well-behaved branches. So modern branch predictors will typically achieve >90% hit rates. But when faced with unpredictable branches with no recognizable patterns, branch predictors are virtually useless. Further reading: \"Branch predictor\" article on Wikipedia . As hinted from above, the culprit(\u5143\u51f6) is this if-statement: if ( data [ c ] >= 128 ) sum += data [ c ]; Notice that the data is evenly distributed between 0 and 255. When the data is sorted, roughly the first half of the iterations will not enter the if-statement. After that, they will all enter the if-statement. This is very friendly to the branch predictor since the branch consecutively goes the same direction many times. Even a simple saturating counter will correctly predict the branch except for the few iterations after it switches direction. Quick visualization: T = branch taken N = branch not taken data[] = 0, 1, 2, 3, 4, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ... branch = N N N N N ... N N T T T ... T T T ... = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT (easy to predict) However, when the data is completely random, the branch predictor is rendered useless, because it can't predict random data. Thus there will probably be around 50% misprediction (no better than random guessing). data[] = 226, 185, 125, 158, 198, 144, 217, 79, 202, 118, 14, 150, 177, 182, 133, ... branch = T, T, N, T, T, T, T, N, T, N, N, T, T, T, N ... = TTNTTTTNTNNTTTN ... (completely random - hard to predict) So what can be done? If the compiler isn't able to optimize the branch into a conditional move, you can try some hacks if you are willing to sacrifice readability for performance. Replace: if (data[c] >= 128) sum += data[c]; with: int t = (data[c] - 128) >> 31; sum += ~t & data[c]; This eliminates the branch and replaces it with some bitwise operations. (Note that this hack is not strictly equivalent to the original if-statement. But in this case, it's valid for all the input values of data[] .) Benchmarks: Core i7 920 @ 3.5 GHz C++ - Visual Studio 2010 - x64 Release // Branch - Random seconds = 11.777 // Branch - Sorted seconds = 2.352 // Branchless - Random seconds = 2.564 // Branchless - Sorted seconds = 2.587 Java - NetBeans 7.1.1 JDK 7 - x64 // Branch - Random seconds = 10.93293813 // Branch - Sorted seconds = 5.643797077 // Branchless - Random seconds = 3.113581453 // Branchless - Sorted seconds = 3.186068823 Observations: With the Branch: There is a huge difference between the sorted and unsorted data. With the Hack: There is no difference between sorted and unsorted data. In the C++ case, the hack is actually a tad slower than with the branch when the data is sorted. A general rule of thumb is to avoid data-dependent branching in critical loops (such as in this example). Update: GCC 4.6.1 with -O3 or -ftree-vectorize on x64 is able to generate a conditional move. So there is no difference between the sorted and unsorted data - both are fast. (Or somewhat fast: for the already-sorted case, cmov can be slower especially if GCC puts it on the critical path instead of just add , especially on Intel before Broadwell where cmov has 2 cycle latency: gcc optimization flag -O3 makes code slower than -O2 ) VC++ 2010 is unable to generate conditional moves for this branch even under /Ox . Intel C++ Compiler (ICC) 11 does something miraculous. It interchanges the two loops , thereby hoisting the unpredictable branch to the outer loop. So not only is it immune to the mispredictions, it is also twice as fast as whatever VC++ and GCC can generate! In other words, ICC took advantage of the test-loop to defeat the benchmark... If you give the Intel compiler the branchless code, it just out-right vectorizes it... and is just as fast as with the branch (with the loop interchange). This goes to show that even mature modern compilers can vary wildly in their ability to optimize code...","title":"Introduction"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#branch#predictor","text":"\u5728C family programming\uff0c\u53ef\u4ee5\u5f00\u542f\u8fd9\u4e2aoptimization\uff0c\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C++\\Language-reference\\Attribute\\Likely-and-unlikely \u7ae0\u8282\u3002","title":"Branch predictor"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#computerhope#branch#prediction","text":"NOTE: \u89e3\u91ca\u5f97\u6bd4\u8f83\u597d Branch prediction is a technique used in CPU design that attempts to guess the outcome of a conditional operation and prepare for the most likely result. A digital circuit that performs this operation is known as a branch predictor . It is an important component of modern CPU architectures , such as the x86 .","title":"computerhope Branch prediction"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#how#does#it#work","text":"When a conditional operation such as an if\u2026else statement needs to be processed, the branch predictor \"speculates\" which condition most likely will be met. It then executes the operations required by the most likely result ahead of time. This way, they're already complete if and when the guess was correct. At runtime , if the guess turns out not to be correct, the CPU executes the other branch of operation, incurring a slight delay. But if the guess was correct, speed is significantly increased. The first time a conditional operation is seen, the branch predictor does not have much information to use as the basis of a guess. But the more frequently the same operation is used, the more accurate its guess can become.","title":"How does it work?"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#wikipedia#branch#predictor","text":"In computer architecture , a branch predictor [ 1] [ 2] [ 3] [ 4] [ 5] is a digital circuit that tries to guess which way a branch (e.g., an if\u2013then\u2013else structure ) will go before this is known definitively. The purpose of the branch predictor is to improve the flow in the instruction pipeline . Branch predictors play a critical role in achieving high effective performance in many modern pipelined microprocessor architectures[ 6] such as x86 .","title":"wikipedia Branch predictor"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#stackoverflow#why#is#processing#a#sorted#array#faster#than#processing#an#unsorted#array","text":"Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: #include <algorithm> #include <ctime> #include <iostream> int main () { // Generate data const unsigned arraySize = 32768 ; int data [ arraySize ]; for ( unsigned c = 0 ; c < arraySize ; ++ c ) data [ c ] = std :: rand () % 256 ; // !!! With this, the next loop runs faster. std :: sort ( data , data + arraySize ); // Test clock_t start = clock (); long long sum = 0 ; for ( unsigned i = 0 ; i < 100000 ; ++ i ) { // Primary loop for ( unsigned c = 0 ; c < arraySize ; ++ c ) { if ( data [ c ] >= 128 ) sum += data [ c ]; } } double elapsedTime = static_cast < double > ( clock () - start ) / CLOCKS_PER_SEC ; std :: cout << elapsedTime << std :: endl ; std :: cout << \"sum = \" << sum << std :: endl ; } // g++ --std=c++11 test.cpp 1\u3001Without std::sort(data, data + arraySize); , the code runs in 11.54 seconds. 2\u3001With the sorted data, the code runs in 1.93 seconds. Initially, I thought this might be just a language or compiler anomaly, so I tried Java: import java.util.Arrays ; import java.util.Random ; public class Main { public static void main ( String [] args ) { // Generate data int arraySize = 32768 ; int data [] = new int [ arraySize ] ; Random rnd = new Random ( 0 ); for ( int c = 0 ; c < arraySize ; ++ c ) data [ c ] = rnd . nextInt () % 256 ; // !!! With this, the next loop runs faster Arrays . sort ( data ); // Test long start = System . nanoTime (); long sum = 0 ; for ( int i = 0 ; i < 100000 ; ++ i ) { // Primary loop for ( int c = 0 ; c < arraySize ; ++ c ) { if ( data [ c ] >= 128 ) sum += data [ c ] ; } } System . out . println (( System . nanoTime () - start ) / 1000000000.0 ); System . out . println ( \"sum = \" + sum ); } } With a similar but less extreme result. My first thought was that sorting brings the data into the cache , but then I thought how silly that was because the array was just generated. What is going on? Why is processing a sorted array faster than processing an unsorted array? The code is summing up some independent terms, so the order should not matter.","title":"stackoverflow Why is processing a sorted array faster than processing an unsorted array?"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#a","text":"You are a victim of branch prediction fail.","title":"A"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#what#is#branch#prediction","text":"NOTE: \u9996\u5148\u4ee5train\u901a\u8fc7railroad junction\u4e3a\u4f8b\u6765\u8fdb\u884c\u8bf4\u660e Consider a railroad junction(\u4ea4\u53c9\u70b9): Image by Mecanismo, via Wikimedia Commons. Used under the CC-By-SA 3.0 license. Now for the sake of argument, suppose this is back in the 1800s - before long distance or radio communication. You are the operator of a junction(\u8fde\u63a5) and you hear a train coming. You have no idea which way it is supposed to go. You stop the train to ask the driver which direction they want. And then you set the switch appropriately. Trains are heavy and have a lot of inertia. So they take forever to start up and slow down. Is there a better way? You guess which direction the train will go! If you guessed right, it continues on. If you guessed wrong, the captain(\u961f\u957f) will stop, back up, and yell at you to flip the switch. Then it can restart down the other path. If you guess right every time , the train will never have to stop. If you guess wrong too often , the train will spend a lot of time stopping, backing up, and restarting. Consider an if-statement: At the processor level, it is a branch instruction : You are a processor and you see a branch. You have no idea which way it will go. What do you do? You halt execution and wait until the previous instructions are complete. Then you continue down the correct path. Modern processors are complicated and have long pipelines. So they take forever to \"warm up\" and \"slow down\". Is there a better way? You guess which direction the branch will go! If you guessed right, you continue executing. If you guessed wrong, you need to flush the pipeline and roll back to the branch. Then you can restart down the other path. If you guess right every time , the execution will never have to stop. If you guess wrong too often , you spend a lot of time stalling, rolling back, and restarting. This is branch prediction . I admit it's not the best analogy since the train could just signal the direction with a flag. But in computers, the processor doesn't know which direction a branch will go until the last moment. So how would you strategically guess to minimize the number of times that the train must back up and go down the other path? You look at the past history! If the train goes left 99% of the time, then you guess left. If it alternates, then you alternate your guesses. If it goes one way every three times, you guess the same... In other words, you try to identify a pattern and follow it. This is more or less how branch predictors work. Most applications have well-behaved branches. So modern branch predictors will typically achieve >90% hit rates. But when faced with unpredictable branches with no recognizable patterns, branch predictors are virtually useless. Further reading: \"Branch predictor\" article on Wikipedia .","title":"What is Branch Prediction?"},{"location":"CPU/Execution-of-instruction/Speculative/Branch-predictor/#as#hinted#from#above#the#culprit#is#this#if-statement","text":"if ( data [ c ] >= 128 ) sum += data [ c ]; Notice that the data is evenly distributed between 0 and 255. When the data is sorted, roughly the first half of the iterations will not enter the if-statement. After that, they will all enter the if-statement. This is very friendly to the branch predictor since the branch consecutively goes the same direction many times. Even a simple saturating counter will correctly predict the branch except for the few iterations after it switches direction. Quick visualization: T = branch taken N = branch not taken data[] = 0, 1, 2, 3, 4, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ... branch = N N N N N ... N N T T T ... T T T ... = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT (easy to predict) However, when the data is completely random, the branch predictor is rendered useless, because it can't predict random data. Thus there will probably be around 50% misprediction (no better than random guessing). data[] = 226, 185, 125, 158, 198, 144, 217, 79, 202, 118, 14, 150, 177, 182, 133, ... branch = T, T, N, T, T, T, T, N, T, N, N, T, T, T, N ... = TTNTTTTNTNNTTTN ... (completely random - hard to predict) So what can be done? If the compiler isn't able to optimize the branch into a conditional move, you can try some hacks if you are willing to sacrifice readability for performance. Replace: if (data[c] >= 128) sum += data[c]; with: int t = (data[c] - 128) >> 31; sum += ~t & data[c]; This eliminates the branch and replaces it with some bitwise operations. (Note that this hack is not strictly equivalent to the original if-statement. But in this case, it's valid for all the input values of data[] .) Benchmarks: Core i7 920 @ 3.5 GHz C++ - Visual Studio 2010 - x64 Release // Branch - Random seconds = 11.777 // Branch - Sorted seconds = 2.352 // Branchless - Random seconds = 2.564 // Branchless - Sorted seconds = 2.587 Java - NetBeans 7.1.1 JDK 7 - x64 // Branch - Random seconds = 10.93293813 // Branch - Sorted seconds = 5.643797077 // Branchless - Random seconds = 3.113581453 // Branchless - Sorted seconds = 3.186068823 Observations: With the Branch: There is a huge difference between the sorted and unsorted data. With the Hack: There is no difference between sorted and unsorted data. In the C++ case, the hack is actually a tad slower than with the branch when the data is sorted. A general rule of thumb is to avoid data-dependent branching in critical loops (such as in this example). Update: GCC 4.6.1 with -O3 or -ftree-vectorize on x64 is able to generate a conditional move. So there is no difference between the sorted and unsorted data - both are fast. (Or somewhat fast: for the already-sorted case, cmov can be slower especially if GCC puts it on the critical path instead of just add , especially on Intel before Broadwell where cmov has 2 cycle latency: gcc optimization flag -O3 makes code slower than -O2 ) VC++ 2010 is unable to generate conditional moves for this branch even under /Ox . Intel C++ Compiler (ICC) 11 does something miraculous. It interchanges the two loops , thereby hoisting the unpredictable branch to the outer loop. So not only is it immune to the mispredictions, it is also twice as fast as whatever VC++ and GCC can generate! In other words, ICC took advantage of the test-loop to defeat the benchmark... If you give the Intel compiler the branchless code, it just out-right vectorizes it... and is just as fast as with the branch (with the loop interchange). This goes to show that even mature modern compilers can vary wildly in their ability to optimize code...","title":"As hinted from above, the culprit(\u5143\u51f6) is this if-statement:"},{"location":"CPU/Execution-of-instruction/Speculative/Memory-dependence-prediction/","text":"Memory dependence prediction wikipedia Memory dependence prediction Memory dependence prediction is a technique, employed by high-performance out-of-order execution microprocessors that execute memory access operations (loads and stores) out of program order, to predict true dependencies between loads and stores at instruction execution time. With the predicted dependence information, the processor can then decide to speculatively execute certain loads and stores out of order, while preventing other loads and stores from executing out-of-order (keeping them in-order). Later in the pipeline , memory disambiguation techniques are used to determine if the loads and stores were correctly executed and, if not, to recover.","title":"Introduction"},{"location":"CPU/Execution-of-instruction/Speculative/Memory-dependence-prediction/#memory#dependence#prediction","text":"","title":"Memory dependence prediction"},{"location":"CPU/Execution-of-instruction/Speculative/Memory-dependence-prediction/#wikipedia#memory#dependence#prediction","text":"Memory dependence prediction is a technique, employed by high-performance out-of-order execution microprocessors that execute memory access operations (loads and stores) out of program order, to predict true dependencies between loads and stores at instruction execution time. With the predicted dependence information, the processor can then decide to speculatively execute certain loads and stores out of order, while preventing other loads and stores from executing out-of-order (keeping them in-order). Later in the pipeline , memory disambiguation techniques are used to determine if the loads and stores were correctly executed and, if not, to recover.","title":"wikipedia Memory dependence prediction"},{"location":"CPU/Instruction-set-architectures/","text":"Instruction set architecture wikipedia Instruction set architecture An instruction set architecture ( ISA ) is an abstract model of a computer . It is also referred to as architecture or computer architecture . A realization of an ISA is called an implementation . An ISA permits multiple implementations that may vary in performance , physical size, and monetary cost (among other things); because the ISA serves as the interface between software and hardware . Software that has been written for an ISA can run on different implementations of the same ISA. This has enabled binary compatibility between different generations of computers to be easily achieved, and the development of computer families . Both of these developments have helped to lower the cost of computers and to increase their applicability. For these reasons, the ISA is one of the most important abstractions in computing today. NOTE: \u8fd9\u6bb5\u8bdd\u5728\u300a\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-4.1-\u6307\u4ee4\u7cfb\u7edf\u7684\u53d1\u5c55\u548c\u6027\u80fd\u8981\u6c42\u300b\u4e2d\u4e5f\u6709\u63d0\u53ca\u3002ISA\u548c**\u7cfb\u5217\u8ba1\u7b97\u673a**\u7684\u6982\u5ff5\u5bc6\u5207\u76f8\u5173\u3002 An ISA defines everything a machine language programmer needs to know in order to program a computer. What an ISA defines differs between ISAs; in general, ISAs define the supported data types , what state there is (such as the main memory and registers ) and their semantics (such as the memory consistency and addressing modes ), the instruction set (the set of machine instructions that comprises a computer's machine language), and the input/output model. An ISA specifies the behavior of machine code running on implementations of that ISA in a fashion that does not depend on the characteristics of that implementation, providing binary compatibility between implementations. This enables multiple implementations of an ISA that differ in performance , physical size, and monetary cost (among other things), but that are capable of running the same machine code, so that a lower-performance, lower-cost machine can be replaced with a higher-cost, higher-performance machine without having to replace software. It also enables the evolution of the microarchitectures of the implementations of that ISA, so that a newer, higher-performance implementation of an ISA can run software that runs on previous generations of implementations. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u5206\u79bb\u62bd\u8c61\u4e0e\u5b9e\u73b0\u5e26\u6765\u7684\u597d\u5904\u3002 If an operating system maintains a standard and compatible application binary interface (ABI) for a particular ISA, machine code for that ISA and operating system will run on future implementations of that ISA and newer versions of that operating system. However, if an ISA supports running multiple operating systems, it does not guarantee that machine code for one operating system will run on another operating system, unless the first operating system supports running machine code built for the other operating system. An ISA can be extended by adding instructions or other capabilities, or adding support for larger addresses and data values; an implementation of the extended ISA will still be able to execute machine code for versions of the ISA without those extensions. Machine code using those extensions will only run on implementations that support those extensions. The binary compatibility that they provide make ISAs one of the most fundamental abstractions in computing . Example \u4e0b\u9762\u7f57\u5217\u4e86\u4e00\u4e9b\u6bd4\u8f83\u5e38\u89c1\u7684ISA: x86 ARM MIPS Power ISA SPARC Itanium Classification of ISAs complex instruction set computer (CISC) reduced instruction set computer (RISC) Instructions Machine language is built up from discrete statements or instructions . On the processing architecture, a given instruction may specify: particular registers (for arithmetic, addressing, or control functions) particular memory locations (or offsets to them) particular addressing modes (used to interpret the operands) More complex operations are built up by combining these simple instructions, which are executed sequentially, or as otherwise directed by control flow instructions. Instruction types Data handling and memory operations Arithmetic and logic operations NOTE: \u8fd9\u662f\u6700\u6700\u57fa\u672c\u7684\u6307\u4ee4 Control flow operations NOTE: \u63a7\u5236\u6d41\u6307\u4ee4 Coprocessor instructions wikipedia Machine code NOTE: \u673a\u5668\u7801\uff0c\u90fd\u662f01\u3002 Machine code is a strictly numerical language which is intended to run as fast as possible","title":"Introduction"},{"location":"CPU/Instruction-set-architectures/#instruction#set#architecture","text":"","title":"Instruction set architecture"},{"location":"CPU/Instruction-set-architectures/#wikipedia#instruction#set#architecture","text":"An instruction set architecture ( ISA ) is an abstract model of a computer . It is also referred to as architecture or computer architecture . A realization of an ISA is called an implementation . An ISA permits multiple implementations that may vary in performance , physical size, and monetary cost (among other things); because the ISA serves as the interface between software and hardware . Software that has been written for an ISA can run on different implementations of the same ISA. This has enabled binary compatibility between different generations of computers to be easily achieved, and the development of computer families . Both of these developments have helped to lower the cost of computers and to increase their applicability. For these reasons, the ISA is one of the most important abstractions in computing today. NOTE: \u8fd9\u6bb5\u8bdd\u5728\u300a\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-4.1-\u6307\u4ee4\u7cfb\u7edf\u7684\u53d1\u5c55\u548c\u6027\u80fd\u8981\u6c42\u300b\u4e2d\u4e5f\u6709\u63d0\u53ca\u3002ISA\u548c**\u7cfb\u5217\u8ba1\u7b97\u673a**\u7684\u6982\u5ff5\u5bc6\u5207\u76f8\u5173\u3002 An ISA defines everything a machine language programmer needs to know in order to program a computer. What an ISA defines differs between ISAs; in general, ISAs define the supported data types , what state there is (such as the main memory and registers ) and their semantics (such as the memory consistency and addressing modes ), the instruction set (the set of machine instructions that comprises a computer's machine language), and the input/output model. An ISA specifies the behavior of machine code running on implementations of that ISA in a fashion that does not depend on the characteristics of that implementation, providing binary compatibility between implementations. This enables multiple implementations of an ISA that differ in performance , physical size, and monetary cost (among other things), but that are capable of running the same machine code, so that a lower-performance, lower-cost machine can be replaced with a higher-cost, higher-performance machine without having to replace software. It also enables the evolution of the microarchitectures of the implementations of that ISA, so that a newer, higher-performance implementation of an ISA can run software that runs on previous generations of implementations. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u63cf\u8ff0\u7684\u5176\u5b9e\u662f\u5206\u79bb\u62bd\u8c61\u4e0e\u5b9e\u73b0\u5e26\u6765\u7684\u597d\u5904\u3002 If an operating system maintains a standard and compatible application binary interface (ABI) for a particular ISA, machine code for that ISA and operating system will run on future implementations of that ISA and newer versions of that operating system. However, if an ISA supports running multiple operating systems, it does not guarantee that machine code for one operating system will run on another operating system, unless the first operating system supports running machine code built for the other operating system. An ISA can be extended by adding instructions or other capabilities, or adding support for larger addresses and data values; an implementation of the extended ISA will still be able to execute machine code for versions of the ISA without those extensions. Machine code using those extensions will only run on implementations that support those extensions. The binary compatibility that they provide make ISAs one of the most fundamental abstractions in computing .","title":"wikipedia Instruction set architecture"},{"location":"CPU/Instruction-set-architectures/#example","text":"\u4e0b\u9762\u7f57\u5217\u4e86\u4e00\u4e9b\u6bd4\u8f83\u5e38\u89c1\u7684ISA: x86 ARM MIPS Power ISA SPARC Itanium","title":"Example"},{"location":"CPU/Instruction-set-architectures/#classification#of#isas","text":"complex instruction set computer (CISC) reduced instruction set computer (RISC)","title":"Classification of ISAs"},{"location":"CPU/Instruction-set-architectures/#instructions","text":"Machine language is built up from discrete statements or instructions . On the processing architecture, a given instruction may specify: particular registers (for arithmetic, addressing, or control functions) particular memory locations (or offsets to them) particular addressing modes (used to interpret the operands) More complex operations are built up by combining these simple instructions, which are executed sequentially, or as otherwise directed by control flow instructions.","title":"Instructions"},{"location":"CPU/Instruction-set-architectures/#instruction#types","text":"","title":"Instruction types"},{"location":"CPU/Instruction-set-architectures/#data#handling#and#memory#operations","text":"","title":"Data handling and memory operations"},{"location":"CPU/Instruction-set-architectures/#arithmetic#and#logic#operations","text":"NOTE: \u8fd9\u662f\u6700\u6700\u57fa\u672c\u7684\u6307\u4ee4","title":"Arithmetic and logic operations"},{"location":"CPU/Instruction-set-architectures/#control#flow#operations","text":"NOTE: \u63a7\u5236\u6d41\u6307\u4ee4","title":"Control flow operations"},{"location":"CPU/Instruction-set-architectures/#coprocessor#instructions","text":"","title":"Coprocessor instructions"},{"location":"CPU/Instruction-set-architectures/#wikipedia#machine#code","text":"NOTE: \u673a\u5668\u7801\uff0c\u90fd\u662f01\u3002 Machine code is a strictly numerical language which is intended to run as fast as possible","title":"wikipedia Machine code"},{"location":"CPU/Instruction-set-architectures/Comparison-of-instruction-set-architectures/","text":"Comparison of instruction set architectures An instruction set architecture ( ISA ) is an abstract model of a computer . It is also referred to as architecture or computer architecture . A realization of an ISA is called an implementation . An ISA permits multiple implementations that may vary in performance , physical size, and monetary cost (among other things); because the ISA serves as the interface between software and hardware . Software that has been written for an ISA can run on different implementations of the same ISA. This has enabled binary compatibility between different generations of computers to be easily achieved, and the development of computer families. Both of these developments have helped to lower the cost of computers and to increase their applicability. For these reasons, the ISA is one of the most important abstractions in computing today. An ISA defines everything a machine language programmer needs to know in order to program a computer. What an ISA defines differs between ISAs; in general, ISAs define the supported data types , what state there is (such as the main memory and registers ) and their semantics (such as the memory consistency and addressing modes ), the instruction set (the set of machine instructions that comprises a computer's machine language), and the input/output model. Base In the early decades of computing, there were computers that used binary , decimal [ 1] and even ternary .[ 2] [ 3] Contemporary computers are almost exclusively binary. Bits Computer architectures are often described as n - bit architectures. Today n is often 8, 16, 32, or 64, but other sizes have been used. This is actually a strong simplification. A computer architecture often has a few more or less \"natural\" datasizes in the instruction set , but the hardware implementation of these may be very different. Many architectures have instructions operating on half and/or twice the size of respective processors' major internal datapaths. Examples of this are the 8080 , Z80 , MC68000 as well as many others. On this type of implementations, a twice as wide operation typically also takes around twice as many clock cycles (which is not the case on high performance implementations). On the 68000, for instance, this means 8 instead of 4 clock ticks, and this particular chip may be described as a 32-bit architecture with a 16-bit implementation. The external databus width is often not useful to determine the width of the architecture; the NS32008, NS32016 and NS32032 were basically the same 32-bit chip with different external data buses. The NS32764 had a 64-bit bus, but used 32-bit registers. The width of addresses may or may not be different from the width of data. Early 32-bit microprocessors often had a 24-bit address, as did the System/360 processors. Operands Main article: instruction set \u00a7 Number of operands The number of operands is one of the factors that may give an indication about the performance of the instruction set. A three-operand architecture will allow A := B + C to be computed in one instruction. A two-operand architecture will allow A := A + B to be computed in one instruction, so two instructions will need to be executed to simulate a single three-operand instruction A := B A := A + C Endianness An architecture may use \"big\" or \"little\" endianness, or both, or be configurable to use either. Little endian processors order bytes in memory with the least significant byte of a multi-byte value in the lowest-numbered memory location. Big endian architectures instead order them with the most significant byte at the lowest-numbered address. The x86 architecture as well as several 8-bit architectures are little endian. Most RISC architectures (SPARC, Power, PowerPC, MIPS) were originally big endian (ARM was little endian), but many (including ARM) are now configurable. Endianness only applies to processors that allow individual addressing of units of data (such as bytes ) that are smaller than the basic addressable machine word. Instruction sets Usually the number of registers is a power of two, e.g. 8, 16, 32. In some cases a hardwired-to-zero pseudo-register is included, as \"part\" of register files of architectures, mostly to simplify indexing modes. This table only counts the integer \"registers\" usable by general instructions at any moment. Architectures always include special-purpose registers such as the program pointer (PC). Those are not counted unless mentioned. Note that some architectures, such as SPARC, have register window ; for those architectures, the count below indicates how many registers are available within a register window. Also, non-architected registers for register renaming are not counted. Note, a common type of architecture, \"load-store\", is a synonym for \"Register Register\" below, meaning no instructions access memory except special \u2013 load to register(s) \u2013 and store from register(s) \u2013 with the possible exceptions of atomic memory operations for locking. The table below compares basic information about instruction sets to be implemented in the CPU architectures: NOTE: \u8fd9\u4e9b\u4e0d\u540c\u7684instruction set\u5f80\u5f80\u7528\u4e8e\u4e0d\u540c\u7684\u9886\u57df\uff0c\u5982 arm\u5f80\u5f80\u5728\u624b\u673a\u3001\u667a\u80fd\u79fb\u52a8\u8bbe\u5907\u4e2d \u3002","title":"Comparison-of-instruction-set-architectures"},{"location":"CPU/Instruction-set-architectures/Comparison-of-instruction-set-architectures/#comparison#of#instruction#set#architectures","text":"An instruction set architecture ( ISA ) is an abstract model of a computer . It is also referred to as architecture or computer architecture . A realization of an ISA is called an implementation . An ISA permits multiple implementations that may vary in performance , physical size, and monetary cost (among other things); because the ISA serves as the interface between software and hardware . Software that has been written for an ISA can run on different implementations of the same ISA. This has enabled binary compatibility between different generations of computers to be easily achieved, and the development of computer families. Both of these developments have helped to lower the cost of computers and to increase their applicability. For these reasons, the ISA is one of the most important abstractions in computing today. An ISA defines everything a machine language programmer needs to know in order to program a computer. What an ISA defines differs between ISAs; in general, ISAs define the supported data types , what state there is (such as the main memory and registers ) and their semantics (such as the memory consistency and addressing modes ), the instruction set (the set of machine instructions that comprises a computer's machine language), and the input/output model.","title":"Comparison of instruction set architectures"},{"location":"CPU/Instruction-set-architectures/Comparison-of-instruction-set-architectures/#base","text":"In the early decades of computing, there were computers that used binary , decimal [ 1] and even ternary .[ 2] [ 3] Contemporary computers are almost exclusively binary.","title":"Base"},{"location":"CPU/Instruction-set-architectures/Comparison-of-instruction-set-architectures/#bits","text":"Computer architectures are often described as n - bit architectures. Today n is often 8, 16, 32, or 64, but other sizes have been used. This is actually a strong simplification. A computer architecture often has a few more or less \"natural\" datasizes in the instruction set , but the hardware implementation of these may be very different. Many architectures have instructions operating on half and/or twice the size of respective processors' major internal datapaths. Examples of this are the 8080 , Z80 , MC68000 as well as many others. On this type of implementations, a twice as wide operation typically also takes around twice as many clock cycles (which is not the case on high performance implementations). On the 68000, for instance, this means 8 instead of 4 clock ticks, and this particular chip may be described as a 32-bit architecture with a 16-bit implementation. The external databus width is often not useful to determine the width of the architecture; the NS32008, NS32016 and NS32032 were basically the same 32-bit chip with different external data buses. The NS32764 had a 64-bit bus, but used 32-bit registers. The width of addresses may or may not be different from the width of data. Early 32-bit microprocessors often had a 24-bit address, as did the System/360 processors.","title":"Bits"},{"location":"CPU/Instruction-set-architectures/Comparison-of-instruction-set-architectures/#operands","text":"Main article: instruction set \u00a7 Number of operands The number of operands is one of the factors that may give an indication about the performance of the instruction set. A three-operand architecture will allow A := B + C to be computed in one instruction. A two-operand architecture will allow A := A + B to be computed in one instruction, so two instructions will need to be executed to simulate a single three-operand instruction A := B A := A + C","title":"Operands"},{"location":"CPU/Instruction-set-architectures/Comparison-of-instruction-set-architectures/#endianness","text":"An architecture may use \"big\" or \"little\" endianness, or both, or be configurable to use either. Little endian processors order bytes in memory with the least significant byte of a multi-byte value in the lowest-numbered memory location. Big endian architectures instead order them with the most significant byte at the lowest-numbered address. The x86 architecture as well as several 8-bit architectures are little endian. Most RISC architectures (SPARC, Power, PowerPC, MIPS) were originally big endian (ARM was little endian), but many (including ARM) are now configurable. Endianness only applies to processors that allow individual addressing of units of data (such as bytes ) that are smaller than the basic addressable machine word.","title":"Endianness"},{"location":"CPU/Instruction-set-architectures/Comparison-of-instruction-set-architectures/#instruction#sets","text":"Usually the number of registers is a power of two, e.g. 8, 16, 32. In some cases a hardwired-to-zero pseudo-register is included, as \"part\" of register files of architectures, mostly to simplify indexing modes. This table only counts the integer \"registers\" usable by general instructions at any moment. Architectures always include special-purpose registers such as the program pointer (PC). Those are not counted unless mentioned. Note that some architectures, such as SPARC, have register window ; for those architectures, the count below indicates how many registers are available within a register window. Also, non-architected registers for register renaming are not counted. Note, a common type of architecture, \"load-store\", is a synonym for \"Register Register\" below, meaning no instructions access memory except special \u2013 load to register(s) \u2013 and store from register(s) \u2013 with the possible exceptions of atomic memory operations for locking. The table below compares basic information about instruction sets to be implemented in the CPU architectures: NOTE: \u8fd9\u4e9b\u4e0d\u540c\u7684instruction set\u5f80\u5f80\u7528\u4e8e\u4e0d\u540c\u7684\u9886\u57df\uff0c\u5982 arm\u5f80\u5f80\u5728\u624b\u673a\u3001\u667a\u80fd\u79fb\u52a8\u8bbe\u5907\u4e2d \u3002","title":"Instruction sets"},{"location":"CPU/Instruction-set-architectures/Classification/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u6240\u4ecb\u7ecd\u7684\u4e24\u79cd\u5206\u7c7b\u5176\u5b9e\u662f\u57fa\u4e8ememory access\u7684\u3002","title":"Introduction"},{"location":"CPU/Instruction-set-architectures/Classification/#_1","text":"\u672c\u7ae0\u6240\u4ecb\u7ecd\u7684\u4e24\u79cd\u5206\u7c7b\u5176\u5b9e\u662f\u57fa\u4e8ememory access\u7684\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Instruction-set-architectures/Classification/Load%E2%80%93store-architecture/","text":"Load\u2013store architecture load \u5bf9\u5e94\u7684\u662fread data from memory\uff1b store \u5bf9\u5e94\u7684\u662fwrite data to memory\uff1b wikipedia Load\u2013store architecture In computer engineering , a load\u2013store architecture is an instruction set architecture that divides instructions into two categories: memory access ( load and store between memory and registers ), and ALU operations (which only occur between registers).[ 1] :9-12 NOTE: \u663e\u7136\uff0c\u5728 load\u2013store architecture \u4e2d\uff0c\u6240\u6709\u7684operand\u5fc5\u987b\u8981\u5148load\u5230ALU\u4e2d\uff0cALU\u5728\u6267\u884c\u6307\u4ee4\u7684\u8fc7\u7a0b\u4e2d\uff0c\u662f\u4e0d\u4f1aaccess memory\u7684\uff1b\u5728ALU\u8fd0\u7b97\u5b8c\u6210\u540e\uff0c\u5c31\u5c06\u8ba1\u7b97\u7684\u7ed3\u679cstore\u5230memory\u4e2d\uff1b RISC instruction set architectures such as PowerPC , SPARC , RISC-V , ARM , and MIPS are load\u2013store architectures .[ 1] :9\u201312 For instance, in a load\u2013store approach both operands and destination for an ADD operation must be in registers . This differs from a register\u2013memory architecture (for example, a CISC instruction set architecture such as x86 ) in which one of the operands for the ADD operation may be in memory, while the other is in a register.[ 1] :9\u201312 The earliest example of a load\u2013store architecture was the CDC 6600 .[ 1] :54\u201356 Almost all vector processors (including many GPUs [ 2] ) use the load\u2013store approach.[ 3] See also Load\u2013store unit Register memory architecture chortle Load and Store The operands for all arithmetic and logic operations are contained in registers. To operate on data in main memory, the data is first copied into registers. A load operation copies data from main memory into a register. A store operation copies data from a register into main memory . When a word (4 bytes) is loaded or stored the memory address must be a multiple of four. This is called an alignment restriction. Addresses that are a multiple of four are called word aligned . This restriction makes the hardware simpler and faster. The lw instruction loads a word into a register from memory. The sw instruction stores a word from a register into memory. Each instruction specifies a register and a memory address (details in a few pages). reading list https://azeria-labs.com/memory-instructions-load-and-store-part-4/","title":"Introduction"},{"location":"CPU/Instruction-set-architectures/Classification/Load%E2%80%93store-architecture/#loadstore#architecture","text":"load \u5bf9\u5e94\u7684\u662fread data from memory\uff1b store \u5bf9\u5e94\u7684\u662fwrite data to memory\uff1b","title":"Load\u2013store architecture"},{"location":"CPU/Instruction-set-architectures/Classification/Load%E2%80%93store-architecture/#wikipedia#loadstore#architecture","text":"In computer engineering , a load\u2013store architecture is an instruction set architecture that divides instructions into two categories: memory access ( load and store between memory and registers ), and ALU operations (which only occur between registers).[ 1] :9-12 NOTE: \u663e\u7136\uff0c\u5728 load\u2013store architecture \u4e2d\uff0c\u6240\u6709\u7684operand\u5fc5\u987b\u8981\u5148load\u5230ALU\u4e2d\uff0cALU\u5728\u6267\u884c\u6307\u4ee4\u7684\u8fc7\u7a0b\u4e2d\uff0c\u662f\u4e0d\u4f1aaccess memory\u7684\uff1b\u5728ALU\u8fd0\u7b97\u5b8c\u6210\u540e\uff0c\u5c31\u5c06\u8ba1\u7b97\u7684\u7ed3\u679cstore\u5230memory\u4e2d\uff1b RISC instruction set architectures such as PowerPC , SPARC , RISC-V , ARM , and MIPS are load\u2013store architectures .[ 1] :9\u201312 For instance, in a load\u2013store approach both operands and destination for an ADD operation must be in registers . This differs from a register\u2013memory architecture (for example, a CISC instruction set architecture such as x86 ) in which one of the operands for the ADD operation may be in memory, while the other is in a register.[ 1] :9\u201312 The earliest example of a load\u2013store architecture was the CDC 6600 .[ 1] :54\u201356 Almost all vector processors (including many GPUs [ 2] ) use the load\u2013store approach.[ 3]","title":"wikipedia Load\u2013store architecture"},{"location":"CPU/Instruction-set-architectures/Classification/Load%E2%80%93store-architecture/#see#also","text":"Load\u2013store unit Register memory architecture","title":"See also"},{"location":"CPU/Instruction-set-architectures/Classification/Load%E2%80%93store-architecture/#chortle#load#and#store","text":"The operands for all arithmetic and logic operations are contained in registers. To operate on data in main memory, the data is first copied into registers. A load operation copies data from main memory into a register. A store operation copies data from a register into main memory . When a word (4 bytes) is loaded or stored the memory address must be a multiple of four. This is called an alignment restriction. Addresses that are a multiple of four are called word aligned . This restriction makes the hardware simpler and faster. The lw instruction loads a word into a register from memory. The sw instruction stores a word from a register into memory. Each instruction specifies a register and a memory address (details in a few pages).","title":"chortle Load and Store"},{"location":"CPU/Instruction-set-architectures/Classification/Load%E2%80%93store-architecture/#reading#list","text":"https://azeria-labs.com/memory-instructions-load-and-store-part-4/","title":"reading list"},{"location":"CPU/Instruction-set-architectures/Classification/Register-memory-architecture/","text":"Register memory architecture wikipedia Register memory architecture In computer engineering , a register\u2013memory architecture is an instruction set architecture that allows operations to be performed on (or from) memory, as well as registers .[ 1] If the architecture allows all operands to be in memory or in registers, or in combinations, it is called a \"register plus memory\" architecture.[ 1] In a register\u2013memory approach one of the operands for ADD operation may be in memory, while the other is in a register. This differs from a load/store architecture (used by RISC designs such as MIPS ) in which both operands for an ADD operation must be in registers before the ADD.[ 1] Examples of register memory architecture are IBM System/360 , its successors, and Intel x86 .[ 1] Examples of register plus memory architecture are VAX and the Motorola 68000 family .[ 1] See also Load/store architecture Addressing mode","title":"Introduction"},{"location":"CPU/Instruction-set-architectures/Classification/Register-memory-architecture/#register#memory#architecture","text":"","title":"Register memory architecture"},{"location":"CPU/Instruction-set-architectures/Classification/Register-memory-architecture/#wikipedia#register#memory#architecture","text":"In computer engineering , a register\u2013memory architecture is an instruction set architecture that allows operations to be performed on (or from) memory, as well as registers .[ 1] If the architecture allows all operands to be in memory or in registers, or in combinations, it is called a \"register plus memory\" architecture.[ 1] In a register\u2013memory approach one of the operands for ADD operation may be in memory, while the other is in a register. This differs from a load/store architecture (used by RISC designs such as MIPS ) in which both operands for an ADD operation must be in registers before the ADD.[ 1] Examples of register memory architecture are IBM System/360 , its successors, and Intel x86 .[ 1] Examples of register plus memory architecture are VAX and the Motorola 68000 family .[ 1]","title":"wikipedia Register memory architecture"},{"location":"CPU/Instruction-set-architectures/Classification/Register-memory-architecture/#see#also","text":"Load/store architecture Addressing mode","title":"See also"},{"location":"CPU/Manufacturer/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bba\u5e73\u65f6\u975e\u5e38\u5e38\u89c1\u7684ISA \u5382\u5bb6\u3002","title":"Introduction"},{"location":"CPU/Manufacturer/#_1","text":"\u672c\u7ae0\u8ba8\u8bba\u5e73\u65f6\u975e\u5e38\u5e38\u89c1\u7684ISA \u5382\u5bb6\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Manufacturer/ARM/ARM-Assembly-Basics/","text":"ARM assembly azeria ARM Assembly","title":"ARM-Assembly-Basics"},{"location":"CPU/Manufacturer/ARM/ARM-Assembly-Basics/#arm#assembly","text":"","title":"ARM assembly"},{"location":"CPU/Manufacturer/ARM/ARM-Assembly-Basics/#azeria#arm#assembly","text":"","title":"azeria ARM Assembly"},{"location":"CPU/Manufacturer/ARM/ARM-architecture/","text":"ARM architecture ARM architecture","title":"ARM-architecture"},{"location":"CPU/Manufacturer/ARM/ARM-architecture/#arm#architecture","text":"","title":"ARM architecture"},{"location":"CPU/Manufacturer/ARM/ARM-architecture/#arm#architecture_1","text":"","title":"ARM architecture"},{"location":"CPU/Manufacturer/Intel/","text":"Intel Resource & Design Center Intel 8086","title":"Introduction"},{"location":"CPU/Manufacturer/Intel/#intel#resource#design#center","text":"","title":"Intel Resource &amp; Design Center"},{"location":"CPU/Manufacturer/Intel/#intel#8086","text":"","title":"Intel 8086"},{"location":"CPU/Manufacturer/Intel/Register/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u63cf\u8ff0Intel CPU\u7684register\u3002Intel CPU\u662f\u5728\u4e0d\u65ad\u6f14\u8fdb\u4e2d\u7684\uff0c\u6211\u4eec\u4ee5\u76f8\u5bf9\u7b80\u5355\u7684x86\u4e3a\u4f8b\u6765\u8fdb\u884c\u63cf\u8ff0\u3002 \u5165\u95e8\u8bfb\u7269: skullsecurity Registers This section is the first section specific to assembly. So if you're reading through the full guide, get ready for some actual learning! A register is like a variable, except that there are a fixed number of registers. Each register is a special spot in the CPU where a single value is stored. A register is the only place where math can be done ( addition , subtraction , etc). Registers frequently hold pointers which reference memory. Movement of values between registers and memory is very common. Intel assembly has 8 general purpose 32-bit registers: eax , ebx , ecx , edx , esi , edi , ebp , esp . Although any data can be moved between any of these registers, compilers commonly use the same registers for the same uses, and some instructions (such as multiplication and division) can only use the registers they're designed to use. Different compilers may have completely different conventions on how the various registers are used. For the purposes of this document, I will discuss the most common compiler, Microsoft's. Volatility Some registers are typically volatile across functions, and others remain unchanged. This is a feature of the compiler's standards and must be looked after in the code, registers are not preserved automatically (although in some assembly languages they are -- but not in x86 ). What that means is, when a function is called, there is no guarantee that volatile registers will retain their value when the function returns, and it's the function's responsibility to preserve non-volatile registers. The conventions used by Microsoft's compiler are: Volatile : ecx , edx Non-Volatile : ebx , esi , edi , ebp Special : eax , esp (discussed later) General Purpose Registers This section will look at the 8 general purpose registers on the x86 architecture. eax eax is a 32-bit general-purpose register with two common uses: to store the return value of a function as a special register for certain calculations. It is technically a volatile register , since the value isn't preserved. Instead, its value is set to the return value of a function before a function returns. Other than esp , this is probably the most important register to remember for this reason. eax is also used specifically in certain calculations, such as multiplication and division , as a special register . That use will be examined in the instructions section. Here is an example of a function returning in C: return 3 ; // Return the value 3 Here's the same code in assembly: mov eax, 3 ; Set eax (the return value) to 3 ret ; Return TODO: \u9700\u8981\u6dfb\u52a0\u5177\u4f53\u4f8b\u5b50\u6765\u8fdb\u884c\u5b9e\u8df5 ebx ebx is a non-volatile general-purpose register. It has no specific uses, but is often set to a commonly used value (such as 0) throughout a function to speed up calculations. ecx ecx is a volatile general-purpose register that is occasionally used as a function parameter or as a loop counter . Functions of the \" __fastcall \" convention pass the first two parameters to a function using ecx and edx . Additionally, when calling a member function of a class, a pointer to that class is often passed in ecx no matter what the calling convention is. Additionally, ecx is often used as a loop counter . for loops generally, although not always, set the accumulator variable to ecx . rep- instructions also use ecx as a counter, automatically decrementing it till it reaches 0. This class of function will be discussed in a later section. edx edx is a volatile general-purpose register that is occasionally used as a function parameter. Like ecx , edx is used for \" __fastcall \" functions. Besides fastcall , edx is generally used for storing short-term variables within a function. esi esi is a non-volatile general-purpose register that is often used as a pointer. Specifically, for \"rep-\" class instructions, which require a source and a destination for data, esi points to the \"source\". esi often stores data that is used throughout a function because it doesn't change. edi edi is a non-volatile general-purpose register that is often used as a pointer. It is similar to esi , except that it is generally used as a destination for data. ebp ebp is a non-volatile general-purpose register that has two distinct uses depending on compile settings: it is either the frame pointer or a general purpose register . If compilation is not optimized, or code is written by hand, ebp keeps track of where the stack is at the beginning of a function (the stack will be explained in great detail in a later section). Because the stack changes throughout a function, having ebp set to the original value allows variables stored on the stack to be referenced easily. This will be explored in detail when the stack is explained. If compilation is optimized, ebp is used as a general register for storing any kind of data, while calculations for the stack pointer are done based on the stack pointer moving (which gets confusing -- luckily, IDA automatically detects and corrects a moving stack pointer !) esp esp is a special register that stores a pointer to the top of the stack (the top is actually at a lower virtual address than the bottom as the stack grows downwards in memory towards the heap ). Math is rarely done directly on esp , and the value of esp must be the same at the beginning and the end of each function. esp will be examined in much greater detail in a later section. Special Purpose Registers For special purpose and floating point registers not listed here, have a look at the Wikipedia Article or other reference sites. eip eip , or the instruction pointer , is a special-purpose register which stores a pointer to the address of the instruction that is currently executing. Making a jump is like adding to or subtracting from the instruction pointer. After each instruction, a value equal to the size of the instruction is added to eip , which means that eip points at the machine code for the next instruction. This simple example shows the automatic addition to eip at every step: eip + 1 53 push ebx eip + 4 8 B 54 24 08 mov edx , [ esp + arg_0 ] eip + 2 31 DB xor ebx , ebx eip + 2 89 D3 mov ebx , edx eip + 3 8 D 42 07 lea eax , [ edx + 7 ] ..... flags In the flags register , each bit has a specific meaning and they are used to store meta-information about the results of previous operations. For example, whether the last calculation overflowed the register or whether the operands were equal. Our interest in the flags register is usually around the cmp and test operations which will commonly set or unset the zero, carry and overflow flags. These flags will then be tested by a conditional jump which may be controlling program flow or a loop. Canonical mnemonics for register microsoft x64 Architecture 64-bit register Lower 32 bits Lower 16 bits Lower 8 bits rax eax ax al rbx ebx bx bl rcx ecx cx cl rdx edx dx dl rsi esi si sil rdi edi di dil rbp ebp bp bpl rsp esp sp spl r8 r8d r8w r8b r9 r9d r9w r9b r10 r10d r10w r10b r11 r11d r11w r11b r12 r12d r12w r12b r13 r13d r13w r13b r14 r14d r14w r14b r15 r15d r15w r15b virginia x86 Assembly Guide \u53e6\u5916\u53c2\u89c1: skullsecurity Registers#16-bit and 8-bit Registers utoronto x86 Registers","title":"Introduction"},{"location":"CPU/Manufacturer/Intel/Register/#_1","text":"\u672c\u7ae0\u63cf\u8ff0Intel CPU\u7684register\u3002Intel CPU\u662f\u5728\u4e0d\u65ad\u6f14\u8fdb\u4e2d\u7684\uff0c\u6211\u4eec\u4ee5\u76f8\u5bf9\u7b80\u5355\u7684x86\u4e3a\u4f8b\u6765\u8fdb\u884c\u63cf\u8ff0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Manufacturer/Intel/Register/#skullsecurity#registers","text":"This section is the first section specific to assembly. So if you're reading through the full guide, get ready for some actual learning! A register is like a variable, except that there are a fixed number of registers. Each register is a special spot in the CPU where a single value is stored. A register is the only place where math can be done ( addition , subtraction , etc). Registers frequently hold pointers which reference memory. Movement of values between registers and memory is very common. Intel assembly has 8 general purpose 32-bit registers: eax , ebx , ecx , edx , esi , edi , ebp , esp . Although any data can be moved between any of these registers, compilers commonly use the same registers for the same uses, and some instructions (such as multiplication and division) can only use the registers they're designed to use. Different compilers may have completely different conventions on how the various registers are used. For the purposes of this document, I will discuss the most common compiler, Microsoft's.","title":"\u5165\u95e8\u8bfb\u7269: skullsecurity Registers"},{"location":"CPU/Manufacturer/Intel/Register/#volatility","text":"Some registers are typically volatile across functions, and others remain unchanged. This is a feature of the compiler's standards and must be looked after in the code, registers are not preserved automatically (although in some assembly languages they are -- but not in x86 ). What that means is, when a function is called, there is no guarantee that volatile registers will retain their value when the function returns, and it's the function's responsibility to preserve non-volatile registers. The conventions used by Microsoft's compiler are: Volatile : ecx , edx Non-Volatile : ebx , esi , edi , ebp Special : eax , esp (discussed later)","title":"Volatility"},{"location":"CPU/Manufacturer/Intel/Register/#general#purpose#registers","text":"This section will look at the 8 general purpose registers on the x86 architecture.","title":"General Purpose Registers"},{"location":"CPU/Manufacturer/Intel/Register/#eax","text":"eax is a 32-bit general-purpose register with two common uses: to store the return value of a function as a special register for certain calculations. It is technically a volatile register , since the value isn't preserved. Instead, its value is set to the return value of a function before a function returns. Other than esp , this is probably the most important register to remember for this reason. eax is also used specifically in certain calculations, such as multiplication and division , as a special register . That use will be examined in the instructions section. Here is an example of a function returning in C: return 3 ; // Return the value 3 Here's the same code in assembly: mov eax, 3 ; Set eax (the return value) to 3 ret ; Return TODO: \u9700\u8981\u6dfb\u52a0\u5177\u4f53\u4f8b\u5b50\u6765\u8fdb\u884c\u5b9e\u8df5","title":"eax"},{"location":"CPU/Manufacturer/Intel/Register/#ebx","text":"ebx is a non-volatile general-purpose register. It has no specific uses, but is often set to a commonly used value (such as 0) throughout a function to speed up calculations.","title":"ebx"},{"location":"CPU/Manufacturer/Intel/Register/#ecx","text":"ecx is a volatile general-purpose register that is occasionally used as a function parameter or as a loop counter . Functions of the \" __fastcall \" convention pass the first two parameters to a function using ecx and edx . Additionally, when calling a member function of a class, a pointer to that class is often passed in ecx no matter what the calling convention is. Additionally, ecx is often used as a loop counter . for loops generally, although not always, set the accumulator variable to ecx . rep- instructions also use ecx as a counter, automatically decrementing it till it reaches 0. This class of function will be discussed in a later section.","title":"ecx"},{"location":"CPU/Manufacturer/Intel/Register/#edx","text":"edx is a volatile general-purpose register that is occasionally used as a function parameter. Like ecx , edx is used for \" __fastcall \" functions. Besides fastcall , edx is generally used for storing short-term variables within a function.","title":"edx"},{"location":"CPU/Manufacturer/Intel/Register/#esi","text":"esi is a non-volatile general-purpose register that is often used as a pointer. Specifically, for \"rep-\" class instructions, which require a source and a destination for data, esi points to the \"source\". esi often stores data that is used throughout a function because it doesn't change.","title":"esi"},{"location":"CPU/Manufacturer/Intel/Register/#edi","text":"edi is a non-volatile general-purpose register that is often used as a pointer. It is similar to esi , except that it is generally used as a destination for data.","title":"edi"},{"location":"CPU/Manufacturer/Intel/Register/#ebp","text":"ebp is a non-volatile general-purpose register that has two distinct uses depending on compile settings: it is either the frame pointer or a general purpose register . If compilation is not optimized, or code is written by hand, ebp keeps track of where the stack is at the beginning of a function (the stack will be explained in great detail in a later section). Because the stack changes throughout a function, having ebp set to the original value allows variables stored on the stack to be referenced easily. This will be explored in detail when the stack is explained. If compilation is optimized, ebp is used as a general register for storing any kind of data, while calculations for the stack pointer are done based on the stack pointer moving (which gets confusing -- luckily, IDA automatically detects and corrects a moving stack pointer !)","title":"ebp"},{"location":"CPU/Manufacturer/Intel/Register/#esp","text":"esp is a special register that stores a pointer to the top of the stack (the top is actually at a lower virtual address than the bottom as the stack grows downwards in memory towards the heap ). Math is rarely done directly on esp , and the value of esp must be the same at the beginning and the end of each function. esp will be examined in much greater detail in a later section.","title":"esp"},{"location":"CPU/Manufacturer/Intel/Register/#special#purpose#registers","text":"For special purpose and floating point registers not listed here, have a look at the Wikipedia Article or other reference sites.","title":"Special Purpose Registers"},{"location":"CPU/Manufacturer/Intel/Register/#eip","text":"eip , or the instruction pointer , is a special-purpose register which stores a pointer to the address of the instruction that is currently executing. Making a jump is like adding to or subtracting from the instruction pointer. After each instruction, a value equal to the size of the instruction is added to eip , which means that eip points at the machine code for the next instruction. This simple example shows the automatic addition to eip at every step: eip + 1 53 push ebx eip + 4 8 B 54 24 08 mov edx , [ esp + arg_0 ] eip + 2 31 DB xor ebx , ebx eip + 2 89 D3 mov ebx , edx eip + 3 8 D 42 07 lea eax , [ edx + 7 ] .....","title":"eip"},{"location":"CPU/Manufacturer/Intel/Register/#flags","text":"In the flags register , each bit has a specific meaning and they are used to store meta-information about the results of previous operations. For example, whether the last calculation overflowed the register or whether the operands were equal. Our interest in the flags register is usually around the cmp and test operations which will commonly set or unset the zero, carry and overflow flags. These flags will then be tested by a conditional jump which may be controlling program flow or a loop.","title":"flags"},{"location":"CPU/Manufacturer/Intel/Register/#canonical#mnemonics#for#register","text":"","title":"Canonical mnemonics for register"},{"location":"CPU/Manufacturer/Intel/Register/#microsoft#x64#architecture","text":"64-bit register Lower 32 bits Lower 16 bits Lower 8 bits rax eax ax al rbx ebx bx bl rcx ecx cx cl rdx edx dx dl rsi esi si sil rdi edi di dil rbp ebp bp bpl rsp esp sp spl r8 r8d r8w r8b r9 r9d r9w r9b r10 r10d r10w r10b r11 r11d r11w r11b r12 r12d r12w r12b r13 r13d r13w r13b r14 r14d r14w r14b r15 r15d r15w r15b","title":"microsoft x64 Architecture"},{"location":"CPU/Manufacturer/Intel/Register/#virginia#x86#assembly#guide","text":"\u53e6\u5916\u53c2\u89c1: skullsecurity Registers#16-bit and 8-bit Registers utoronto x86 Registers","title":"virginia x86 Assembly Guide"},{"location":"CPU/Manufacturer/Intel/Register/FLAGS-register/","text":"FLAGS register \u7ef4\u57fa\u767e\u79d1 FLAGS register","title":"FLAGS-register"},{"location":"CPU/Manufacturer/Intel/Register/FLAGS-register/#flags#register","text":"","title":"FLAGS register"},{"location":"CPU/Manufacturer/Intel/Register/FLAGS-register/#flags#register_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1FLAGS register"},{"location":"CPU/Manufacturer/Intel/doc/","text":"doc Intel\u00ae 64 and IA-32 Architectures Software Developer Manuals NOTE: \u8fd9\u662f\u5728\u9605\u8bfb preshing Memory Reordering Caught in the Act \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u7684\u94fe\u63a5: Intel lists several such surprises in Volume 3, \u00a78.2.3 of their x86/64 Architecture Specification .","title":"Introduction"},{"location":"CPU/Manufacturer/Intel/doc/#doc","text":"","title":"doc"},{"location":"CPU/Manufacturer/Intel/doc/#intel#64#and#ia-32#architectures#software#developer#manuals","text":"NOTE: \u8fd9\u662f\u5728\u9605\u8bfb preshing Memory Reordering Caught in the Act \u65f6\uff0c\u5176\u4e2d\u7ed9\u51fa\u7684\u94fe\u63a5: Intel lists several such surprises in Volume 3, \u00a78.2.3 of their x86/64 Architecture Specification .","title":"Intel\u00ae 64 and IA-32 Architectures Software Developer Manuals"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/","text":"\u5b66\u4e60\u7f16\u7a0b\u5176\u5b9e\u5c31\u662f\u5b66**\u9ad8\u7ea7\u8bed\u8a00**\uff0c\u5373\u90a3\u4e9b\u4e3a\u4eba\u7c7b\u8bbe\u8ba1\u7684\u8ba1\u7b97\u673a\u8bed\u8a00\u3002 \u4f46\u662f\uff0c\u8ba1\u7b97\u673a\u4e0d\u7406\u89e3\u9ad8\u7ea7\u8bed\u8a00\uff0c\u5fc5\u987b\u901a\u8fc7\u7f16\u8bd1\u5668\u8f6c\u6210**\u4e8c\u8fdb\u5236\u4ee3\u7801**\uff0c\u624d\u80fd\u8fd0\u884c\u3002\u5b66\u4f1a\u9ad8\u7ea7\u8bed\u8a00\uff0c\u5e76\u4e0d\u7b49\u4e8e\u7406\u89e3\u8ba1\u7b97\u673a\u5b9e\u9645\u7684\u8fd0\u884c\u6b65\u9aa4\u3002 \u8ba1\u7b97\u673a\u771f\u6b63\u80fd\u591f\u7406\u89e3\u7684\u662f**\u4f4e\u7ea7\u8bed\u8a00**\uff0c\u5b83\u4e13\u95e8\u7528\u6765\u63a7\u5236\u786c\u4ef6\u3002 \u6c47\u7f16\u8bed\u8a00**\u5c31\u662f\u4f4e\u7ea7\u8bed\u8a00\uff0c\u76f4\u63a5\u63cf\u8ff0/\u63a7\u5236 CPU \u7684\u8fd0\u884c\u3002\u5982\u679c\u4f60\u60f3\u4e86\u89e3 CPU \u5230\u5e95\u5e72\u4e86\u4e9b\u4ec0\u4e48\uff0c\u4ee5\u53ca\u4ee3\u7801\u7684\u8fd0\u884c\u6b65\u9aa4\uff0c\u5c31\u4e00\u5b9a\u8981\u5b66\u4e60**\u6c47\u7f16\u8bed\u8a00 \u3002 \u6c47\u7f16\u8bed\u8a00\u4e0d\u5bb9\u6613\u5b66\u4e60\uff0c\u5c31\u8fde\u7b80\u660e\u627c\u8981\u7684\u4ecb\u7ecd\u90fd\u5f88\u96be\u627e\u5230\u3002\u4e0b\u9762\u6211\u5c1d\u8bd5\u5199\u4e00\u7bc7\u6700\u597d\u61c2\u7684\u6c47\u7f16\u8bed\u8a00\u6559\u7a0b\uff0c\u89e3\u91ca CPU \u5982\u4f55\u6267\u884c\u4ee3\u7801\u3002 \u4e00\u3001\u6c47\u7f16\u8bed\u8a00\u662f\u4ec0\u4e48\uff1f \u6211\u4eec\u77e5\u9053\uff0cCPU \u53ea\u8d1f\u8d23\u8ba1\u7b97\uff0c\u672c\u8eab\u4e0d\u5177\u5907\u667a\u80fd\u3002\u4f60\u8f93\u5165\u4e00\u6761\u6307\u4ee4\uff08instruction\uff09\uff0c\u5b83\u5c31\u8fd0\u884c\u4e00\u6b21\uff0c\u7136\u540e\u505c\u4e0b\u6765\uff0c\u7b49\u5f85\u4e0b\u4e00\u6761\u6307\u4ee4\u3002 \u8fd9\u4e9b**\u6307\u4ee4**\u90fd\u662f**\u4e8c\u8fdb\u5236**\u7684\uff0c\u79f0\u4e3a**\u64cd\u4f5c\u7801**\uff08opcode\uff09\uff0c\u6bd4\u5982\u52a0\u6cd5\u6307\u4ee4\u5c31\u662f 00000011 \u3002 \u7f16\u8bd1\u5668 \u7684\u4f5c\u7528\uff0c\u5c31\u662f\u5c06\u9ad8\u7ea7\u8bed\u8a00\u5199\u597d\u7684\u7a0b\u5e8f\uff0c\u7ffb\u8bd1\u6210\u4e00\u6761\u6761\u64cd\u4f5c\u7801\u3002 \u5bf9\u4e8e\u4eba\u7c7b\u6765\u8bf4\uff0c \u4e8c\u8fdb\u5236\u7a0b\u5e8f**\u662f\u4e0d\u53ef\u8bfb\u7684\uff0c\u6839\u672c\u770b\u4e0d\u51fa\u6765\u673a\u5668\u5e72\u4e86\u4ec0\u4e48\u3002\u4e3a\u4e86\u89e3\u51b3\u53ef\u8bfb\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5076\u5c14\u7684\u7f16\u8f91\u9700\u6c42\uff0c\u5c31\u8bde\u751f\u4e86**\u6c47\u7f16\u8bed\u8a00 \u3002( ) \u6c47\u7f16\u8bed\u8a00\u662f\u4e8c\u8fdb\u5236\u6307\u4ee4\u7684\u6587\u672c\u5f62\u5f0f \uff0c\u4e0e**\u6307\u4ee4**\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\u5173\u7cfb\u3002\u6bd4\u5982\uff0c\u52a0\u6cd5\u6307\u4ee4 00000011 \u5199\u6210\u6c47\u7f16\u8bed\u8a00\u5c31\u662f ADD\u3002\u53ea\u8981\u8fd8\u539f\u6210\u4e8c\u8fdb\u5236\uff0c**\u6c47\u7f16\u8bed\u8a00**\u5c31\u53ef\u4ee5\u88ab CPU \u76f4\u63a5\u6267\u884c\uff0c\u6240\u4ee5\u5b83\u662f\u6700\u5e95\u5c42\u7684\u4f4e\u7ea7\u8bed\u8a00\u3002 \u4e8c\u3001\u6765\u5386 \u6700\u65e9\u7684\u65f6\u5019\uff0c\u7f16\u5199\u7a0b\u5e8f\u5c31\u662f\u624b\u5199\u4e8c\u8fdb\u5236\u6307\u4ee4\uff0c\u7136\u540e\u901a\u8fc7\u5404\u79cd\u5f00\u5173\u8f93\u5165\u8ba1\u7b97\u673a\uff0c\u6bd4\u5982\u8981\u505a\u52a0\u6cd5\u4e86\uff0c\u5c31\u6309\u4e00\u4e0b\u52a0\u6cd5\u5f00\u5173\u3002\u540e\u6765\uff0c\u53d1\u660e\u4e86\u7eb8\u5e26\u6253\u5b54\u673a\uff0c\u901a\u8fc7\u5728\u7eb8\u5e26\u4e0a\u6253\u5b54\uff0c\u5c06**\u4e8c\u8fdb\u5236\u6307\u4ee4**\u81ea\u52a8\u8f93\u5165\u8ba1\u7b97\u673a\u3002 \u4e3a\u4e86\u89e3\u51b3\u4e8c\u8fdb\u5236\u6307\u4ee4\u7684\u53ef\u8bfb\u6027\u95ee\u9898\uff0c\u5de5\u7a0b\u5e08\u5c06\u90a3\u4e9b\u6307\u4ee4\u5199\u6210\u4e86\u516b\u8fdb\u5236\u3002\u4e8c\u8fdb\u5236\u8f6c\u516b\u8fdb\u5236\u662f\u8f7b\u800c\u6613\u4e3e\u7684\uff0c\u4f46\u662f\u516b\u8fdb\u5236\u7684\u53ef\u8bfb\u6027\u4e5f\u4e0d\u884c\u3002\u5f88\u81ea\u7136\u5730\uff0c\u6700\u540e\u8fd8\u662f\u7528\u6587\u5b57\u8868\u8fbe\uff0c\u52a0\u6cd5\u6307\u4ee4\u5199\u6210 ADD\u3002**\u5185\u5b58\u5730\u5740**\u4e5f\u4e0d\u518d\u76f4\u63a5\u5f15\u7528\uff0c\u800c\u662f\u7528**\u6807\u7b7e**\u8868\u793a\u3002 \u8fd9\u6837\u7684\u8bdd\uff0c\u5c31\u591a\u51fa\u4e00\u4e2a\u6b65\u9aa4\uff0c\u8981\u628a\u8fd9\u4e9b**\u6587\u5b57\u6307\u4ee4**\u7ffb\u8bd1\u6210**\u4e8c\u8fdb\u5236**\uff0c\u8fd9\u4e2a\u6b65\u9aa4\u5c31\u79f0\u4e3a assembling \uff0c\u5b8c\u6210\u8fd9\u4e2a\u6b65\u9aa4\u7684\u7a0b\u5e8f\u5c31\u53eb\u505a assembler \u3002\u5b83\u5904\u7406\u7684\u6587\u672c\uff0c\u81ea\u7136\u5c31\u53eb\u505a aseembly code \u3002\u6807\u51c6\u5316\u4ee5\u540e\uff0c\u79f0\u4e3a~\uff0c\u7f29\u5199\u4e3a asm \uff0c\u4e2d\u6587\u8bd1\u4e3a**\u6c47\u7f16\u8bed\u8a00**\u3002 \u6bcf\u4e00\u79cd CPU \u7684\u673a\u5668\u6307\u4ee4\u90fd\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u6c47\u7f16\u8bed\u8a00\u4e5f\u4e0d\u4e00\u6837\u3002\u672c\u6587\u4ecb\u7ecd\u7684\u662f\u76ee\u524d\u6700\u5e38\u89c1\u7684 x86 \u6c47\u7f16\u8bed\u8a00\uff0c\u5373 Intel \u516c\u53f8\u7684 CPU \u4f7f\u7528\u7684\u90a3\u4e00\u79cd\u3002 \u4e09\u3001\u5bc4\u5b58\u5668 \u5b66\u4e60\u6c47\u7f16\u8bed\u8a00\uff0c\u9996\u5148\u5fc5\u987b\u4e86\u89e3\u4e24\u4e2a\u77e5\u8bc6\u70b9\uff1a\u5bc4\u5b58\u5668\u548c\u5185\u5b58\u6a21\u578b\u3002 \u5148\u6765\u770b\u5bc4\u5b58\u5668\u3002CPU \u672c\u8eab\u53ea\u8d1f\u8d23\u8fd0\u7b97\uff0c\u4e0d\u8d1f\u8d23\u50a8\u5b58\u6570\u636e\u3002\u6570\u636e\u4e00\u822c\u90fd\u50a8\u5b58\u5728\u5185\u5b58\u4e4b\u4e2d\uff0cCPU \u8981\u7528\u7684\u65f6\u5019\u5c31\u53bb\u5185\u5b58\u8bfb\u5199\u6570\u636e\u3002\u4f46\u662f\uff0c CPU \u7684\u8fd0\u7b97\u901f\u5ea6\u8fdc\u9ad8\u4e8e\u5185\u5b58\u7684\u8bfb\u5199\u901f\u5ea6 \uff0c\u4e3a\u4e86\u907f\u514d\u88ab\u62d6\u6162\uff0cCPU \u90fd\u81ea\u5e26**\u4e00\u7ea7\u7f13\u5b58**\u548c**\u4e8c\u7ea7\u7f13\u5b58**\u3002\u57fa\u672c\u4e0a\uff0c**CPU \u7f13\u5b58**\u53ef\u4ee5\u770b\u4f5c\u662f\u8bfb\u5199\u901f\u5ea6\u8f83\u5feb\u7684\u5185\u5b58\u3002 \u4f46\u662f\uff0c CPU \u7f13\u5b58**\u8fd8\u662f\u4e0d\u591f\u5feb\uff0c\u53e6\u5916\u6570\u636e\u5728\u7f13\u5b58\u91cc\u9762\u7684**\u5730\u5740\u662f\u4e0d\u56fa\u5b9a**\u7684\uff0cCPU \u6bcf\u6b21\u8bfb\u5199\u90fd\u8981**\u5bfb\u5740**\u4e5f\u4f1a\u62d6\u6162\u901f\u5ea6\u3002\u56e0\u6b64\uff0c\u9664\u4e86\u7f13\u5b58\u4e4b\u5916\uff0cCPU \u8fd8\u81ea\u5e26\u4e86**\u5bc4\u5b58\u5668 \uff08register\uff09\uff0c\u7528\u6765\u50a8\u5b58\u6700\u5e38\u7528\u7684\u6570\u636e\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u90a3\u4e9b\u6700\u9891\u7e41\u8bfb\u5199\u7684\u6570\u636e\uff08\u6bd4\u5982\u5faa\u73af\u53d8\u91cf\uff09\uff0c\u90fd\u4f1a\u653e\u5728\u5bc4\u5b58\u5668\u91cc\u9762\uff0cCPU \u4f18\u5148\u8bfb\u5199**\u5bc4\u5b58\u5668**\uff0c\u518d\u7531**\u5bc4\u5b58\u5668**\u8ddf\u5185\u5b58\u4ea4\u6362\u6570\u636e\u3002 \u5bc4\u5b58\u5668\u4e0d\u4f9d\u9760\u5730\u5740\u533a\u5206\u6570\u636e\uff0c\u800c\u4f9d\u9760\u540d\u79f0 \u3002\u6bcf\u4e00\u4e2a**\u5bc4\u5b58\u5668\u90fd**\u6709\u81ea\u5df1\u7684**\u540d\u79f0**\uff0c\u6211\u4eec\u544a\u8bc9 CPU \u53bb\u5177\u4f53\u7684\u54ea\u4e00\u4e2a\u5bc4\u5b58\u5668\u62ff\u6570\u636e\uff0c\u8fd9\u6837\u7684\u901f\u5ea6\u662f\u6700\u5feb\u7684\u3002\u6709\u4eba\u6bd4\u55bb\u5bc4\u5b58\u5668\u662f CPU \u7684**\u96f6\u7ea7\u7f13\u5b58**\u3002 \u56db\u3001\u5bc4\u5b58\u5668\u7684\u79cd\u7c7b \u65e9\u671f\u7684 x86 CPU \u53ea\u67098\u4e2a\u5bc4\u5b58\u5668\uff0c\u800c\u4e14\u6bcf\u4e2a\u90fd\u6709\u4e0d\u540c\u7684\u7528\u9014\u3002\u73b0\u5728\u7684\u5bc4\u5b58\u5668\u5df2\u7ecf\u6709100\u591a\u4e2a\u4e86\uff0c\u90fd\u53d8\u6210**\u901a\u7528\u5bc4\u5b58\u5668**\uff0c\u4e0d\u7279\u522b\u6307\u5b9a\u7528\u9014\u4e86\uff0c\u4f46\u662f\u65e9\u671f\u5bc4\u5b58\u5668\u7684\u540d\u5b57\u90fd\u88ab\u4fdd\u5b58\u4e86\u4e0b\u6765\u3002 EAX EBX ECX EDX EDI ESI EBP ESP \u4e0a\u9762\u8fd98\u4e2a\u5bc4\u5b58\u5668\u4e4b\u4e2d\uff0c\u524d\u9762\u4e03\u4e2a\u90fd\u662f\u901a\u7528\u7684\u3002 ESP \u5bc4\u5b58\u5668**\u6709\u7279\u5b9a\u7528\u9014\uff0c\u4fdd\u5b58\u5f53\u524d **Stack \u7684\u5730\u5740\uff08\u8be6\u89c1\u4e0b\u4e00\u8282\uff09\u3002 \u6211\u4eec\u5e38\u5e38\u770b\u5230 32\u4f4d CPU\u300164\u4f4d CPU \u8fd9\u6837\u7684\u540d\u79f0\uff0c\u5176\u5b9e\u6307\u7684\u5c31\u662f\u5bc4\u5b58\u5668\u7684\u5927\u5c0f\u300232 \u4f4d CPU \u7684\u5bc4\u5b58\u5668\u5927\u5c0f\u5c31\u662f4\u4e2a\u5b57\u8282\u3002 \u4e94\u3001\u5185\u5b58\u6a21\u578b\uff1aHeap **\u5bc4\u5b58\u5668**\u53ea\u80fd\u5b58\u653e\u5f88\u5c11\u91cf\u7684\u6570\u636e\uff0c\u5927\u591a\u6570\u65f6\u5019\uff0cCPU \u8981\u6307\u6325\u5bc4\u5b58\u5668\uff0c\u76f4\u63a5\u8ddf\u5185\u5b58\u4ea4\u6362\u6570\u636e\u3002\u6240\u4ee5\uff0c\u9664\u4e86\u5bc4\u5b58\u5668\uff0c\u8fd8\u5fc5\u987b\u4e86\u89e3\u5185\u5b58\u600e\u4e48\u50a8\u5b58\u6570\u636e\u3002 \u7a0b\u5e8f\u8fd0\u884c\u7684\u65f6\u5019\uff0c \u64cd\u4f5c\u7cfb\u7edf**\u4f1a\u7ed9\u5b83\u5206\u914d\u4e00\u6bb5\u5185\u5b58\uff0c\u7528\u6765\u50a8\u5b58\u7a0b\u5e8f\u548c\u8fd0\u884c\u4ea7\u751f\u7684\u6570\u636e\u3002\u8fd9\u6bb5\u5185\u5b58\u6709**\u8d77\u59cb\u5730\u5740**\u548c**\u7ed3\u675f\u5730\u5740 \uff0c\u6bd4\u5982\u4ece 0x1000 \u5230 0x8000 \uff0c\u8d77\u59cb\u5730\u5740\u662f\u8f83\u5c0f\u7684\u90a3\u4e2a\u5730\u5740\uff0c\u7ed3\u675f\u5730\u5740\u662f\u8f83\u5927\u7684\u90a3\u4e2a\u5730\u5740\u3002 \u7a0b\u5e8f\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u4e8e\u52a8\u6001\u7684\u5185\u5b58\u5360\u7528\u8bf7\u6c42\uff08\u6bd4\u5982\u65b0\u5efa\u5bf9\u8c61\uff0c\u6216\u8005\u4f7f\u7528 malloc \u547d\u4ee4\uff09\uff0c\u7cfb\u7edf\u5c31\u4f1a\u4ece\u9884\u5148\u5206\u914d\u597d\u7684\u90a3\u6bb5\u5185\u5b58\u4e4b\u4e2d\uff0c\u5212\u51fa\u4e00\u90e8\u5206\u7ed9\u7528\u6237\uff0c\u5177\u4f53\u89c4\u5219\u662f\u4ece**\u8d77\u59cb\u5730\u5740**\u5f00\u59cb\u5212\u5206\uff08\u5b9e\u9645\u4e0a\uff0c**\u8d77\u59cb\u5730\u5740**\u4f1a\u6709\u4e00\u6bb5\u9759\u6001\u6570\u636e\uff0c\u8fd9\u91cc\u5ffd\u7565\uff09\u3002\u4e3e\u4f8b\u6765\u8bf4\uff0c\u7528\u6237\u8981\u6c42\u5f97\u523010\u4e2a\u5b57\u8282\u5185\u5b58\uff0c\u90a3\u4e48\u4ece\u8d77\u59cb\u5730\u5740 0x1000 \u5f00\u59cb\u7ed9\u4ed6\u5206\u914d\uff0c\u4e00\u76f4\u5206\u914d\u5230\u5730\u5740 0x100A \uff0c\u5982\u679c\u518d\u8981\u6c42\u5f97\u523022\u4e2a\u5b57\u8282\uff0c\u90a3\u4e48\u5c31\u5206\u914d\u5230 0x1020 \u3002 \u8fd9\u79cd\u56e0\u4e3a\u7528\u6237\u4e3b\u52a8\u8bf7\u6c42\u800c\u5212\u5206\u51fa\u6765\u7684\u5185\u5b58\u533a\u57df\uff0c\u53eb\u505a Heap \uff08\u5806\uff09\u3002\u5b83\u7531\u8d77\u59cb\u5730\u5740\u5f00\u59cb\uff0c\u4ece\u4f4e\u4f4d\uff08\u5730\u5740\uff09\u5411\u9ad8\u4f4d\uff08\u5730\u5740\uff09\u589e\u957f\u3002Heap \u7684\u4e00\u4e2a\u91cd\u8981\u7279\u70b9\u5c31\u662f\u4e0d\u4f1a\u81ea\u52a8\u6d88\u5931\uff0c\u5fc5\u987b\u624b\u52a8\u91ca\u653e\uff0c\u6216\u8005\u7531\u5783\u573e\u56de\u6536\u673a\u5236\u6765\u56de\u6536\u3002 \u516d\u3001\u5185\u5b58\u6a21\u578b\uff1aStack \u9664\u4e86 Heap \u4ee5\u5916\uff0c\u5176\u4ed6\u7684\u5185\u5b58\u5360\u7528\u53eb\u505a Stack \uff08\u6808\uff09\u3002\u7b80\u5355\u8bf4\uff0cStack \u662f\u7531\u4e8e\u51fd\u6570\u8fd0\u884c\u800c\u4e34\u65f6\u5360\u7528\u7684\u5185\u5b58\u533a\u57df\u3002 \u8bf7\u770b\u4e0b\u9762\u7684\u4f8b\u5b50\u3002 int main() { int a = 2; int b = 3; } \u4e0a\u9762\u4ee3\u7801\u4e2d\uff0c\u7cfb\u7edf\u5f00\u59cb\u6267\u884c main \u51fd\u6570\u65f6\uff0c\u4f1a\u4e3a\u5b83\u5728\u5185\u5b58\u91cc\u9762\u5efa\u7acb\u4e00\u4e2a**\u5e27**\uff08frame\uff09\uff0c\u6240\u6709 main \u7684\u5185\u90e8\u53d8\u91cf\uff08\u6bd4\u5982 a \u548c b \uff09\u90fd\u4fdd\u5b58\u5728\u8fd9\u4e2a\u5e27\u91cc\u9762\u3002 main \u51fd\u6570\u6267\u884c\u7ed3\u675f\u540e\uff0c\u8be5**\u5e27**\u5c31\u4f1a\u88ab\u56de\u6536\uff0c\u91ca\u653e\u6240\u6709\u7684\u5185\u90e8\u53d8\u91cf\uff0c\u4e0d\u518d\u5360\u7528\u7a7a\u95f4\u3002 \u5982\u679c\u51fd\u6570\u5185\u90e8\u8c03\u7528\u4e86\u5176\u4ed6\u51fd\u6570\uff0c\u4f1a\u53d1\u751f\u4ec0\u4e48\u60c5\u51b5\uff1f int main() { int a = 2; int b = 3; return add_a_and_b(a, b); } \u4e0a\u9762\u4ee3\u7801\u4e2d\uff0c main \u51fd\u6570\u5185\u90e8\u8c03\u7528\u4e86 add_a_and_b \u51fd\u6570\u3002\u6267\u884c\u5230\u8fd9\u4e00\u884c\u7684\u65f6\u5019\uff0c\u7cfb\u7edf\u4e5f\u4f1a\u4e3a add_a_and_b \u65b0\u5efa\u4e00\u4e2a\u5e27\uff0c\u7528\u6765\u50a8\u5b58\u5b83\u7684\u5185\u90e8\u53d8\u91cf\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u6b64\u65f6\u540c\u65f6\u5b58\u5728\u4e24\u4e2a\u5e27\uff1a main \u548c add_a_and_b \u3002\u4e00\u822c\u6765\u8bf4\uff0c\u8c03\u7528\u6808\u6709\u591a\u5c11\u5c42\uff0c\u5c31\u6709\u591a\u5c11\u5e27\u3002 \u7b49\u5230 add_a_and_b \u8fd0\u884c\u7ed3\u675f\uff0c\u5b83\u7684\u5e27\u5c31\u4f1a\u88ab\u56de\u6536\uff0c\u7cfb\u7edf\u4f1a\u56de\u5230\u51fd\u6570 main \u521a\u624d\u4e2d\u65ad\u6267\u884c\u7684\u5730\u65b9\uff0c\u7ee7\u7eed\u5f80\u4e0b\u6267\u884c\u3002\u901a\u8fc7\u8fd9\u79cd\u673a\u5236\uff0c\u5c31\u5b9e\u73b0\u4e86\u51fd\u6570\u7684\u5c42\u5c42\u8c03\u7528\uff0c\u5e76\u4e14\u6bcf\u4e00\u5c42\u90fd\u80fd\u4f7f\u7528\u81ea\u5df1\u7684\u672c\u5730\u53d8\u91cf\u3002 \u6240\u6709\u7684\u5e27\u90fd\u5b58\u653e\u5728 Stack\uff0c\u7531\u4e8e\u5e27\u662f\u4e00\u5c42\u5c42\u53e0\u52a0\u7684\uff0c\u6240\u4ee5 Stack \u53eb\u505a\u6808\u3002\u751f\u6210\u65b0\u7684\u5e27\uff0c\u53eb\u505a\"\u5165\u6808\"\uff0c\u82f1\u6587\u662f push\uff1b\u6808\u7684\u56de\u6536\u53eb\u505a\"\u51fa\u6808\"\uff0c\u82f1\u6587\u662f pop\u3002Stack \u7684\u7279\u70b9\u5c31\u662f\uff0c\u6700\u665a\u5165\u6808\u7684\u5e27\u6700\u65e9\u51fa\u6808\uff08\u56e0\u4e3a\u6700\u5185\u5c42\u7684\u51fd\u6570\u8c03\u7528\uff0c\u6700\u5148\u7ed3\u675f\u8fd0\u884c\uff09\uff0c\u8fd9\u5c31\u53eb\u505a\"\u540e\u8fdb\u5148\u51fa\"\u7684\u6570\u636e\u7ed3\u6784\u3002\u6bcf\u4e00\u6b21\u51fd\u6570\u6267\u884c\u7ed3\u675f\uff0c\u5c31\u81ea\u52a8\u91ca\u653e\u4e00\u4e2a\u5e27\uff0c\u6240\u6709\u51fd\u6570\u6267\u884c\u7ed3\u675f\uff0c\u6574\u4e2a Stack \u5c31\u90fd\u91ca\u653e\u4e86\u3002 Stack \u662f\u7531\u5185\u5b58\u533a\u57df\u7684\u7ed3\u675f\u5730\u5740\u5f00\u59cb\uff0c\u4ece\u9ad8\u4f4d\uff08\u5730\u5740\uff09\u5411\u4f4e\u4f4d\uff08\u5730\u5740\uff09\u5206\u914d\u3002\u6bd4\u5982\uff0c\u5185\u5b58\u533a\u57df\u7684\u7ed3\u675f\u5730\u5740\u662f 0x8000 \uff0c\u7b2c\u4e00\u5e27\u5047\u5b9a\u662f16\u5b57\u8282\uff0c\u90a3\u4e48\u4e0b\u4e00\u6b21\u5206\u914d\u7684\u5730\u5740\u5c31\u4f1a\u4ece 0x7FF0 \u5f00\u59cb\uff1b\u7b2c\u4e8c\u5e27\u5047\u5b9a\u9700\u898164\u5b57\u8282\uff0c\u90a3\u4e48\u5730\u5740\u5c31\u4f1a\u79fb\u52a8\u5230 0x7FB0 \u3002 \u4e03\u3001CPU \u6307\u4ee4 7.1 \u4e00\u4e2a\u5b9e\u4f8b \u4e86\u89e3\u5bc4\u5b58\u5668\u548c\u5185\u5b58\u6a21\u578b\u4ee5\u540e\uff0c\u5c31\u53ef\u4ee5\u6765\u770b\u6c47\u7f16\u8bed\u8a00\u5230\u5e95\u662f\u4ec0\u4e48\u4e86\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u7a0b\u5e8f example.c \u3002 int add_a_and_b(int a, int b) { return a + b; } int main() { return add_a_and_b(2, 3); } gcc \u5c06\u8fd9\u4e2a\u7a0b\u5e8f\u8f6c\u6210\u6c47\u7f16\u8bed\u8a00\u3002 $ gcc -S example.c \u4e0a\u9762\u7684\u547d\u4ee4\u6267\u884c\u4ee5\u540e\uff0c\u4f1a\u751f\u6210\u4e00\u4e2a\u6587\u672c\u6587\u4ef6 example.s \uff0c\u91cc\u9762\u5c31\u662f\u6c47\u7f16\u8bed\u8a00\uff0c\u5305\u542b\u4e86\u51e0\u5341\u884c\u6307\u4ee4\u3002\u8fd9\u4e48\u8bf4\u5427\uff0c\u4e00\u4e2a\u9ad8\u7ea7\u8bed\u8a00\u7684\u7b80\u5355\u64cd\u4f5c\uff0c\u5e95\u5c42\u53ef\u80fd\u7531\u51e0\u4e2a\uff0c\u751a\u81f3\u51e0\u5341\u4e2a CPU \u6307\u4ee4\u6784\u6210\u3002CPU \u4f9d\u6b21\u6267\u884c\u8fd9\u4e9b\u6307\u4ee4\uff0c\u5b8c\u6210\u8fd9\u4e00\u6b65\u64cd\u4f5c\u3002 example.s \u7ecf\u8fc7\u7b80\u5316\u4ee5\u540e\uff0c\u5927\u6982\u662f\u4e0b\u9762\u7684\u6837\u5b50\u3002 _add_a_and_b: push %ebx mov %eax, [%esp+8] mov %ebx, [%esp+12] add %eax, %ebx pop %ebx ret _main: push 3 push 2 call _add_a_and_b add %esp, 8 ret \u53ef\u4ee5\u770b\u5230\uff0c\u539f\u7a0b\u5e8f\u7684\u4e24\u4e2a\u51fd\u6570 add_a_and_b \u548c main \uff0c\u5bf9\u5e94\u4e24\u4e2a\u6807\u7b7e _add_a_and_b \u548c _main \u3002\u6bcf\u4e2a\u6807\u7b7e\u91cc\u9762\u662f\u8be5\u51fd\u6570\u6240\u8f6c\u6210\u7684 CPU \u8fd0\u884c\u6d41\u7a0b\u3002 \u6bcf\u4e00\u884c\u5c31\u662f CPU \u6267\u884c\u7684\u4e00\u6b21\u64cd\u4f5c\u3002\u5b83\u53c8\u5206\u6210\u4e24\u90e8\u5206\uff0c\u5c31\u4ee5\u5176\u4e2d\u4e00\u884c\u4e3a\u4f8b\u3002 push %ebx \u8fd9\u4e00\u884c\u91cc\u9762\uff0c push \u662f CPU \u6307\u4ee4\uff0c %ebx \u662f\u8be5\u6307\u4ee4\u8981\u7528\u5230\u7684**\u8fd0\u7b97\u5b50**\u3002\u4e00\u4e2a CPU \u6307\u4ee4\u53ef\u4ee5\u6709\u96f6\u4e2a\u5230\u591a\u4e2a\u8fd0\u7b97\u5b50\u3002 \u4e0b\u9762\u6211\u5c31\u4e00\u884c\u4e00\u884c\u8bb2\u89e3\u8fd9\u4e2a\u6c47\u7f16\u7a0b\u5e8f\uff0c\u5efa\u8bae\u8bfb\u8005\u6700\u597d\u628a\u8fd9\u4e2a\u7a0b\u5e8f\uff0c\u5728\u53e6\u4e00\u4e2a\u7a97\u53e3\u62f7\u8d1d\u4e00\u4efd\uff0c\u7701\u5f97\u9605\u8bfb\u7684\u65f6\u5019\u518d\u628a\u9875\u9762\u6eda\u52a8\u4e0a\u6765\u3002 7.2 push \u6307\u4ee4 \u6839\u636e\u7ea6\u5b9a\uff0c\u7a0b\u5e8f\u4ece _main \u6807\u7b7e\u5f00\u59cb\u6267\u884c\uff0c\u8fd9\u65f6\u4f1a\u5728 Stack \u4e0a\u4e3a main \u5efa\u7acb\u4e00\u4e2a\u5e27\uff0c\u5e76\u5c06 Stack \u6240\u6307\u5411\u7684\u5730\u5740\uff0c\u5199\u5165 ESP \u5bc4\u5b58\u5668 \u3002\u540e\u9762\u5982\u679c\u6709\u6570\u636e\u8981\u5199\u5165 main \u8fd9\u4e2a\u5e27\uff0c\u5c31\u4f1a\u5199\u5728 **ESP \u5bc4\u5b58\u5668**\u6240\u4fdd\u5b58\u7684\u5730\u5740\u3002 \u7136\u540e\uff0c\u5f00\u59cb\u6267\u884c\u7b2c\u4e00\u884c\u4ee3\u7801\u3002 push 3 push \u6307\u4ee4\u7528\u4e8e\u5c06\u8fd0\u7b97\u5b50\u653e\u5165 Stack\uff0c\u8fd9\u91cc\u5c31\u662f\u5c06 3 \u5199\u5165 main \u8fd9\u4e2a\u5e27\u3002 \u867d\u7136\u770b\u4e0a\u53bb\u5f88\u7b80\u5355\uff0c push \u6307\u4ee4\u5176\u5b9e\u6709\u4e00\u4e2a\u524d\u7f6e\u64cd\u4f5c\u3002\u5b83\u4f1a\u5148\u53d6\u51fa **ESP \u5bc4\u5b58\u5668**\u91cc\u9762\u7684\u5730\u5740\uff0c\u5c06\u5176\u51cf\u53bb4\u4e2a\u5b57\u8282\uff0c\u7136\u540e\u5c06\u65b0\u5730\u5740\u5199\u5165 ESP \u5bc4\u5b58\u5668\u3002\u4f7f\u7528\u51cf\u6cd5\u662f\u56e0\u4e3a Stack \u4ece\u9ad8\u4f4d\u5411\u4f4e\u4f4d\u53d1\u5c55\uff0c4\u4e2a\u5b57\u8282\u5219\u662f\u56e0\u4e3a 3 \u7684\u7c7b\u578b\u662f int \uff0c\u5360\u75284\u4e2a\u5b57\u8282\u3002\u5f97\u5230\u65b0\u5730\u5740\u4ee5\u540e\uff0c 3 \u5c31\u4f1a\u5199\u5165\u8fd9\u4e2a\u5730\u5740\u5f00\u59cb\u7684\u56db\u4e2a\u5b57\u8282\u3002 push 2 \u7b2c\u4e8c\u884c\u4e5f\u662f\u4e00\u6837\uff0c push \u6307\u4ee4\u5c06 2 \u5199\u5165 main \u8fd9\u4e2a\u5e27\uff0c\u4f4d\u7f6e\u7d27\u8d34\u7740\u524d\u9762\u5199\u5165\u7684 3 \u3002\u8fd9\u65f6\uff0cESP \u5bc4\u5b58\u5668\u4f1a\u518d\u51cf\u53bb 4\u4e2a\u5b57\u8282\uff08\u7d2f\u8ba1\u51cf\u53bb8\uff09\u3002 7.3 call \u6307\u4ee4 \u7b2c\u4e09\u884c\u7684 call \u6307\u4ee4\u7528\u6765\u8c03\u7528\u51fd\u6570\u3002 call _add_a_and_b \u4e0a\u9762\u7684\u4ee3\u7801\u8868\u793a\u8c03\u7528 add_a_and_b \u51fd\u6570\u3002\u8fd9\u65f6\uff0c\u7a0b\u5e8f\u5c31\u4f1a\u53bb\u627e _add_a_and_b \u6807\u7b7e\uff0c\u5e76\u4e3a\u8be5\u51fd\u6570\u5efa\u7acb\u4e00\u4e2a\u65b0\u7684\u5e27\u3002 \u4e0b\u9762\u5c31\u5f00\u59cb\u6267\u884c _add_a_and_b \u7684\u4ee3\u7801\u3002 push %ebx \u8fd9\u4e00\u884c\u8868\u793a\u5c06 EBX \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u503c\uff0c\u5199\u5165 _add_a_and_b \u8fd9\u4e2a\u5e27\u3002\u8fd9\u662f\u56e0\u4e3a\u540e\u9762\u8981\u7528\u5230\u8fd9\u4e2a\u5bc4\u5b58\u5668\uff0c\u5c31\u5148\u628a\u91cc\u9762\u7684\u503c\u53d6\u51fa\u6765\uff0c\u7528\u5b8c\u540e\u518d\u5199\u56de\u53bb\u3002 \u8fd9\u65f6\uff0c push \u6307\u4ee4\u4f1a\u518d\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\u51cf\u53bb4\u4e2a\u5b57\u8282\uff08\u7d2f\u8ba1\u51cf\u53bb12\uff09\u3002 7.4 mov \u6307\u4ee4 mov \u6307\u4ee4\u7528\u4e8e\u5c06\u4e00\u4e2a\u503c\u5199\u5165\u67d0\u4e2a\u5bc4\u5b58\u5668\u3002 mov %eax, [%esp+8] \u8fd9\u4e00\u884c\u4ee3\u7801\u8868\u793a\uff0c\u5148\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\u52a0\u4e0a8\u4e2a\u5b57\u8282\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u5730\u5740\uff0c\u7136\u540e\u6309\u7167\u8fd9\u4e2a\u5730\u5740\u5728 Stack \u53d6\u51fa\u6570\u636e\u3002\u6839\u636e\u524d\u9762\u7684\u6b65\u9aa4\uff0c\u53ef\u4ee5\u63a8\u7b97\u51fa\u8fd9\u91cc\u53d6\u51fa\u7684\u662f 2 \uff0c\u518d\u5c06 2 \u5199\u5165 EAX \u5bc4\u5b58\u5668\u3002 \u4e0b\u4e00\u884c\u4ee3\u7801\u4e5f\u662f\u5e72\u540c\u6837\u7684\u4e8b\u60c5\u3002 mov %ebx, [%esp+12] \u4e0a\u9762\u7684\u4ee3\u7801\u5c06 ESP \u5bc4\u5b58\u5668\u7684\u503c\u52a012\u4e2a\u5b57\u8282\uff0c\u518d\u6309\u7167\u8fd9\u4e2a\u5730\u5740\u5728 Stack \u53d6\u51fa\u6570\u636e\uff0c\u8fd9\u6b21\u53d6\u51fa\u7684\u662f 3 \uff0c\u5c06\u5176\u5199\u5165 EBX \u5bc4\u5b58\u5668\u3002 7.5 add \u6307\u4ee4 add \u6307\u4ee4\u7528\u4e8e\u5c06\u4e24\u4e2a\u8fd0\u7b97\u5b50\u76f8\u52a0\uff0c\u5e76\u5c06\u7ed3\u679c\u5199\u5165\u7b2c\u4e00\u4e2a\u8fd0\u7b97\u5b50\u3002 add %eax, %ebx \u4e0a\u9762\u7684\u4ee3\u7801\u5c06 EAX \u5bc4\u5b58\u5668\u7684\u503c\uff08\u53732\uff09\u52a0\u4e0a EBX \u5bc4\u5b58\u5668\u7684\u503c\uff08\u53733\uff09\uff0c\u5f97\u5230\u7ed3\u679c5\uff0c\u518d\u5c06\u8fd9\u4e2a\u7ed3\u679c\u5199\u5165\u7b2c\u4e00\u4e2a\u8fd0\u7b97\u5b50 EAX \u5bc4\u5b58\u5668\u3002 7.6 pop \u6307\u4ee4 pop \u6307\u4ee4\u7528\u4e8e\u53d6\u51fa Stack \u6700\u8fd1\u4e00\u4e2a\u5199\u5165\u7684\u503c\uff08\u5373\u6700\u4f4e\u4f4d\u5730\u5740\u7684\u503c\uff09\uff0c\u5e76\u5c06\u8fd9\u4e2a\u503c\u5199\u5165\u8fd0\u7b97\u5b50\u6307\u5b9a\u7684\u4f4d\u7f6e\u3002 pop %ebx \u4e0a\u9762\u7684\u4ee3\u7801\u8868\u793a\uff0c\u53d6\u51fa Stack \u6700\u8fd1\u5199\u5165\u7684\u503c\uff08\u5373 EBX \u5bc4\u5b58\u5668\u7684\u539f\u59cb\u503c\uff09\uff0c\u518d\u5c06\u8fd9\u4e2a\u503c\u5199\u56de EBX \u5bc4\u5b58\u5668\uff08\u56e0\u4e3a\u52a0\u6cd5\u5df2\u7ecf\u505a\u5b8c\u4e86\uff0cEBX \u5bc4\u5b58\u5668\u7528\u4e0d\u5230\u4e86\uff09\u3002 \u6ce8\u610f\uff0c pop \u6307\u4ee4\u8fd8\u4f1a\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\u52a04\uff0c\u5373\u56de\u65364\u4e2a\u5b57\u8282\u3002 7.7 ret \u6307\u4ee4 ret \u6307\u4ee4\u7528\u4e8e\u7ec8\u6b62\u5f53\u524d\u51fd\u6570\u7684\u6267\u884c\uff0c\u5c06\u8fd0\u884c\u6743\u4ea4\u8fd8\u7ed9\u4e0a\u5c42\u51fd\u6570\u3002\u4e5f\u5c31\u662f\uff0c\u5f53\u524d\u51fd\u6570\u7684\u5e27\u5c06\u88ab\u56de\u6536\u3002 ret \u53ef\u4ee5\u770b\u5230\uff0c\u8be5\u6307\u4ee4\u6ca1\u6709\u8fd0\u7b97\u5b50\u3002 \u968f\u7740 add_a_and_b \u51fd\u6570\u7ec8\u6b62\u6267\u884c\uff0c\u7cfb\u7edf\u5c31\u56de\u5230\u521a\u624d main \u51fd\u6570\u4e2d\u65ad\u7684\u5730\u65b9\uff0c\u7ee7\u7eed\u5f80\u4e0b\u6267\u884c\u3002 add %esp, 8 \u4e0a\u9762\u7684\u4ee3\u7801\u8868\u793a\uff0c\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\uff0c\u624b\u52a8\u52a0\u4e0a8\u4e2a\u5b57\u8282\uff0c\u518d\u5199\u56de ESP \u5bc4\u5b58\u5668\u3002\u8fd9\u662f\u56e0\u4e3a ESP \u5bc4\u5b58\u5668\u7684\u662f Stack \u7684\u5199\u5165\u5f00\u59cb\u5730\u5740\uff0c\u524d\u9762\u7684 pop \u64cd\u4f5c\u5df2\u7ecf\u56de\u6536\u4e864\u4e2a\u5b57\u8282\uff0c\u8fd9\u91cc\u518d\u56de\u65368\u4e2a\u5b57\u8282\uff0c\u7b49\u4e8e\u5168\u90e8\u56de\u6536\u3002 ret \u6700\u540e\uff0c main \u51fd\u6570\u8fd0\u884c\u7ed3\u675f\uff0c ret \u6307\u4ee4\u9000\u51fa\u7a0b\u5e8f\u6267\u884c\u3002 \u516b\u3001\u53c2\u8003\u94fe\u63a5 Introduction to reverse engineering and Assembly , by Youness sAlaoui x86 Assembly Guide , by University of Virginia Computer Science","title":"Introduce to assembly language"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#_1","text":"\u6211\u4eec\u77e5\u9053\uff0cCPU \u53ea\u8d1f\u8d23\u8ba1\u7b97\uff0c\u672c\u8eab\u4e0d\u5177\u5907\u667a\u80fd\u3002\u4f60\u8f93\u5165\u4e00\u6761\u6307\u4ee4\uff08instruction\uff09\uff0c\u5b83\u5c31\u8fd0\u884c\u4e00\u6b21\uff0c\u7136\u540e\u505c\u4e0b\u6765\uff0c\u7b49\u5f85\u4e0b\u4e00\u6761\u6307\u4ee4\u3002 \u8fd9\u4e9b**\u6307\u4ee4**\u90fd\u662f**\u4e8c\u8fdb\u5236**\u7684\uff0c\u79f0\u4e3a**\u64cd\u4f5c\u7801**\uff08opcode\uff09\uff0c\u6bd4\u5982\u52a0\u6cd5\u6307\u4ee4\u5c31\u662f 00000011 \u3002 \u7f16\u8bd1\u5668 \u7684\u4f5c\u7528\uff0c\u5c31\u662f\u5c06\u9ad8\u7ea7\u8bed\u8a00\u5199\u597d\u7684\u7a0b\u5e8f\uff0c\u7ffb\u8bd1\u6210\u4e00\u6761\u6761\u64cd\u4f5c\u7801\u3002 \u5bf9\u4e8e\u4eba\u7c7b\u6765\u8bf4\uff0c \u4e8c\u8fdb\u5236\u7a0b\u5e8f**\u662f\u4e0d\u53ef\u8bfb\u7684\uff0c\u6839\u672c\u770b\u4e0d\u51fa\u6765\u673a\u5668\u5e72\u4e86\u4ec0\u4e48\u3002\u4e3a\u4e86\u89e3\u51b3\u53ef\u8bfb\u6027\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5076\u5c14\u7684\u7f16\u8f91\u9700\u6c42\uff0c\u5c31\u8bde\u751f\u4e86**\u6c47\u7f16\u8bed\u8a00 \u3002( ) \u6c47\u7f16\u8bed\u8a00\u662f\u4e8c\u8fdb\u5236\u6307\u4ee4\u7684\u6587\u672c\u5f62\u5f0f \uff0c\u4e0e**\u6307\u4ee4**\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\u5173\u7cfb\u3002\u6bd4\u5982\uff0c\u52a0\u6cd5\u6307\u4ee4 00000011 \u5199\u6210\u6c47\u7f16\u8bed\u8a00\u5c31\u662f ADD\u3002\u53ea\u8981\u8fd8\u539f\u6210\u4e8c\u8fdb\u5236\uff0c**\u6c47\u7f16\u8bed\u8a00**\u5c31\u53ef\u4ee5\u88ab CPU \u76f4\u63a5\u6267\u884c\uff0c\u6240\u4ee5\u5b83\u662f\u6700\u5e95\u5c42\u7684\u4f4e\u7ea7\u8bed\u8a00\u3002","title":"\u4e00\u3001\u6c47\u7f16\u8bed\u8a00\u662f\u4ec0\u4e48\uff1f"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#_2","text":"\u6700\u65e9\u7684\u65f6\u5019\uff0c\u7f16\u5199\u7a0b\u5e8f\u5c31\u662f\u624b\u5199\u4e8c\u8fdb\u5236\u6307\u4ee4\uff0c\u7136\u540e\u901a\u8fc7\u5404\u79cd\u5f00\u5173\u8f93\u5165\u8ba1\u7b97\u673a\uff0c\u6bd4\u5982\u8981\u505a\u52a0\u6cd5\u4e86\uff0c\u5c31\u6309\u4e00\u4e0b\u52a0\u6cd5\u5f00\u5173\u3002\u540e\u6765\uff0c\u53d1\u660e\u4e86\u7eb8\u5e26\u6253\u5b54\u673a\uff0c\u901a\u8fc7\u5728\u7eb8\u5e26\u4e0a\u6253\u5b54\uff0c\u5c06**\u4e8c\u8fdb\u5236\u6307\u4ee4**\u81ea\u52a8\u8f93\u5165\u8ba1\u7b97\u673a\u3002 \u4e3a\u4e86\u89e3\u51b3\u4e8c\u8fdb\u5236\u6307\u4ee4\u7684\u53ef\u8bfb\u6027\u95ee\u9898\uff0c\u5de5\u7a0b\u5e08\u5c06\u90a3\u4e9b\u6307\u4ee4\u5199\u6210\u4e86\u516b\u8fdb\u5236\u3002\u4e8c\u8fdb\u5236\u8f6c\u516b\u8fdb\u5236\u662f\u8f7b\u800c\u6613\u4e3e\u7684\uff0c\u4f46\u662f\u516b\u8fdb\u5236\u7684\u53ef\u8bfb\u6027\u4e5f\u4e0d\u884c\u3002\u5f88\u81ea\u7136\u5730\uff0c\u6700\u540e\u8fd8\u662f\u7528\u6587\u5b57\u8868\u8fbe\uff0c\u52a0\u6cd5\u6307\u4ee4\u5199\u6210 ADD\u3002**\u5185\u5b58\u5730\u5740**\u4e5f\u4e0d\u518d\u76f4\u63a5\u5f15\u7528\uff0c\u800c\u662f\u7528**\u6807\u7b7e**\u8868\u793a\u3002 \u8fd9\u6837\u7684\u8bdd\uff0c\u5c31\u591a\u51fa\u4e00\u4e2a\u6b65\u9aa4\uff0c\u8981\u628a\u8fd9\u4e9b**\u6587\u5b57\u6307\u4ee4**\u7ffb\u8bd1\u6210**\u4e8c\u8fdb\u5236**\uff0c\u8fd9\u4e2a\u6b65\u9aa4\u5c31\u79f0\u4e3a assembling \uff0c\u5b8c\u6210\u8fd9\u4e2a\u6b65\u9aa4\u7684\u7a0b\u5e8f\u5c31\u53eb\u505a assembler \u3002\u5b83\u5904\u7406\u7684\u6587\u672c\uff0c\u81ea\u7136\u5c31\u53eb\u505a aseembly code \u3002\u6807\u51c6\u5316\u4ee5\u540e\uff0c\u79f0\u4e3a~\uff0c\u7f29\u5199\u4e3a asm \uff0c\u4e2d\u6587\u8bd1\u4e3a**\u6c47\u7f16\u8bed\u8a00**\u3002 \u6bcf\u4e00\u79cd CPU \u7684\u673a\u5668\u6307\u4ee4\u90fd\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u6c47\u7f16\u8bed\u8a00\u4e5f\u4e0d\u4e00\u6837\u3002\u672c\u6587\u4ecb\u7ecd\u7684\u662f\u76ee\u524d\u6700\u5e38\u89c1\u7684 x86 \u6c47\u7f16\u8bed\u8a00\uff0c\u5373 Intel \u516c\u53f8\u7684 CPU \u4f7f\u7528\u7684\u90a3\u4e00\u79cd\u3002","title":"\u4e8c\u3001\u6765\u5386"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#_3","text":"\u5b66\u4e60\u6c47\u7f16\u8bed\u8a00\uff0c\u9996\u5148\u5fc5\u987b\u4e86\u89e3\u4e24\u4e2a\u77e5\u8bc6\u70b9\uff1a\u5bc4\u5b58\u5668\u548c\u5185\u5b58\u6a21\u578b\u3002 \u5148\u6765\u770b\u5bc4\u5b58\u5668\u3002CPU \u672c\u8eab\u53ea\u8d1f\u8d23\u8fd0\u7b97\uff0c\u4e0d\u8d1f\u8d23\u50a8\u5b58\u6570\u636e\u3002\u6570\u636e\u4e00\u822c\u90fd\u50a8\u5b58\u5728\u5185\u5b58\u4e4b\u4e2d\uff0cCPU \u8981\u7528\u7684\u65f6\u5019\u5c31\u53bb\u5185\u5b58\u8bfb\u5199\u6570\u636e\u3002\u4f46\u662f\uff0c CPU \u7684\u8fd0\u7b97\u901f\u5ea6\u8fdc\u9ad8\u4e8e\u5185\u5b58\u7684\u8bfb\u5199\u901f\u5ea6 \uff0c\u4e3a\u4e86\u907f\u514d\u88ab\u62d6\u6162\uff0cCPU \u90fd\u81ea\u5e26**\u4e00\u7ea7\u7f13\u5b58**\u548c**\u4e8c\u7ea7\u7f13\u5b58**\u3002\u57fa\u672c\u4e0a\uff0c**CPU \u7f13\u5b58**\u53ef\u4ee5\u770b\u4f5c\u662f\u8bfb\u5199\u901f\u5ea6\u8f83\u5feb\u7684\u5185\u5b58\u3002 \u4f46\u662f\uff0c CPU \u7f13\u5b58**\u8fd8\u662f\u4e0d\u591f\u5feb\uff0c\u53e6\u5916\u6570\u636e\u5728\u7f13\u5b58\u91cc\u9762\u7684**\u5730\u5740\u662f\u4e0d\u56fa\u5b9a**\u7684\uff0cCPU \u6bcf\u6b21\u8bfb\u5199\u90fd\u8981**\u5bfb\u5740**\u4e5f\u4f1a\u62d6\u6162\u901f\u5ea6\u3002\u56e0\u6b64\uff0c\u9664\u4e86\u7f13\u5b58\u4e4b\u5916\uff0cCPU \u8fd8\u81ea\u5e26\u4e86**\u5bc4\u5b58\u5668 \uff08register\uff09\uff0c\u7528\u6765\u50a8\u5b58\u6700\u5e38\u7528\u7684\u6570\u636e\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u90a3\u4e9b\u6700\u9891\u7e41\u8bfb\u5199\u7684\u6570\u636e\uff08\u6bd4\u5982\u5faa\u73af\u53d8\u91cf\uff09\uff0c\u90fd\u4f1a\u653e\u5728\u5bc4\u5b58\u5668\u91cc\u9762\uff0cCPU \u4f18\u5148\u8bfb\u5199**\u5bc4\u5b58\u5668**\uff0c\u518d\u7531**\u5bc4\u5b58\u5668**\u8ddf\u5185\u5b58\u4ea4\u6362\u6570\u636e\u3002 \u5bc4\u5b58\u5668\u4e0d\u4f9d\u9760\u5730\u5740\u533a\u5206\u6570\u636e\uff0c\u800c\u4f9d\u9760\u540d\u79f0 \u3002\u6bcf\u4e00\u4e2a**\u5bc4\u5b58\u5668\u90fd**\u6709\u81ea\u5df1\u7684**\u540d\u79f0**\uff0c\u6211\u4eec\u544a\u8bc9 CPU \u53bb\u5177\u4f53\u7684\u54ea\u4e00\u4e2a\u5bc4\u5b58\u5668\u62ff\u6570\u636e\uff0c\u8fd9\u6837\u7684\u901f\u5ea6\u662f\u6700\u5feb\u7684\u3002\u6709\u4eba\u6bd4\u55bb\u5bc4\u5b58\u5668\u662f CPU \u7684**\u96f6\u7ea7\u7f13\u5b58**\u3002","title":"\u4e09\u3001\u5bc4\u5b58\u5668"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#_4","text":"\u65e9\u671f\u7684 x86 CPU \u53ea\u67098\u4e2a\u5bc4\u5b58\u5668\uff0c\u800c\u4e14\u6bcf\u4e2a\u90fd\u6709\u4e0d\u540c\u7684\u7528\u9014\u3002\u73b0\u5728\u7684\u5bc4\u5b58\u5668\u5df2\u7ecf\u6709100\u591a\u4e2a\u4e86\uff0c\u90fd\u53d8\u6210**\u901a\u7528\u5bc4\u5b58\u5668**\uff0c\u4e0d\u7279\u522b\u6307\u5b9a\u7528\u9014\u4e86\uff0c\u4f46\u662f\u65e9\u671f\u5bc4\u5b58\u5668\u7684\u540d\u5b57\u90fd\u88ab\u4fdd\u5b58\u4e86\u4e0b\u6765\u3002 EAX EBX ECX EDX EDI ESI EBP ESP \u4e0a\u9762\u8fd98\u4e2a\u5bc4\u5b58\u5668\u4e4b\u4e2d\uff0c\u524d\u9762\u4e03\u4e2a\u90fd\u662f\u901a\u7528\u7684\u3002 ESP \u5bc4\u5b58\u5668**\u6709\u7279\u5b9a\u7528\u9014\uff0c\u4fdd\u5b58\u5f53\u524d **Stack \u7684\u5730\u5740\uff08\u8be6\u89c1\u4e0b\u4e00\u8282\uff09\u3002 \u6211\u4eec\u5e38\u5e38\u770b\u5230 32\u4f4d CPU\u300164\u4f4d CPU \u8fd9\u6837\u7684\u540d\u79f0\uff0c\u5176\u5b9e\u6307\u7684\u5c31\u662f\u5bc4\u5b58\u5668\u7684\u5927\u5c0f\u300232 \u4f4d CPU \u7684\u5bc4\u5b58\u5668\u5927\u5c0f\u5c31\u662f4\u4e2a\u5b57\u8282\u3002","title":"\u56db\u3001\u5bc4\u5b58\u5668\u7684\u79cd\u7c7b"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#heap","text":"**\u5bc4\u5b58\u5668**\u53ea\u80fd\u5b58\u653e\u5f88\u5c11\u91cf\u7684\u6570\u636e\uff0c\u5927\u591a\u6570\u65f6\u5019\uff0cCPU \u8981\u6307\u6325\u5bc4\u5b58\u5668\uff0c\u76f4\u63a5\u8ddf\u5185\u5b58\u4ea4\u6362\u6570\u636e\u3002\u6240\u4ee5\uff0c\u9664\u4e86\u5bc4\u5b58\u5668\uff0c\u8fd8\u5fc5\u987b\u4e86\u89e3\u5185\u5b58\u600e\u4e48\u50a8\u5b58\u6570\u636e\u3002 \u7a0b\u5e8f\u8fd0\u884c\u7684\u65f6\u5019\uff0c \u64cd\u4f5c\u7cfb\u7edf**\u4f1a\u7ed9\u5b83\u5206\u914d\u4e00\u6bb5\u5185\u5b58\uff0c\u7528\u6765\u50a8\u5b58\u7a0b\u5e8f\u548c\u8fd0\u884c\u4ea7\u751f\u7684\u6570\u636e\u3002\u8fd9\u6bb5\u5185\u5b58\u6709**\u8d77\u59cb\u5730\u5740**\u548c**\u7ed3\u675f\u5730\u5740 \uff0c\u6bd4\u5982\u4ece 0x1000 \u5230 0x8000 \uff0c\u8d77\u59cb\u5730\u5740\u662f\u8f83\u5c0f\u7684\u90a3\u4e2a\u5730\u5740\uff0c\u7ed3\u675f\u5730\u5740\u662f\u8f83\u5927\u7684\u90a3\u4e2a\u5730\u5740\u3002 \u7a0b\u5e8f\u8fd0\u884c\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u4e8e\u52a8\u6001\u7684\u5185\u5b58\u5360\u7528\u8bf7\u6c42\uff08\u6bd4\u5982\u65b0\u5efa\u5bf9\u8c61\uff0c\u6216\u8005\u4f7f\u7528 malloc \u547d\u4ee4\uff09\uff0c\u7cfb\u7edf\u5c31\u4f1a\u4ece\u9884\u5148\u5206\u914d\u597d\u7684\u90a3\u6bb5\u5185\u5b58\u4e4b\u4e2d\uff0c\u5212\u51fa\u4e00\u90e8\u5206\u7ed9\u7528\u6237\uff0c\u5177\u4f53\u89c4\u5219\u662f\u4ece**\u8d77\u59cb\u5730\u5740**\u5f00\u59cb\u5212\u5206\uff08\u5b9e\u9645\u4e0a\uff0c**\u8d77\u59cb\u5730\u5740**\u4f1a\u6709\u4e00\u6bb5\u9759\u6001\u6570\u636e\uff0c\u8fd9\u91cc\u5ffd\u7565\uff09\u3002\u4e3e\u4f8b\u6765\u8bf4\uff0c\u7528\u6237\u8981\u6c42\u5f97\u523010\u4e2a\u5b57\u8282\u5185\u5b58\uff0c\u90a3\u4e48\u4ece\u8d77\u59cb\u5730\u5740 0x1000 \u5f00\u59cb\u7ed9\u4ed6\u5206\u914d\uff0c\u4e00\u76f4\u5206\u914d\u5230\u5730\u5740 0x100A \uff0c\u5982\u679c\u518d\u8981\u6c42\u5f97\u523022\u4e2a\u5b57\u8282\uff0c\u90a3\u4e48\u5c31\u5206\u914d\u5230 0x1020 \u3002 \u8fd9\u79cd\u56e0\u4e3a\u7528\u6237\u4e3b\u52a8\u8bf7\u6c42\u800c\u5212\u5206\u51fa\u6765\u7684\u5185\u5b58\u533a\u57df\uff0c\u53eb\u505a Heap \uff08\u5806\uff09\u3002\u5b83\u7531\u8d77\u59cb\u5730\u5740\u5f00\u59cb\uff0c\u4ece\u4f4e\u4f4d\uff08\u5730\u5740\uff09\u5411\u9ad8\u4f4d\uff08\u5730\u5740\uff09\u589e\u957f\u3002Heap \u7684\u4e00\u4e2a\u91cd\u8981\u7279\u70b9\u5c31\u662f\u4e0d\u4f1a\u81ea\u52a8\u6d88\u5931\uff0c\u5fc5\u987b\u624b\u52a8\u91ca\u653e\uff0c\u6216\u8005\u7531\u5783\u573e\u56de\u6536\u673a\u5236\u6765\u56de\u6536\u3002","title":"\u4e94\u3001\u5185\u5b58\u6a21\u578b\uff1aHeap"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#stack","text":"\u9664\u4e86 Heap \u4ee5\u5916\uff0c\u5176\u4ed6\u7684\u5185\u5b58\u5360\u7528\u53eb\u505a Stack \uff08\u6808\uff09\u3002\u7b80\u5355\u8bf4\uff0cStack \u662f\u7531\u4e8e\u51fd\u6570\u8fd0\u884c\u800c\u4e34\u65f6\u5360\u7528\u7684\u5185\u5b58\u533a\u57df\u3002 \u8bf7\u770b\u4e0b\u9762\u7684\u4f8b\u5b50\u3002 int main() { int a = 2; int b = 3; } \u4e0a\u9762\u4ee3\u7801\u4e2d\uff0c\u7cfb\u7edf\u5f00\u59cb\u6267\u884c main \u51fd\u6570\u65f6\uff0c\u4f1a\u4e3a\u5b83\u5728\u5185\u5b58\u91cc\u9762\u5efa\u7acb\u4e00\u4e2a**\u5e27**\uff08frame\uff09\uff0c\u6240\u6709 main \u7684\u5185\u90e8\u53d8\u91cf\uff08\u6bd4\u5982 a \u548c b \uff09\u90fd\u4fdd\u5b58\u5728\u8fd9\u4e2a\u5e27\u91cc\u9762\u3002 main \u51fd\u6570\u6267\u884c\u7ed3\u675f\u540e\uff0c\u8be5**\u5e27**\u5c31\u4f1a\u88ab\u56de\u6536\uff0c\u91ca\u653e\u6240\u6709\u7684\u5185\u90e8\u53d8\u91cf\uff0c\u4e0d\u518d\u5360\u7528\u7a7a\u95f4\u3002 \u5982\u679c\u51fd\u6570\u5185\u90e8\u8c03\u7528\u4e86\u5176\u4ed6\u51fd\u6570\uff0c\u4f1a\u53d1\u751f\u4ec0\u4e48\u60c5\u51b5\uff1f int main() { int a = 2; int b = 3; return add_a_and_b(a, b); } \u4e0a\u9762\u4ee3\u7801\u4e2d\uff0c main \u51fd\u6570\u5185\u90e8\u8c03\u7528\u4e86 add_a_and_b \u51fd\u6570\u3002\u6267\u884c\u5230\u8fd9\u4e00\u884c\u7684\u65f6\u5019\uff0c\u7cfb\u7edf\u4e5f\u4f1a\u4e3a add_a_and_b \u65b0\u5efa\u4e00\u4e2a\u5e27\uff0c\u7528\u6765\u50a8\u5b58\u5b83\u7684\u5185\u90e8\u53d8\u91cf\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u6b64\u65f6\u540c\u65f6\u5b58\u5728\u4e24\u4e2a\u5e27\uff1a main \u548c add_a_and_b \u3002\u4e00\u822c\u6765\u8bf4\uff0c\u8c03\u7528\u6808\u6709\u591a\u5c11\u5c42\uff0c\u5c31\u6709\u591a\u5c11\u5e27\u3002 \u7b49\u5230 add_a_and_b \u8fd0\u884c\u7ed3\u675f\uff0c\u5b83\u7684\u5e27\u5c31\u4f1a\u88ab\u56de\u6536\uff0c\u7cfb\u7edf\u4f1a\u56de\u5230\u51fd\u6570 main \u521a\u624d\u4e2d\u65ad\u6267\u884c\u7684\u5730\u65b9\uff0c\u7ee7\u7eed\u5f80\u4e0b\u6267\u884c\u3002\u901a\u8fc7\u8fd9\u79cd\u673a\u5236\uff0c\u5c31\u5b9e\u73b0\u4e86\u51fd\u6570\u7684\u5c42\u5c42\u8c03\u7528\uff0c\u5e76\u4e14\u6bcf\u4e00\u5c42\u90fd\u80fd\u4f7f\u7528\u81ea\u5df1\u7684\u672c\u5730\u53d8\u91cf\u3002 \u6240\u6709\u7684\u5e27\u90fd\u5b58\u653e\u5728 Stack\uff0c\u7531\u4e8e\u5e27\u662f\u4e00\u5c42\u5c42\u53e0\u52a0\u7684\uff0c\u6240\u4ee5 Stack \u53eb\u505a\u6808\u3002\u751f\u6210\u65b0\u7684\u5e27\uff0c\u53eb\u505a\"\u5165\u6808\"\uff0c\u82f1\u6587\u662f push\uff1b\u6808\u7684\u56de\u6536\u53eb\u505a\"\u51fa\u6808\"\uff0c\u82f1\u6587\u662f pop\u3002Stack \u7684\u7279\u70b9\u5c31\u662f\uff0c\u6700\u665a\u5165\u6808\u7684\u5e27\u6700\u65e9\u51fa\u6808\uff08\u56e0\u4e3a\u6700\u5185\u5c42\u7684\u51fd\u6570\u8c03\u7528\uff0c\u6700\u5148\u7ed3\u675f\u8fd0\u884c\uff09\uff0c\u8fd9\u5c31\u53eb\u505a\"\u540e\u8fdb\u5148\u51fa\"\u7684\u6570\u636e\u7ed3\u6784\u3002\u6bcf\u4e00\u6b21\u51fd\u6570\u6267\u884c\u7ed3\u675f\uff0c\u5c31\u81ea\u52a8\u91ca\u653e\u4e00\u4e2a\u5e27\uff0c\u6240\u6709\u51fd\u6570\u6267\u884c\u7ed3\u675f\uff0c\u6574\u4e2a Stack \u5c31\u90fd\u91ca\u653e\u4e86\u3002 Stack \u662f\u7531\u5185\u5b58\u533a\u57df\u7684\u7ed3\u675f\u5730\u5740\u5f00\u59cb\uff0c\u4ece\u9ad8\u4f4d\uff08\u5730\u5740\uff09\u5411\u4f4e\u4f4d\uff08\u5730\u5740\uff09\u5206\u914d\u3002\u6bd4\u5982\uff0c\u5185\u5b58\u533a\u57df\u7684\u7ed3\u675f\u5730\u5740\u662f 0x8000 \uff0c\u7b2c\u4e00\u5e27\u5047\u5b9a\u662f16\u5b57\u8282\uff0c\u90a3\u4e48\u4e0b\u4e00\u6b21\u5206\u914d\u7684\u5730\u5740\u5c31\u4f1a\u4ece 0x7FF0 \u5f00\u59cb\uff1b\u7b2c\u4e8c\u5e27\u5047\u5b9a\u9700\u898164\u5b57\u8282\uff0c\u90a3\u4e48\u5730\u5740\u5c31\u4f1a\u79fb\u52a8\u5230 0x7FB0 \u3002","title":"\u516d\u3001\u5185\u5b58\u6a21\u578b\uff1aStack"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#cpu","text":"","title":"\u4e03\u3001CPU \u6307\u4ee4"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#71","text":"\u4e86\u89e3\u5bc4\u5b58\u5668\u548c\u5185\u5b58\u6a21\u578b\u4ee5\u540e\uff0c\u5c31\u53ef\u4ee5\u6765\u770b\u6c47\u7f16\u8bed\u8a00\u5230\u5e95\u662f\u4ec0\u4e48\u4e86\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u7a0b\u5e8f example.c \u3002 int add_a_and_b(int a, int b) { return a + b; } int main() { return add_a_and_b(2, 3); } gcc \u5c06\u8fd9\u4e2a\u7a0b\u5e8f\u8f6c\u6210\u6c47\u7f16\u8bed\u8a00\u3002 $ gcc -S example.c \u4e0a\u9762\u7684\u547d\u4ee4\u6267\u884c\u4ee5\u540e\uff0c\u4f1a\u751f\u6210\u4e00\u4e2a\u6587\u672c\u6587\u4ef6 example.s \uff0c\u91cc\u9762\u5c31\u662f\u6c47\u7f16\u8bed\u8a00\uff0c\u5305\u542b\u4e86\u51e0\u5341\u884c\u6307\u4ee4\u3002\u8fd9\u4e48\u8bf4\u5427\uff0c\u4e00\u4e2a\u9ad8\u7ea7\u8bed\u8a00\u7684\u7b80\u5355\u64cd\u4f5c\uff0c\u5e95\u5c42\u53ef\u80fd\u7531\u51e0\u4e2a\uff0c\u751a\u81f3\u51e0\u5341\u4e2a CPU \u6307\u4ee4\u6784\u6210\u3002CPU \u4f9d\u6b21\u6267\u884c\u8fd9\u4e9b\u6307\u4ee4\uff0c\u5b8c\u6210\u8fd9\u4e00\u6b65\u64cd\u4f5c\u3002 example.s \u7ecf\u8fc7\u7b80\u5316\u4ee5\u540e\uff0c\u5927\u6982\u662f\u4e0b\u9762\u7684\u6837\u5b50\u3002 _add_a_and_b: push %ebx mov %eax, [%esp+8] mov %ebx, [%esp+12] add %eax, %ebx pop %ebx ret _main: push 3 push 2 call _add_a_and_b add %esp, 8 ret \u53ef\u4ee5\u770b\u5230\uff0c\u539f\u7a0b\u5e8f\u7684\u4e24\u4e2a\u51fd\u6570 add_a_and_b \u548c main \uff0c\u5bf9\u5e94\u4e24\u4e2a\u6807\u7b7e _add_a_and_b \u548c _main \u3002\u6bcf\u4e2a\u6807\u7b7e\u91cc\u9762\u662f\u8be5\u51fd\u6570\u6240\u8f6c\u6210\u7684 CPU \u8fd0\u884c\u6d41\u7a0b\u3002 \u6bcf\u4e00\u884c\u5c31\u662f CPU \u6267\u884c\u7684\u4e00\u6b21\u64cd\u4f5c\u3002\u5b83\u53c8\u5206\u6210\u4e24\u90e8\u5206\uff0c\u5c31\u4ee5\u5176\u4e2d\u4e00\u884c\u4e3a\u4f8b\u3002 push %ebx \u8fd9\u4e00\u884c\u91cc\u9762\uff0c push \u662f CPU \u6307\u4ee4\uff0c %ebx \u662f\u8be5\u6307\u4ee4\u8981\u7528\u5230\u7684**\u8fd0\u7b97\u5b50**\u3002\u4e00\u4e2a CPU \u6307\u4ee4\u53ef\u4ee5\u6709\u96f6\u4e2a\u5230\u591a\u4e2a\u8fd0\u7b97\u5b50\u3002 \u4e0b\u9762\u6211\u5c31\u4e00\u884c\u4e00\u884c\u8bb2\u89e3\u8fd9\u4e2a\u6c47\u7f16\u7a0b\u5e8f\uff0c\u5efa\u8bae\u8bfb\u8005\u6700\u597d\u628a\u8fd9\u4e2a\u7a0b\u5e8f\uff0c\u5728\u53e6\u4e00\u4e2a\u7a97\u53e3\u62f7\u8d1d\u4e00\u4efd\uff0c\u7701\u5f97\u9605\u8bfb\u7684\u65f6\u5019\u518d\u628a\u9875\u9762\u6eda\u52a8\u4e0a\u6765\u3002","title":"7.1 \u4e00\u4e2a\u5b9e\u4f8b"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#72#push","text":"\u6839\u636e\u7ea6\u5b9a\uff0c\u7a0b\u5e8f\u4ece _main \u6807\u7b7e\u5f00\u59cb\u6267\u884c\uff0c\u8fd9\u65f6\u4f1a\u5728 Stack \u4e0a\u4e3a main \u5efa\u7acb\u4e00\u4e2a\u5e27\uff0c\u5e76\u5c06 Stack \u6240\u6307\u5411\u7684\u5730\u5740\uff0c\u5199\u5165 ESP \u5bc4\u5b58\u5668 \u3002\u540e\u9762\u5982\u679c\u6709\u6570\u636e\u8981\u5199\u5165 main \u8fd9\u4e2a\u5e27\uff0c\u5c31\u4f1a\u5199\u5728 **ESP \u5bc4\u5b58\u5668**\u6240\u4fdd\u5b58\u7684\u5730\u5740\u3002 \u7136\u540e\uff0c\u5f00\u59cb\u6267\u884c\u7b2c\u4e00\u884c\u4ee3\u7801\u3002 push 3 push \u6307\u4ee4\u7528\u4e8e\u5c06\u8fd0\u7b97\u5b50\u653e\u5165 Stack\uff0c\u8fd9\u91cc\u5c31\u662f\u5c06 3 \u5199\u5165 main \u8fd9\u4e2a\u5e27\u3002 \u867d\u7136\u770b\u4e0a\u53bb\u5f88\u7b80\u5355\uff0c push \u6307\u4ee4\u5176\u5b9e\u6709\u4e00\u4e2a\u524d\u7f6e\u64cd\u4f5c\u3002\u5b83\u4f1a\u5148\u53d6\u51fa **ESP \u5bc4\u5b58\u5668**\u91cc\u9762\u7684\u5730\u5740\uff0c\u5c06\u5176\u51cf\u53bb4\u4e2a\u5b57\u8282\uff0c\u7136\u540e\u5c06\u65b0\u5730\u5740\u5199\u5165 ESP \u5bc4\u5b58\u5668\u3002\u4f7f\u7528\u51cf\u6cd5\u662f\u56e0\u4e3a Stack \u4ece\u9ad8\u4f4d\u5411\u4f4e\u4f4d\u53d1\u5c55\uff0c4\u4e2a\u5b57\u8282\u5219\u662f\u56e0\u4e3a 3 \u7684\u7c7b\u578b\u662f int \uff0c\u5360\u75284\u4e2a\u5b57\u8282\u3002\u5f97\u5230\u65b0\u5730\u5740\u4ee5\u540e\uff0c 3 \u5c31\u4f1a\u5199\u5165\u8fd9\u4e2a\u5730\u5740\u5f00\u59cb\u7684\u56db\u4e2a\u5b57\u8282\u3002 push 2 \u7b2c\u4e8c\u884c\u4e5f\u662f\u4e00\u6837\uff0c push \u6307\u4ee4\u5c06 2 \u5199\u5165 main \u8fd9\u4e2a\u5e27\uff0c\u4f4d\u7f6e\u7d27\u8d34\u7740\u524d\u9762\u5199\u5165\u7684 3 \u3002\u8fd9\u65f6\uff0cESP \u5bc4\u5b58\u5668\u4f1a\u518d\u51cf\u53bb 4\u4e2a\u5b57\u8282\uff08\u7d2f\u8ba1\u51cf\u53bb8\uff09\u3002","title":"7.2 push \u6307\u4ee4"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#73#call","text":"\u7b2c\u4e09\u884c\u7684 call \u6307\u4ee4\u7528\u6765\u8c03\u7528\u51fd\u6570\u3002 call _add_a_and_b \u4e0a\u9762\u7684\u4ee3\u7801\u8868\u793a\u8c03\u7528 add_a_and_b \u51fd\u6570\u3002\u8fd9\u65f6\uff0c\u7a0b\u5e8f\u5c31\u4f1a\u53bb\u627e _add_a_and_b \u6807\u7b7e\uff0c\u5e76\u4e3a\u8be5\u51fd\u6570\u5efa\u7acb\u4e00\u4e2a\u65b0\u7684\u5e27\u3002 \u4e0b\u9762\u5c31\u5f00\u59cb\u6267\u884c _add_a_and_b \u7684\u4ee3\u7801\u3002 push %ebx \u8fd9\u4e00\u884c\u8868\u793a\u5c06 EBX \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u503c\uff0c\u5199\u5165 _add_a_and_b \u8fd9\u4e2a\u5e27\u3002\u8fd9\u662f\u56e0\u4e3a\u540e\u9762\u8981\u7528\u5230\u8fd9\u4e2a\u5bc4\u5b58\u5668\uff0c\u5c31\u5148\u628a\u91cc\u9762\u7684\u503c\u53d6\u51fa\u6765\uff0c\u7528\u5b8c\u540e\u518d\u5199\u56de\u53bb\u3002 \u8fd9\u65f6\uff0c push \u6307\u4ee4\u4f1a\u518d\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\u51cf\u53bb4\u4e2a\u5b57\u8282\uff08\u7d2f\u8ba1\u51cf\u53bb12\uff09\u3002","title":"7.3 call \u6307\u4ee4"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#74#mov","text":"mov \u6307\u4ee4\u7528\u4e8e\u5c06\u4e00\u4e2a\u503c\u5199\u5165\u67d0\u4e2a\u5bc4\u5b58\u5668\u3002 mov %eax, [%esp+8] \u8fd9\u4e00\u884c\u4ee3\u7801\u8868\u793a\uff0c\u5148\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\u52a0\u4e0a8\u4e2a\u5b57\u8282\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684\u5730\u5740\uff0c\u7136\u540e\u6309\u7167\u8fd9\u4e2a\u5730\u5740\u5728 Stack \u53d6\u51fa\u6570\u636e\u3002\u6839\u636e\u524d\u9762\u7684\u6b65\u9aa4\uff0c\u53ef\u4ee5\u63a8\u7b97\u51fa\u8fd9\u91cc\u53d6\u51fa\u7684\u662f 2 \uff0c\u518d\u5c06 2 \u5199\u5165 EAX \u5bc4\u5b58\u5668\u3002 \u4e0b\u4e00\u884c\u4ee3\u7801\u4e5f\u662f\u5e72\u540c\u6837\u7684\u4e8b\u60c5\u3002 mov %ebx, [%esp+12] \u4e0a\u9762\u7684\u4ee3\u7801\u5c06 ESP \u5bc4\u5b58\u5668\u7684\u503c\u52a012\u4e2a\u5b57\u8282\uff0c\u518d\u6309\u7167\u8fd9\u4e2a\u5730\u5740\u5728 Stack \u53d6\u51fa\u6570\u636e\uff0c\u8fd9\u6b21\u53d6\u51fa\u7684\u662f 3 \uff0c\u5c06\u5176\u5199\u5165 EBX \u5bc4\u5b58\u5668\u3002","title":"7.4 mov \u6307\u4ee4"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#75#add","text":"add \u6307\u4ee4\u7528\u4e8e\u5c06\u4e24\u4e2a\u8fd0\u7b97\u5b50\u76f8\u52a0\uff0c\u5e76\u5c06\u7ed3\u679c\u5199\u5165\u7b2c\u4e00\u4e2a\u8fd0\u7b97\u5b50\u3002 add %eax, %ebx \u4e0a\u9762\u7684\u4ee3\u7801\u5c06 EAX \u5bc4\u5b58\u5668\u7684\u503c\uff08\u53732\uff09\u52a0\u4e0a EBX \u5bc4\u5b58\u5668\u7684\u503c\uff08\u53733\uff09\uff0c\u5f97\u5230\u7ed3\u679c5\uff0c\u518d\u5c06\u8fd9\u4e2a\u7ed3\u679c\u5199\u5165\u7b2c\u4e00\u4e2a\u8fd0\u7b97\u5b50 EAX \u5bc4\u5b58\u5668\u3002","title":"7.5 add \u6307\u4ee4"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#76#pop","text":"pop \u6307\u4ee4\u7528\u4e8e\u53d6\u51fa Stack \u6700\u8fd1\u4e00\u4e2a\u5199\u5165\u7684\u503c\uff08\u5373\u6700\u4f4e\u4f4d\u5730\u5740\u7684\u503c\uff09\uff0c\u5e76\u5c06\u8fd9\u4e2a\u503c\u5199\u5165\u8fd0\u7b97\u5b50\u6307\u5b9a\u7684\u4f4d\u7f6e\u3002 pop %ebx \u4e0a\u9762\u7684\u4ee3\u7801\u8868\u793a\uff0c\u53d6\u51fa Stack \u6700\u8fd1\u5199\u5165\u7684\u503c\uff08\u5373 EBX \u5bc4\u5b58\u5668\u7684\u539f\u59cb\u503c\uff09\uff0c\u518d\u5c06\u8fd9\u4e2a\u503c\u5199\u56de EBX \u5bc4\u5b58\u5668\uff08\u56e0\u4e3a\u52a0\u6cd5\u5df2\u7ecf\u505a\u5b8c\u4e86\uff0cEBX \u5bc4\u5b58\u5668\u7528\u4e0d\u5230\u4e86\uff09\u3002 \u6ce8\u610f\uff0c pop \u6307\u4ee4\u8fd8\u4f1a\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\u52a04\uff0c\u5373\u56de\u65364\u4e2a\u5b57\u8282\u3002","title":"7.6 pop \u6307\u4ee4"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#77#ret","text":"ret \u6307\u4ee4\u7528\u4e8e\u7ec8\u6b62\u5f53\u524d\u51fd\u6570\u7684\u6267\u884c\uff0c\u5c06\u8fd0\u884c\u6743\u4ea4\u8fd8\u7ed9\u4e0a\u5c42\u51fd\u6570\u3002\u4e5f\u5c31\u662f\uff0c\u5f53\u524d\u51fd\u6570\u7684\u5e27\u5c06\u88ab\u56de\u6536\u3002 ret \u53ef\u4ee5\u770b\u5230\uff0c\u8be5\u6307\u4ee4\u6ca1\u6709\u8fd0\u7b97\u5b50\u3002 \u968f\u7740 add_a_and_b \u51fd\u6570\u7ec8\u6b62\u6267\u884c\uff0c\u7cfb\u7edf\u5c31\u56de\u5230\u521a\u624d main \u51fd\u6570\u4e2d\u65ad\u7684\u5730\u65b9\uff0c\u7ee7\u7eed\u5f80\u4e0b\u6267\u884c\u3002 add %esp, 8 \u4e0a\u9762\u7684\u4ee3\u7801\u8868\u793a\uff0c\u5c06 ESP \u5bc4\u5b58\u5668\u91cc\u9762\u7684\u5730\u5740\uff0c\u624b\u52a8\u52a0\u4e0a8\u4e2a\u5b57\u8282\uff0c\u518d\u5199\u56de ESP \u5bc4\u5b58\u5668\u3002\u8fd9\u662f\u56e0\u4e3a ESP \u5bc4\u5b58\u5668\u7684\u662f Stack \u7684\u5199\u5165\u5f00\u59cb\u5730\u5740\uff0c\u524d\u9762\u7684 pop \u64cd\u4f5c\u5df2\u7ecf\u56de\u6536\u4e864\u4e2a\u5b57\u8282\uff0c\u8fd9\u91cc\u518d\u56de\u65368\u4e2a\u5b57\u8282\uff0c\u7b49\u4e8e\u5168\u90e8\u56de\u6536\u3002 ret \u6700\u540e\uff0c main \u51fd\u6570\u8fd0\u884c\u7ed3\u675f\uff0c ret \u6307\u4ee4\u9000\u51fa\u7a0b\u5e8f\u6267\u884c\u3002","title":"7.7 ret \u6307\u4ee4"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/introduce-to-assembly-language/#_5","text":"Introduction to reverse engineering and Assembly , by Youness sAlaoui x86 Assembly Guide , by University of Virginia Computer Science","title":"\u516b\u3001\u53c2\u8003\u94fe\u63a5"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/multithreading-and-stack-frame/","text":"\u4eca\u5929\u5728\u9605\u8bfb\u6808\u5e27\u7684\u4ecb\u7ecd\u7684\u65f6\u5019\uff0c\u60f3\u5230\u4e86\u8fd9\u6837\u7684\u4e00\u4e2a\u95ee\u9898\uff1a\u5f53\u6211\u4eec\u4f7f\u7528\u591a\u7ebf\u7a0b\u7684\u65f6\u5019\uff0c\u662f\u53ef\u4ee5\u540c\u65f6\u6267\u884c\u6bcf\u4e2a\u7ebf\u7a0b\u7684\u51fd\u6570\u7684\uff0c\u65e2\u7136\u662f\u540c\u65f6\u6267\u884c\uff0c\u90a3\u4e48\u8fd9\u662f\u5426\u8868\u660e\u6bcf\u4e2a\u7ebf\u7a0b\u90fd\u6709\u81ea\u5df1\u72ec\u7acb\u7684\u5806\u6808\u3002\u663e\u7136\u8fd9\u662f\u9700\u8981\u786c\u4ef6\u652f\u6301\u7684\u3002","title":"Multithreading and stack frame"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/opcode/","text":"","title":"Opcode"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/stack%20frame/","text":"\u6d45\u8c08\u6808\u5e27 \u4ec0\u4e48\u662f\u6808\u5e27\uff1f \u4ec0\u4e48\u662f**\u6808\u5e27**\uff0c\u9996\u5148\u5f15\u7528\u767e\u5ea6\u767e\u79d1\u7684\u7ecf\u5178\u89e3\u91ca\uff1a \u6808\u5e27**\u4e5f\u53eb**\u8fc7\u7a0b\u6d3b\u52a8\u8bb0\u5f55 \uff0c\u662f\u7f16\u8bd1\u5668\u7528\u6765\u5b9e\u73b0\u8fc7\u7a0b/\u51fd\u6570\u8c03\u7528\u7684\u4e00\u79cd**\u6570\u636e\u7ed3\u6784**\u3002 \u5b9e\u9645\u4e0a\uff0c\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff1a \u6808\u5e27**\u5c31\u662f\u5b58\u50a8\u5728**\u7528\u6237\u6808**\u4e0a\u7684\uff08\u5f53\u7136**\u5185\u6838\u6808**\u540c\u6837\u9002\u7528\uff09\u6bcf\u4e00\u6b21**\u51fd\u6570\u8c03\u7528**\u6d89\u53ca\u7684\u76f8\u5173\u4fe1\u606f\u7684**\u8bb0\u5f55\u5355\u5143 \u3002\u4e5f\u8bb8\u8fd9\u6837\u611f\u89c9\u66f4\u590d\u6742\u4e86\uff0c\u597d\u5427\uff0c\u8ba9\u6211\u4eec\u4ece**\u6808**\u5f00\u59cb\u6765\u7406\u89e3\u4ec0\u4e48\u662f**\u6808\u5e27**... \u6808\u5e27**\u8868\u793a\u7a0b\u5e8f\u7684\u51fd\u6570\u8c03\u7528\u8bb0\u5f55\uff0c\u800c**\u6808\u5e27**\u53c8\u662f\u8bb0\u5f55\u5728**\u6808**\u4e0a\u9762\uff0c\u5f88\u660e\u663e\u6808\u4e0a\u4fdd\u6301\u4e86N\u4e2a\u6808\u5e27\u7684\u5b9e\u4f53\uff0c\u90a3\u5c31\u53ef\u4ee5\u8bf4**\u6808\u5e27**\u5c06\u6808\u5206\u5272\u6210\u4e86N\u4e2a\u8bb0\u5f55\u5757\uff0c\u4f46\u662f\u8fd9\u4e9b\u8bb0\u5f55\u5757\u5927\u5c0f\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u56e0\u4e3a**\u6808\u5e27**\u4e0d\u4ec5\u4fdd\u5b58\u8bf8\u5982\uff1a\u51fd\u6570\u5165\u53c2\u3001\u51fa\u53c2\u3001\u8fd4\u56de\u5730\u5740\u548c\u4e0a\u4e00\u4e2a\u6808\u5e27\u7684**\u6808\u5e95\u6307\u9488**\u7b49\u4fe1\u606f\uff0c\u8fd8\u4fdd\u5b58\u4e86\u51fd\u6570\u5185\u90e8\u7684**\u81ea\u52a8\u53d8\u91cf \uff08\u751a\u81f3\u53ef\u4ee5\u662f\u52a8\u6001\u5206\u914d\u5185\u5b58\uff0calloca\u51fd\u6570\u5c31\u53ef\u4ee5\u5b9e\u73b0\uff0c\u4f46\u5728\u67d0\u4e9b\u7cfb\u7edf\u4e2d\u4e0d\u884c\uff09\uff0c\u56e0\u6b64\uff0c\u4e0d\u662f\u6240\u6709\u7684\u6808\u5e27\u7684\u5927\u5c0f\u90fd\u76f8\u540c\u3002 \u662f\u4e0d\u662f\u6bcf\u4e00\u4e2a\u51fd\u6570\u8c03\u7528\u5c31\u5bf9\u5e94\u4e00\u4e2a\u6808\u5e27\uff1f \u7406\u89e3 \u6211\u4eec\u62ff\u4e00\u4e2a\u5b9e\u4f8b\u6765\u63a2\u8ba8\u4e00\u4e0b \u8fd9\u662f\u4e00\u4e2a\u5728linux\u4e0b\u8fd0\u884c\u7684\u7a0b\u5e8f\uff0c\u5f53\u7a0b\u5e8f\u8d70\u5230 *p=fun \u65f6\u4f1a\u8c03\u7528 fun \u51fd\u6570\uff0c\u6700\u540e\u6267\u884c reboot \u6307\u4ee4\u5173\u673a\u91cd\u542f\u3002\u8fd9\u662f\u4e3a\u4ec0\u4e48\u5462\uff1f\u6211\u4eec\u5c31\u62ff\u6808\u5e27\u6765\u770b\u4e00\u4e0b\u3002 \u9996\u5148\u4e86\u89e3\u6808\u5e27\u7684\u57fa\u672c\u7ed3\u6784\u3002 \u6808\u5e27**\u5176\u5b9e\u662f\u4e24\u4e2a**\u6307\u9488\u5bc4\u5b58\u5668 \uff0c \u5bc4\u5b58\u5668ebp**\u4e3a**\u5e27\u6307\u9488 \uff0c\u800c**\u5bc4\u5b58\u5668esp**\u4e3a**\u6808\u6307\u9488**\uff0c\u5f53\u7a0b\u5e8f\u8fd0\u884c\u65f6\uff0c**\u6808\u6307\u9488**\u53ef\u4ee5\u79fb\u52a8(\u5927\u591a\u6570\u7684\u4fe1\u606f\u7684\u8bbf\u95ee\u90fd\u662f\u901a\u8fc7**\u5e27\u6307\u9488**\u7684)\u3002\u603b\u4e4b\u7b80\u5355\u4e00\u53e5\u8bdd\uff0c\u6808\u5e27\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u7528\u6765\u63a7\u5236\u548c\u4fdd\u5b58\u4e00\u4e2a\u8fc7\u7a0b\u7684\u6240\u6709\u4fe1\u606f\u7684\u3002 \u518d\u6765\u770b\u4e00\u4e0b\u6211\u4eec\u5f53\u524d\u7a0b\u5e8f\u7684\u6808\u5e27 \u56e0\u4e3afun1\u4e2d*p=&a;\u5373p\u4e2d\u4fdd\u5b58\u7684a\u7684\u5730\u5740\uff0cp\u6307\u5411a, p--;\u4f1a\u5411\u4e0b\u8d70\uff0cp\u6307\u5411\u4e86\u8c03\u7528fun1\u51fd\u6570\u7684\u8fd4\u56de\u5730\u5740\uff0c\u7136\u540e\u518d\u5c06fun\u7684\u5730\u5740\u7ed9\u4e86P,\u8fd9\u65f6\u5019p\u5c31\u4e0d\u80fd\u6b63\u5e38\u8fd4\u56de\u4e86\uff0c\u800c\u662f\u5f53p\u8981\u8fd4\u56de\u65f6\u5374\u8c03\u7528\u4e86fun\u51fd\u6570\uff0c\u8fd9\u65f6\u5019\u5c31\u4f1a\u6267\u884creboot\u6307\u4ee4\u3002 \u4f60\u542c\u61c2\u4e86\u5417 \u51fd\u6570\u6808\u5e27\u56fe\u89e3 \u6211\u4eec\u77e5\u9053\u5185\u5b58\u7a7a\u95f4\u5927\u81f4\u53ef\u4ee5\u7528\u4e0b\u56fe\u8868\u793a\uff1a \u9996\u5148\u5e94\u8be5\u660e\u767d\uff0c\u6808\u662f\u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u5ef6\u4f38\u7684\u3002\u6bcf\u4e2a\u51fd\u6570\u7684\u6bcf\u6b21\u8c03\u7528\uff0c\u90fd\u6709\u5b83\u81ea\u5df1\u72ec\u7acb\u7684\u4e00\u4e2a\u6808\u5e27\uff0c\u8fd9\u4e2a\u6808\u5e27\u4e2d\u7ef4\u6301\u7740\u6240\u9700\u8981\u7684\u5404\u79cd\u4fe1\u606f\u3002 \u5bc4\u5b58\u5668ebp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u5e95\u90e8\uff08\u9ad8\u5730\u5740\uff09 \u5bc4\u5b58\u5668esp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u9876\u90e8\uff08\u5730\u5740\u5730\uff09 PC\u6307\u9488\uff1a\u6c38\u8fdc\u6307\u5411\u5f53\u524d\u8fd0\u884c\u7a0b\u5e8f\u6307\u4ee4\u7684\u4e0b\u4e00\u6761\u6307\u4ee4 \u800c\u51fd\u6570\u5728\u8c03\u7528\u7684\u65f6\u5019\u90fd\u662f\u5728**\u6808\u7a7a\u95f4**\u4e0a\u5f00\u8f9f\u4e00\u6bb5\u7a7a\u95f4\u4ee5\u4f9b\u51fd\u6570\u4f7f\u7528\uff0c\u6240\u4ee5\u4e0b\u9762\u6765\u8be6\u7ec6\u8c08\u4e00\u8c08\u51fd\u6570\u7684**\u6808\u5e27\u7ed3\u6784**\u3002 \u5982\u56fe\u793a\uff0c\u6808\u662f\u7531**\u9ad8\u5730\u5740**\u5411**\u4f4e\u5730\u5740**\u7684\u65b9\u5411\u751f\u957f\u7684\uff0c\u800c\u4e14\u6808\u6709\u5176**\u6808\u9876**\u548c**\u6808\u5e95**\uff0c\u5728 x86 \u7cfb\u7edf\u7684CPU\u4e2d\uff0c\u5bc4\u5b58\u5668 ebp \u4fdd\u5b58\u7684\u662f**\u6808\u5e95\u5730\u5740**\uff0c\u79f0\u4e3a**\u5e27\u6307\u9488**\uff0c\u5bc4\u5b58\u5668 esp \u4fdd\u5b58\u7684\u662f**\u6808\u9876\u5730\u5740**\uff0c\u79f0\u4e3a**\u6808\u6307\u9488**\u3002\u800c\u4e14\u8fd8\u5e94\u8be5\u660e\u786e\u4e00\u70b9\uff0c \u6808\u6307\u9488**\u548c**\u5e27\u6307\u9488**\u4e00\u6b21\u53ea\u80fd\u5b58\u50a8\u4e00\u4e2a\u5730\u5740\uff0c\u6240\u4ee5\uff0c\u4efb\u4f55\u65f6\u5019\uff0c\u8fd9\u4e00\u5bf9\u6307\u9488\u6307\u5411\u7684\u662f**\u540c\u4e00\u4e2a\u51fd\u6570**\u7684**\u6808\u5e27\u7ed3\u6784 \u3002\u5e76\u4e14 ebp \u4e00\u822c\u7531\u7cfb\u7edf\u6539\u53d8\u5b83\u7684\u503c\uff0c\u800c esp \u4f1a\u968f\u7740\u6570\u636e\u7684**\u5165\u6808**\u548c**\u51fa\u6808**\u800c\u79fb\u52a8\uff0c\u4e5f\u5c31\u662f\u8bf4**esp**\u59cb\u7ec8\u6307\u5411\u6808\u9876\u3002\u5728\u660e\u767d\u4e86\u8fd9\u4e9b\u4e4b\u540e\uff0c\u4e0b\u9762\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u5177\u4f53\u7684\u4f8b\u5b50\uff1a ` \u5c31\u50cf\u524d\u9762\u8bf4\u7684\uff0c\u51fd\u6570\u5728\u6267\u884c\u7684\u65f6\u5019\uff0c\u662f\u5728\u6808\u4e0a\u5f00\u8f9f\u7a7a\u95f4\uff0c\u4e5f\u5c31\u662f\u7b2c\u4e00\u5e45\u56fe\u7684\u7eff\u8272\u90e8\u5206\uff0c\u800c\u4e14\u6211\u4eec\u77e5\u9053\uff0c\u7a0b\u5e8f\u662f\u9010\u53e5\u6267\u884c\u7684\uff0c\u53d8\u91cf\u662f\u9010\u4e2a\u88ab\u5b9a\u4e49\u548c\u5206\u914d\u5185\u5b58\u7a7a\u95f4\u7684\uff0c\u8fd9\u4e5f\u662f**\u6808\u6307\u9488**\u662f\u79fb\u52a8\u7684\u7684\u539f\u56e0\uff0c\u4e0b\u9762\u6211\u4eec\u9010\u53e5\u6267\u884c\u8fd9\u6bb5\u4ee3\u7801\uff08VS2015\u73af\u5883\u4e0b\uff09\uff0c\u6765\u770b**main\u51fd\u6570**\u548c**fun\u51fd\u6570**\u7684\u6808\u5e27\u7ed3\u6784\u7684\u53d8\u5316\uff08\u6ce8\uff1a\u4e3a\u65b9\u4fbf\u7406\u89e3\uff0c\u4e0b\u9762\u7684\u56fe\u90fd\u662f\u53ef\u4ee5\u6309\u987a\u5e8f\u76f4\u63a5\u62fc\u63a5\u7684\uff09\uff1a \u5728\u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u53d8\u91cf\u7684\u5b9a\u4e49\u5206\u914d\u5185\u5b58\u987a\u5e8f\u4fbf\u662f\u5148\u5b9a\u4e49\u5148\u5206\u914d\uff0c\u7ed3\u5408**\u6c47\u7f16\u8bed\u8a00**\u6211\u4eec\u53ef\u4ee5\u770b\u5f97\u66f4\u6e05\u695a\uff1a \u7136\u540e\uff0c\u7a0b\u5e8f\u7ee7\u7eed\u6267\u884c\u8fdb\u5165fun\u51fd\u6570\uff0c\u8fd9\u91cc\u6211\u4eec\u6765\u770b\u6c47\u7f16\u8bed\u8a00\uff1a \u8fd9\u91cc\u6211\u4eec\u4e3b\u8981\u8981\u660e\u767d\u4e24\u70b9\uff0c\uff081\uff09call\u6307\u4ee4\u7684\u4e24\u4e2a\u529f\u80fd\uff0c\uff082\uff09\u4e0a\u56fe\u4e2d\u7ea2\u8272\u7684\u57081\uff0c\u57082\u7684\u76ee\u7684\u3002call\u6307\u4ee4\u7684\u529f\u80fd\u4e0a\u56fe\u63cf\u8ff0\u7684\u5f88\u6e05\u695a\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\uff0c\u57081\u76ee\u7684\u662f\u5c06 main \u51fd\u6570\u7684 pc \u538b\u5165\u5806\u6808\uff0c\u56e0\u4e3a pc \u6307\u9488\u91cc\u9762\u653e\u7684\u59cb\u7ec8\u662f\u4e0b\u4e00\u6761\u8981\u6267\u884c\u6307\u4ee4\u7684\u5730\u5740\uff0c\u8fd9\u91cc\u628a pc \u538b\u5165\u5806\u6808\u8fdb\u884c\u4fdd\u62a4\uff0c\u662f\u56e0\u4e3a\u5728\u6267\u884cfun\u51fd\u6570\u7684\u65f6\u5019 pc \u6307\u9488\u91cc\u9762\u653e\u7684\u5c06\u4f1a\u662ffun\u51fd\u6570\u9700\u8981\u6267\u884c\u91cc\u9762\u7684\u5185\u5bb9\uff0c\u540c\u6837\u7684\u9053\u7406\uff0c\u6211\u4eec\u77e5\u9053**\u67d0\u4e00\u65f6\u523b\u53ea\u80fd\u6709\u4e00\u4e2a\u6808\u5e27\u7ed3\u6784**\uff0c\u5f53\u8981\u53bb\u5230fun\u51fd\u6570\u7684\u65f6\u5019\uff0c\u5bc4\u5b58\u5668 ebp \u5c31\u8981\u5b58\u50a8 fun \u51fd\u6570\u7684\u5e27\u6307\u9488\uff0c\u6240\u4ee5 main \u51fd\u6570\u7684**\u5e27\u6307\u9488**\u5c31\u8981\u4fdd\u5b58\u8d77\u6765\uff0c\u67e5\u770b main \u51fd\u6570\u7684\u6c47\u7f16\u8bed\u8a00\u4e5f\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230\u8fd9\u4e00\u70b9\uff1a \u7136\u540e\u8fdb\u5165\u5230fun\u51fd\u6570\uff1a \u603b\u7ed3\u4e00\u4e0b\u5c31\u662f\u8fd9\u6837\uff1a \u5728 fun \u51fd\u6570\u6267\u884c\u5b8c\u6210\u4e4b\u540e\uff0c\u4f1a\u8fd4\u56de\u5230 main \u51fd\u6570\uff0c\u8fd9\u91cc\u8981\u505a\u7684\u5c31\u662f**\u51fa\u6808**\uff0c\u4e5f\u5c31\u662f\u5c31\u5728\u5f62\u6210 fun \u51fd\u6570\u4e4b\u524d\u538b\u5165\u5806\u6808\u8fdb\u884c\u4fdd\u62a4\u7684 pc \u6307\u9488\uff0cmain\u51fd\u6570\u7684 ebp \u5f39\u51fa\u5806\u6808\uff0c\u91cd\u65b0\u5f62\u6210\u6216\u8005\u8bf4\u8fd8\u539f main \u51fd\u6570\u7684\u6808\u5e27\u7ed3\u6784\u3002 \u6808\u5e27---\u51fd\u6570\u8c03\u7528\u539f\u7406 \u4e00\u3001 \u4ec0\u4e48\u662f\u6808\u5e27\uff1f \u200b \u4ec0\u4e48\u662f\u6808\u5e27\uff0c\u9996\u5148\u5f15\u7528\u767e\u5ea6\u767e\u79d1\u7684\u7ecf\u5178\u89e3\u91ca\uff1a\u201c\u6808\u5e27\u4e5f\u53eb\u8fc7\u7a0b \u6d3b\u52a8\u8bb0\u5f55 \uff0c\u662f \u7f16\u8bd1\u5668 \u7528\u6765\u5b9e\u73b0\u8fc7\u7a0b/ \u51fd\u6570\u8c03\u7528 \u7684\u4e00\u79cd \u6570\u636e\u7ed3\u6784 \u3002\u201d\u3002 \u200b \u5b9e\u9645\u4e0a\uff0c\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff1a\u6808\u5e27\u5c31\u662f\u5b58\u50a8\u5728\u7528\u6237\u6808\u4e0a\u7684\uff08\u5f53\u7136\u5185\u6838\u6808\u540c\u6837\u9002\u7528\uff09\u6bcf\u4e00\u6b21\u51fd\u6570\u8c03\u7528\u6d89\u53ca\u7684\u76f8\u5173\u4fe1\u606f\u7684\u8bb0\u5f55\u5355\u5143\u3002\u4e5f\u8bb8\u8fd9\u6837\u611f\u89c9\u66f4\u590d\u6742\u4e86\uff0c\u597d\u5427\uff0c\u8ba9\u6211\u4eec\u4ece\u6808\u5f00\u59cb\u6765\u7406\u89e3\u4ec0\u4e48\u662f\u6808\u5e27... \u4e8c\u3001\u6808\u5e27 \u200b \u6808\u5e27\u8868\u793a\u7a0b\u5e8f\u7684\u51fd\u6570\u8c03\u7528\u8bb0\u5f55\uff0c\u800c\u6808\u5e27\u53c8\u662f\u8bb0\u5f55\u5728\u6808\u4e0a\u9762\uff0c\u5f88\u660e\u663e\u6808\u4e0a\u4fdd\u6301\u4e86N\u4e2a\u6808\u5e27\u7684\u5b9e\u4f53\uff0c\uff08\u5b9e\u9645\u4e0a\u6211\u4eec\u8fd9\u91cc\u8bf4\u7684\u6808\u5e27\u662f\u8f6f\u4ef6\u4e0a\u7684\u6982\u5ff5\uff0c\u636e\u8bf4\u6709\u786c\u4ef6\u6982\u5ff5\uff0c\u4e0d\u662f\u5f88\u4e86\u89e3\uff09\uff0c\u90a3\u5c31\u53ef\u4ee5\u8bf4\u6808\u5e27\u5c06\u6808\u5206\u5272\u6210\u4e86N\u4e2a\u8bb0\u5f55\u5757\uff0c\u4f46\u662f\u8fd9\u4e9b\u8bb0\u5f55\u5757\u5927\u5c0f\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u56e0\u4e3a\u6808\u5e27\u4e0d\u4ec5\u4fdd\u5b58\u8bf8\u5982\uff1a\u51fd\u6570\u5165\u53c2\u3001\u51fa\u53c2\u3001\u8fd4\u56de\u5730\u5740\u548c\u4e0a\u4e00\u4e2a\u6808\u5e27\u7684\u6808\u5e95\u6307\u9488\u7b49\u4fe1\u606f\uff0c\u8fd8\u4fdd\u5b58\u4e86\u51fd\u6570\u5185\u90e8\u7684\u81ea\u52a8\u53d8\u91cf\uff08\u751a\u81f3\u53ef\u4ee5\u662f\u52a8\u6001\u5206\u914d\u5185\u5b58\uff0calloca\u51fd\u6570\u5c31\u53ef\u4ee5\u5b9e\u73b0\uff0c\u4f46\u5728\u67d0\u4e9b\u7cfb\u7edf\u4e2d\u4e0d\u884c\uff09\uff0c\u56e0\u6b64\uff0c\u4e0d\u662f\u6240\u6709\u7684\u6808\u5e27\u7684\u5927\u5c0f\u90fd\u76f8\u540c\u3002 \u4e09\u3001\u4e0b\u9762\u6211\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u5b9e\u4f8b\u6765\u5206\u6790\u51fd\u6570\u8c03\u7528\u539f\u7406------\u6808\u5e27 \u9996\u5148\u5e94\u8be5\u660e\u767d\uff0c\u6808\u662f\u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u5ef6\u4f38\u7684\u3002\u6bcf\u4e2a\u51fd\u6570\u7684\u6bcf\u6b21\u8c03\u7528\uff0c\u90fd\u6709\u5b83\u81ea\u5df1\u72ec\u7acb\u7684\u4e00\u4e2a\u6808\u5e27\uff0c\u8fd9\u4e2a\u6808\u5e27\u4e2d\u7ef4\u6301\u7740\u6240\u9700\u8981\u7684\u5404\u79cd\u4fe1\u606f\u3002 \u5bc4\u5b58\u5668ebp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u5e95\u90e8\uff08\u9ad8\u5730\u5740\uff09 \u5bc4\u5b58\u5668esp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u9876\u90e8\uff08\u5730\u5740\u5730\uff09 PC\u6307\u9488\uff1a\u6c38\u8fdc\u6307\u5411\u5f53\u524d\u8fd0\u884c\u7a0b\u5e8f\u6307\u4ee4\u7684\u4e0b\u4e00\u6761\u6307\u4ee4 \u4e0b\u56fe\u4e3a\u5178\u578b\u7684\u5b58\u53d6\u5668\u5b89\u6392\uff0c\u89c2\u5bdf\u6808\u5728\u5176\u4e2d\u7684\u4f4d\u7f6e \u89c1\u4e0a\u56fe\uff0c\u2014\u2014\u2014\u2014\u9ed1\u8272\u7ebf\u6307\u5411\u7684\u662f\u8c03\u7528\u8005\u51fd\u6570main() \u200b \u2014\u2014\u2014\u2014\u84dd\u8272\u7ebf\u6307\u5411\u7684\u662f\u88ab\u8c03\u7528\u8005\u51fd\u6570fun() \u200b \u2014\u2014\u2014\u2014\u7ea2\u8272\u7ebf\u6307\u5411\u7684\u7684\u662f\u51fd\u6570fun()\u8fd4\u56de\u65f6\uff0c\u6062\u590d\u5230\u8c03\u7528\u51fd\u6570fun()\u4e4b\u524d\u7684\u72b6\u6001 \u8fd9\u5f20\u56fe\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c06\u8fdb\u7a0b\u7684\u5730\u5740\u7a7a\u95f4\uff0c\u8fdb\u7a0b\u8fd0\u884c\u65f6\u523b\u7684\u72b6\u51b5\u5c55\u73b0\u51fa\u6765\u4e86\uff0c\u662f\u4e00\u4e2a\u7efc\u5408\u6027\u5730\u5185\u5bb9\u3002 \u6839\u636e\u8fd9\u7b80\u5355\u7684\u4ee3\u7801\uff0c\u7136\u540e\u5728\u89c2\u5bdf\u4e0a\u9762\u7684\u6808\u5e27\u56fe\uff0c\u6211\u4eec\u6765\u770b\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u7684\u6808\u5e27\u539f\u7406 [html] view plain copy #include #include \u200b int fun(int x, int y) { \u200b int c = 0xcccccccc; \u200b return c; } int main() { \u200b int a = 0xaaaaaaaa; \u200b int b = 0xbbbbbbbb; \u200b int ret = fun(a, b); \u200b printf(\"You should runing here!\\n\"); \u200b system(\"pause\"); \u200b return 0; } \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u89c2\u5bdf\u4e0a\u9762\u8fd9\u4e2a\u4ee3\u7801\u548c\u6808\u5e27\u56fe\u8fd8\u6709\u4e0b\u9762\u53cd\u6c47\u7f16\u7684\u4ee3\u7801\u53d1\u73b0 \u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u5982\u4e0b\uff1a \uff081\uff09\u5148\u5c06\u8c03\u7528\u8005main()\u51fd\u6570\u7684\u5806\u6808\u7684\u57fa\u5740\uff08ebp\uff09\u5165\u6808\uff0c\u4ee5\u4fdd\u5b58\u4e4b\u524d\u4efb\u52a1\u7684\u4fe1\u606f\u3002 \uff082\uff09\u7136\u540e\u5c06\u8c03\u7528\u8005main()\u51fd\u6570\u7684\u6808\u9876\u6307\u9488\uff08esp\uff09\u7684\u503c\u8d4b\u7ed9ebp\uff0c\u4f5c\u4e3a\u65b0\u7684\u57fa\u5740\uff08\u5373\u88ab\u8c03\u7528\u8005\u51fd\u6570fun()\u7684\u6808\u5e95\uff09\u3002 \uff083\uff09\u7136\u540e\u5728\u8fd9\u4e2a\u57fa\u5740\uff08fun()\u51fd\u6570\u7684\u6808\u5e95\uff09\u4e0a\u5f00\u8f9f\uff08\u4e00\u822c\u7528sub\u547d\u4ee4\uff09\u76f8\u5e94\u7684\u7a7a\u95f4\u7528\u4f5c\u88ab\u8c03\u7528\u8005fun()\u51fd\u6570\u7684\u6808\u7a7a\u95f4\u3002 \uff084\uff09\u51fd\u6570fun()\u8fd4\u56de\u540e\uff0c\u4ece\u5f53\u524d\u6808\u5e27\u7684ebp\u5373\u6062\u590d\u4e3a\u8c03\u7528\u8005\u51fd\u6570main()\u7684\u6808\u9876\uff08esp\uff09\uff0c\u4f7f\u6808\u9876\u6062\u590d\u51fd\u6570fun()\u88ab\u8c03\u7528\u94b1\u7684\u4f4d\u7f6e\uff0c\u7136\u540e\u8c03\u7528\u8005main()\u518d\u4ece\u6062\u590d\u540e\u7684\u6808\u9876\u5f39\u51fa\u4e4b\u524d\u7684ebp\u503c(\u53ef\u4ee5\u8fd9\u4e48\u505a\u662f\u56e0\u4e3a\u8fd9\u4e2a\u503c\u5728\u51fd\u6570\u8c03\u7528\u524d\u524d\u4e00\u6b65\u88ab\u538b\u5165\u6808\u201dmain()\uff1aretaddr\u201d)\u3002\u8fd9\u6837\uff0cebp\u548cesp\u5c31\u90fd\u6062\u590d\u4e86\u8c03\u7528\u51fd\u6570fun\uff08\uff09\u4e4b\u524d\u7684\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u662f\u6808\u6062\u590d\u51fd\u6570fun()\u8c03\u7528\u524d\u7684\u72b6\u6001\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u53cd\u6c47\u7f16\u7684\u4ee3\u7801\uff0c\u6211\u4eec\u77e5\u9053\u4efb\u4f55\u51fd\u6570\u90fd\u53ea\u6709\u4e00\u4efdebp\u548cesp\uff0c\u53ef\u4ee5\u901a\u8fc7\u5185\u5b58\u7a97\u53e3\u53d1\u73b0ebp\u548cesp\u6c38\u8fdc\u4fdd\u5b58\u6700\u65b0\u5f53\u524d\u51fd\u6570\u7684\u6570\u503c\uff0c\u7528\u51fd\u6570\u65f6\uff0c\u539f\u51fd\u6570\u7684ebp\u548cesp\u8981\u4fdd\u5b58\u8d77\u6765\uff0c\u4ee5\u4fbf\u8fd4\u56de\u65f6\u6062\u590d\u3002\u8fd8\u53ef\u4ee5\u53d1\u73b0a\u548cb\uff0c\u662fb\u5148\u5b9e\u4f8b\u5316\uff0c\u518d\u662fa\u5b9e\u4f8b\u5316\uff0c\u5f62\u53c2\u5b9e\u4f8b\u5316\u662f\u4ece\u53f3\u5f80\u5de6\u3002 call\u547d\u4ee4\u7684\u4f5c\u7528 \uff1a1.\u4fdd\u5b58\u5f53\u524d\u6b63\u5728\u8fd0\u884c\u65f6\u6307\u4ee4\u7684\u4e0b\u4e00\u6761\u6307\u4ee4\u7684\u5730\u5740\u5230\u6808\u7ed3\u6784\u4e2d \u200b 2.\u8df3\u8f6c\u5230\u6307\u5b9a\u7684\u51fd\u6570\u5165\u53e3\uff08jmp\uff09","title":"\u6d45\u8c08\u6808\u5e27"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/stack%20frame/#_1","text":"","title":"\u6d45\u8c08\u6808\u5e27"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/stack%20frame/#_2","text":"\u4ec0\u4e48\u662f**\u6808\u5e27**\uff0c\u9996\u5148\u5f15\u7528\u767e\u5ea6\u767e\u79d1\u7684\u7ecf\u5178\u89e3\u91ca\uff1a \u6808\u5e27**\u4e5f\u53eb**\u8fc7\u7a0b\u6d3b\u52a8\u8bb0\u5f55 \uff0c\u662f\u7f16\u8bd1\u5668\u7528\u6765\u5b9e\u73b0\u8fc7\u7a0b/\u51fd\u6570\u8c03\u7528\u7684\u4e00\u79cd**\u6570\u636e\u7ed3\u6784**\u3002 \u5b9e\u9645\u4e0a\uff0c\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff1a \u6808\u5e27**\u5c31\u662f\u5b58\u50a8\u5728**\u7528\u6237\u6808**\u4e0a\u7684\uff08\u5f53\u7136**\u5185\u6838\u6808**\u540c\u6837\u9002\u7528\uff09\u6bcf\u4e00\u6b21**\u51fd\u6570\u8c03\u7528**\u6d89\u53ca\u7684\u76f8\u5173\u4fe1\u606f\u7684**\u8bb0\u5f55\u5355\u5143 \u3002\u4e5f\u8bb8\u8fd9\u6837\u611f\u89c9\u66f4\u590d\u6742\u4e86\uff0c\u597d\u5427\uff0c\u8ba9\u6211\u4eec\u4ece**\u6808**\u5f00\u59cb\u6765\u7406\u89e3\u4ec0\u4e48\u662f**\u6808\u5e27**... \u6808\u5e27**\u8868\u793a\u7a0b\u5e8f\u7684\u51fd\u6570\u8c03\u7528\u8bb0\u5f55\uff0c\u800c**\u6808\u5e27**\u53c8\u662f\u8bb0\u5f55\u5728**\u6808**\u4e0a\u9762\uff0c\u5f88\u660e\u663e\u6808\u4e0a\u4fdd\u6301\u4e86N\u4e2a\u6808\u5e27\u7684\u5b9e\u4f53\uff0c\u90a3\u5c31\u53ef\u4ee5\u8bf4**\u6808\u5e27**\u5c06\u6808\u5206\u5272\u6210\u4e86N\u4e2a\u8bb0\u5f55\u5757\uff0c\u4f46\u662f\u8fd9\u4e9b\u8bb0\u5f55\u5757\u5927\u5c0f\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u56e0\u4e3a**\u6808\u5e27**\u4e0d\u4ec5\u4fdd\u5b58\u8bf8\u5982\uff1a\u51fd\u6570\u5165\u53c2\u3001\u51fa\u53c2\u3001\u8fd4\u56de\u5730\u5740\u548c\u4e0a\u4e00\u4e2a\u6808\u5e27\u7684**\u6808\u5e95\u6307\u9488**\u7b49\u4fe1\u606f\uff0c\u8fd8\u4fdd\u5b58\u4e86\u51fd\u6570\u5185\u90e8\u7684**\u81ea\u52a8\u53d8\u91cf \uff08\u751a\u81f3\u53ef\u4ee5\u662f\u52a8\u6001\u5206\u914d\u5185\u5b58\uff0calloca\u51fd\u6570\u5c31\u53ef\u4ee5\u5b9e\u73b0\uff0c\u4f46\u5728\u67d0\u4e9b\u7cfb\u7edf\u4e2d\u4e0d\u884c\uff09\uff0c\u56e0\u6b64\uff0c\u4e0d\u662f\u6240\u6709\u7684\u6808\u5e27\u7684\u5927\u5c0f\u90fd\u76f8\u540c\u3002 \u662f\u4e0d\u662f\u6bcf\u4e00\u4e2a\u51fd\u6570\u8c03\u7528\u5c31\u5bf9\u5e94\u4e00\u4e2a\u6808\u5e27\uff1f","title":"\u4ec0\u4e48\u662f\u6808\u5e27\uff1f"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/stack%20frame/#_3","text":"\u6211\u4eec\u62ff\u4e00\u4e2a\u5b9e\u4f8b\u6765\u63a2\u8ba8\u4e00\u4e0b \u8fd9\u662f\u4e00\u4e2a\u5728linux\u4e0b\u8fd0\u884c\u7684\u7a0b\u5e8f\uff0c\u5f53\u7a0b\u5e8f\u8d70\u5230 *p=fun \u65f6\u4f1a\u8c03\u7528 fun \u51fd\u6570\uff0c\u6700\u540e\u6267\u884c reboot \u6307\u4ee4\u5173\u673a\u91cd\u542f\u3002\u8fd9\u662f\u4e3a\u4ec0\u4e48\u5462\uff1f\u6211\u4eec\u5c31\u62ff\u6808\u5e27\u6765\u770b\u4e00\u4e0b\u3002 \u9996\u5148\u4e86\u89e3\u6808\u5e27\u7684\u57fa\u672c\u7ed3\u6784\u3002 \u6808\u5e27**\u5176\u5b9e\u662f\u4e24\u4e2a**\u6307\u9488\u5bc4\u5b58\u5668 \uff0c \u5bc4\u5b58\u5668ebp**\u4e3a**\u5e27\u6307\u9488 \uff0c\u800c**\u5bc4\u5b58\u5668esp**\u4e3a**\u6808\u6307\u9488**\uff0c\u5f53\u7a0b\u5e8f\u8fd0\u884c\u65f6\uff0c**\u6808\u6307\u9488**\u53ef\u4ee5\u79fb\u52a8(\u5927\u591a\u6570\u7684\u4fe1\u606f\u7684\u8bbf\u95ee\u90fd\u662f\u901a\u8fc7**\u5e27\u6307\u9488**\u7684)\u3002\u603b\u4e4b\u7b80\u5355\u4e00\u53e5\u8bdd\uff0c\u6808\u5e27\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u7528\u6765\u63a7\u5236\u548c\u4fdd\u5b58\u4e00\u4e2a\u8fc7\u7a0b\u7684\u6240\u6709\u4fe1\u606f\u7684\u3002 \u518d\u6765\u770b\u4e00\u4e0b\u6211\u4eec\u5f53\u524d\u7a0b\u5e8f\u7684\u6808\u5e27 \u56e0\u4e3afun1\u4e2d*p=&a;\u5373p\u4e2d\u4fdd\u5b58\u7684a\u7684\u5730\u5740\uff0cp\u6307\u5411a, p--;\u4f1a\u5411\u4e0b\u8d70\uff0cp\u6307\u5411\u4e86\u8c03\u7528fun1\u51fd\u6570\u7684\u8fd4\u56de\u5730\u5740\uff0c\u7136\u540e\u518d\u5c06fun\u7684\u5730\u5740\u7ed9\u4e86P,\u8fd9\u65f6\u5019p\u5c31\u4e0d\u80fd\u6b63\u5e38\u8fd4\u56de\u4e86\uff0c\u800c\u662f\u5f53p\u8981\u8fd4\u56de\u65f6\u5374\u8c03\u7528\u4e86fun\u51fd\u6570\uff0c\u8fd9\u65f6\u5019\u5c31\u4f1a\u6267\u884creboot\u6307\u4ee4\u3002 \u4f60\u542c\u61c2\u4e86\u5417","title":"\u7406\u89e3"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/stack%20frame/#_4","text":"\u6211\u4eec\u77e5\u9053\u5185\u5b58\u7a7a\u95f4\u5927\u81f4\u53ef\u4ee5\u7528\u4e0b\u56fe\u8868\u793a\uff1a \u9996\u5148\u5e94\u8be5\u660e\u767d\uff0c\u6808\u662f\u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u5ef6\u4f38\u7684\u3002\u6bcf\u4e2a\u51fd\u6570\u7684\u6bcf\u6b21\u8c03\u7528\uff0c\u90fd\u6709\u5b83\u81ea\u5df1\u72ec\u7acb\u7684\u4e00\u4e2a\u6808\u5e27\uff0c\u8fd9\u4e2a\u6808\u5e27\u4e2d\u7ef4\u6301\u7740\u6240\u9700\u8981\u7684\u5404\u79cd\u4fe1\u606f\u3002 \u5bc4\u5b58\u5668ebp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u5e95\u90e8\uff08\u9ad8\u5730\u5740\uff09 \u5bc4\u5b58\u5668esp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u9876\u90e8\uff08\u5730\u5740\u5730\uff09 PC\u6307\u9488\uff1a\u6c38\u8fdc\u6307\u5411\u5f53\u524d\u8fd0\u884c\u7a0b\u5e8f\u6307\u4ee4\u7684\u4e0b\u4e00\u6761\u6307\u4ee4 \u800c\u51fd\u6570\u5728\u8c03\u7528\u7684\u65f6\u5019\u90fd\u662f\u5728**\u6808\u7a7a\u95f4**\u4e0a\u5f00\u8f9f\u4e00\u6bb5\u7a7a\u95f4\u4ee5\u4f9b\u51fd\u6570\u4f7f\u7528\uff0c\u6240\u4ee5\u4e0b\u9762\u6765\u8be6\u7ec6\u8c08\u4e00\u8c08\u51fd\u6570\u7684**\u6808\u5e27\u7ed3\u6784**\u3002 \u5982\u56fe\u793a\uff0c\u6808\u662f\u7531**\u9ad8\u5730\u5740**\u5411**\u4f4e\u5730\u5740**\u7684\u65b9\u5411\u751f\u957f\u7684\uff0c\u800c\u4e14\u6808\u6709\u5176**\u6808\u9876**\u548c**\u6808\u5e95**\uff0c\u5728 x86 \u7cfb\u7edf\u7684CPU\u4e2d\uff0c\u5bc4\u5b58\u5668 ebp \u4fdd\u5b58\u7684\u662f**\u6808\u5e95\u5730\u5740**\uff0c\u79f0\u4e3a**\u5e27\u6307\u9488**\uff0c\u5bc4\u5b58\u5668 esp \u4fdd\u5b58\u7684\u662f**\u6808\u9876\u5730\u5740**\uff0c\u79f0\u4e3a**\u6808\u6307\u9488**\u3002\u800c\u4e14\u8fd8\u5e94\u8be5\u660e\u786e\u4e00\u70b9\uff0c \u6808\u6307\u9488**\u548c**\u5e27\u6307\u9488**\u4e00\u6b21\u53ea\u80fd\u5b58\u50a8\u4e00\u4e2a\u5730\u5740\uff0c\u6240\u4ee5\uff0c\u4efb\u4f55\u65f6\u5019\uff0c\u8fd9\u4e00\u5bf9\u6307\u9488\u6307\u5411\u7684\u662f**\u540c\u4e00\u4e2a\u51fd\u6570**\u7684**\u6808\u5e27\u7ed3\u6784 \u3002\u5e76\u4e14 ebp \u4e00\u822c\u7531\u7cfb\u7edf\u6539\u53d8\u5b83\u7684\u503c\uff0c\u800c esp \u4f1a\u968f\u7740\u6570\u636e\u7684**\u5165\u6808**\u548c**\u51fa\u6808**\u800c\u79fb\u52a8\uff0c\u4e5f\u5c31\u662f\u8bf4**esp**\u59cb\u7ec8\u6307\u5411\u6808\u9876\u3002\u5728\u660e\u767d\u4e86\u8fd9\u4e9b\u4e4b\u540e\uff0c\u4e0b\u9762\u6211\u4eec\u6765\u770b\u4e00\u4e2a\u5177\u4f53\u7684\u4f8b\u5b50\uff1a ` \u5c31\u50cf\u524d\u9762\u8bf4\u7684\uff0c\u51fd\u6570\u5728\u6267\u884c\u7684\u65f6\u5019\uff0c\u662f\u5728\u6808\u4e0a\u5f00\u8f9f\u7a7a\u95f4\uff0c\u4e5f\u5c31\u662f\u7b2c\u4e00\u5e45\u56fe\u7684\u7eff\u8272\u90e8\u5206\uff0c\u800c\u4e14\u6211\u4eec\u77e5\u9053\uff0c\u7a0b\u5e8f\u662f\u9010\u53e5\u6267\u884c\u7684\uff0c\u53d8\u91cf\u662f\u9010\u4e2a\u88ab\u5b9a\u4e49\u548c\u5206\u914d\u5185\u5b58\u7a7a\u95f4\u7684\uff0c\u8fd9\u4e5f\u662f**\u6808\u6307\u9488**\u662f\u79fb\u52a8\u7684\u7684\u539f\u56e0\uff0c\u4e0b\u9762\u6211\u4eec\u9010\u53e5\u6267\u884c\u8fd9\u6bb5\u4ee3\u7801\uff08VS2015\u73af\u5883\u4e0b\uff09\uff0c\u6765\u770b**main\u51fd\u6570**\u548c**fun\u51fd\u6570**\u7684\u6808\u5e27\u7ed3\u6784\u7684\u53d8\u5316\uff08\u6ce8\uff1a\u4e3a\u65b9\u4fbf\u7406\u89e3\uff0c\u4e0b\u9762\u7684\u56fe\u90fd\u662f\u53ef\u4ee5\u6309\u987a\u5e8f\u76f4\u63a5\u62fc\u63a5\u7684\uff09\uff1a \u5728\u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u53d8\u91cf\u7684\u5b9a\u4e49\u5206\u914d\u5185\u5b58\u987a\u5e8f\u4fbf\u662f\u5148\u5b9a\u4e49\u5148\u5206\u914d\uff0c\u7ed3\u5408**\u6c47\u7f16\u8bed\u8a00**\u6211\u4eec\u53ef\u4ee5\u770b\u5f97\u66f4\u6e05\u695a\uff1a \u7136\u540e\uff0c\u7a0b\u5e8f\u7ee7\u7eed\u6267\u884c\u8fdb\u5165fun\u51fd\u6570\uff0c\u8fd9\u91cc\u6211\u4eec\u6765\u770b\u6c47\u7f16\u8bed\u8a00\uff1a \u8fd9\u91cc\u6211\u4eec\u4e3b\u8981\u8981\u660e\u767d\u4e24\u70b9\uff0c\uff081\uff09call\u6307\u4ee4\u7684\u4e24\u4e2a\u529f\u80fd\uff0c\uff082\uff09\u4e0a\u56fe\u4e2d\u7ea2\u8272\u7684\u57081\uff0c\u57082\u7684\u76ee\u7684\u3002call\u6307\u4ee4\u7684\u529f\u80fd\u4e0a\u56fe\u63cf\u8ff0\u7684\u5f88\u6e05\u695a\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\uff0c\u57081\u76ee\u7684\u662f\u5c06 main \u51fd\u6570\u7684 pc \u538b\u5165\u5806\u6808\uff0c\u56e0\u4e3a pc \u6307\u9488\u91cc\u9762\u653e\u7684\u59cb\u7ec8\u662f\u4e0b\u4e00\u6761\u8981\u6267\u884c\u6307\u4ee4\u7684\u5730\u5740\uff0c\u8fd9\u91cc\u628a pc \u538b\u5165\u5806\u6808\u8fdb\u884c\u4fdd\u62a4\uff0c\u662f\u56e0\u4e3a\u5728\u6267\u884cfun\u51fd\u6570\u7684\u65f6\u5019 pc \u6307\u9488\u91cc\u9762\u653e\u7684\u5c06\u4f1a\u662ffun\u51fd\u6570\u9700\u8981\u6267\u884c\u91cc\u9762\u7684\u5185\u5bb9\uff0c\u540c\u6837\u7684\u9053\u7406\uff0c\u6211\u4eec\u77e5\u9053**\u67d0\u4e00\u65f6\u523b\u53ea\u80fd\u6709\u4e00\u4e2a\u6808\u5e27\u7ed3\u6784**\uff0c\u5f53\u8981\u53bb\u5230fun\u51fd\u6570\u7684\u65f6\u5019\uff0c\u5bc4\u5b58\u5668 ebp \u5c31\u8981\u5b58\u50a8 fun \u51fd\u6570\u7684\u5e27\u6307\u9488\uff0c\u6240\u4ee5 main \u51fd\u6570\u7684**\u5e27\u6307\u9488**\u5c31\u8981\u4fdd\u5b58\u8d77\u6765\uff0c\u67e5\u770b main \u51fd\u6570\u7684\u6c47\u7f16\u8bed\u8a00\u4e5f\u53ef\u4ee5\u6e05\u695a\u7684\u770b\u5230\u8fd9\u4e00\u70b9\uff1a \u7136\u540e\u8fdb\u5165\u5230fun\u51fd\u6570\uff1a \u603b\u7ed3\u4e00\u4e0b\u5c31\u662f\u8fd9\u6837\uff1a \u5728 fun \u51fd\u6570\u6267\u884c\u5b8c\u6210\u4e4b\u540e\uff0c\u4f1a\u8fd4\u56de\u5230 main \u51fd\u6570\uff0c\u8fd9\u91cc\u8981\u505a\u7684\u5c31\u662f**\u51fa\u6808**\uff0c\u4e5f\u5c31\u662f\u5c31\u5728\u5f62\u6210 fun \u51fd\u6570\u4e4b\u524d\u538b\u5165\u5806\u6808\u8fdb\u884c\u4fdd\u62a4\u7684 pc \u6307\u9488\uff0cmain\u51fd\u6570\u7684 ebp \u5f39\u51fa\u5806\u6808\uff0c\u91cd\u65b0\u5f62\u6210\u6216\u8005\u8bf4\u8fd8\u539f main \u51fd\u6570\u7684\u6808\u5e27\u7ed3\u6784\u3002","title":"\u51fd\u6570\u6808\u5e27\u56fe\u89e3"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/stack%20frame/#---","text":"\u4e00\u3001 \u4ec0\u4e48\u662f\u6808\u5e27\uff1f \u200b \u4ec0\u4e48\u662f\u6808\u5e27\uff0c\u9996\u5148\u5f15\u7528\u767e\u5ea6\u767e\u79d1\u7684\u7ecf\u5178\u89e3\u91ca\uff1a\u201c\u6808\u5e27\u4e5f\u53eb\u8fc7\u7a0b \u6d3b\u52a8\u8bb0\u5f55 \uff0c\u662f \u7f16\u8bd1\u5668 \u7528\u6765\u5b9e\u73b0\u8fc7\u7a0b/ \u51fd\u6570\u8c03\u7528 \u7684\u4e00\u79cd \u6570\u636e\u7ed3\u6784 \u3002\u201d\u3002 \u200b \u5b9e\u9645\u4e0a\uff0c\u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\uff1a\u6808\u5e27\u5c31\u662f\u5b58\u50a8\u5728\u7528\u6237\u6808\u4e0a\u7684\uff08\u5f53\u7136\u5185\u6838\u6808\u540c\u6837\u9002\u7528\uff09\u6bcf\u4e00\u6b21\u51fd\u6570\u8c03\u7528\u6d89\u53ca\u7684\u76f8\u5173\u4fe1\u606f\u7684\u8bb0\u5f55\u5355\u5143\u3002\u4e5f\u8bb8\u8fd9\u6837\u611f\u89c9\u66f4\u590d\u6742\u4e86\uff0c\u597d\u5427\uff0c\u8ba9\u6211\u4eec\u4ece\u6808\u5f00\u59cb\u6765\u7406\u89e3\u4ec0\u4e48\u662f\u6808\u5e27... \u4e8c\u3001\u6808\u5e27 \u200b \u6808\u5e27\u8868\u793a\u7a0b\u5e8f\u7684\u51fd\u6570\u8c03\u7528\u8bb0\u5f55\uff0c\u800c\u6808\u5e27\u53c8\u662f\u8bb0\u5f55\u5728\u6808\u4e0a\u9762\uff0c\u5f88\u660e\u663e\u6808\u4e0a\u4fdd\u6301\u4e86N\u4e2a\u6808\u5e27\u7684\u5b9e\u4f53\uff0c\uff08\u5b9e\u9645\u4e0a\u6211\u4eec\u8fd9\u91cc\u8bf4\u7684\u6808\u5e27\u662f\u8f6f\u4ef6\u4e0a\u7684\u6982\u5ff5\uff0c\u636e\u8bf4\u6709\u786c\u4ef6\u6982\u5ff5\uff0c\u4e0d\u662f\u5f88\u4e86\u89e3\uff09\uff0c\u90a3\u5c31\u53ef\u4ee5\u8bf4\u6808\u5e27\u5c06\u6808\u5206\u5272\u6210\u4e86N\u4e2a\u8bb0\u5f55\u5757\uff0c\u4f46\u662f\u8fd9\u4e9b\u8bb0\u5f55\u5757\u5927\u5c0f\u4e0d\u662f\u56fa\u5b9a\u7684\uff0c\u56e0\u4e3a\u6808\u5e27\u4e0d\u4ec5\u4fdd\u5b58\u8bf8\u5982\uff1a\u51fd\u6570\u5165\u53c2\u3001\u51fa\u53c2\u3001\u8fd4\u56de\u5730\u5740\u548c\u4e0a\u4e00\u4e2a\u6808\u5e27\u7684\u6808\u5e95\u6307\u9488\u7b49\u4fe1\u606f\uff0c\u8fd8\u4fdd\u5b58\u4e86\u51fd\u6570\u5185\u90e8\u7684\u81ea\u52a8\u53d8\u91cf\uff08\u751a\u81f3\u53ef\u4ee5\u662f\u52a8\u6001\u5206\u914d\u5185\u5b58\uff0calloca\u51fd\u6570\u5c31\u53ef\u4ee5\u5b9e\u73b0\uff0c\u4f46\u5728\u67d0\u4e9b\u7cfb\u7edf\u4e2d\u4e0d\u884c\uff09\uff0c\u56e0\u6b64\uff0c\u4e0d\u662f\u6240\u6709\u7684\u6808\u5e27\u7684\u5927\u5c0f\u90fd\u76f8\u540c\u3002 \u4e09\u3001\u4e0b\u9762\u6211\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u5b9e\u4f8b\u6765\u5206\u6790\u51fd\u6570\u8c03\u7528\u539f\u7406------\u6808\u5e27 \u9996\u5148\u5e94\u8be5\u660e\u767d\uff0c\u6808\u662f\u4ece\u9ad8\u5730\u5740\u5411\u4f4e\u5730\u5740\u5ef6\u4f38\u7684\u3002\u6bcf\u4e2a\u51fd\u6570\u7684\u6bcf\u6b21\u8c03\u7528\uff0c\u90fd\u6709\u5b83\u81ea\u5df1\u72ec\u7acb\u7684\u4e00\u4e2a\u6808\u5e27\uff0c\u8fd9\u4e2a\u6808\u5e27\u4e2d\u7ef4\u6301\u7740\u6240\u9700\u8981\u7684\u5404\u79cd\u4fe1\u606f\u3002 \u5bc4\u5b58\u5668ebp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u5e95\u90e8\uff08\u9ad8\u5730\u5740\uff09 \u5bc4\u5b58\u5668esp\u6307\u5411\u5f53\u524d\u7684\u6808\u5e27\u7684\u9876\u90e8\uff08\u5730\u5740\u5730\uff09 PC\u6307\u9488\uff1a\u6c38\u8fdc\u6307\u5411\u5f53\u524d\u8fd0\u884c\u7a0b\u5e8f\u6307\u4ee4\u7684\u4e0b\u4e00\u6761\u6307\u4ee4 \u4e0b\u56fe\u4e3a\u5178\u578b\u7684\u5b58\u53d6\u5668\u5b89\u6392\uff0c\u89c2\u5bdf\u6808\u5728\u5176\u4e2d\u7684\u4f4d\u7f6e \u89c1\u4e0a\u56fe\uff0c\u2014\u2014\u2014\u2014\u9ed1\u8272\u7ebf\u6307\u5411\u7684\u662f\u8c03\u7528\u8005\u51fd\u6570main() \u200b \u2014\u2014\u2014\u2014\u84dd\u8272\u7ebf\u6307\u5411\u7684\u662f\u88ab\u8c03\u7528\u8005\u51fd\u6570fun() \u200b \u2014\u2014\u2014\u2014\u7ea2\u8272\u7ebf\u6307\u5411\u7684\u7684\u662f\u51fd\u6570fun()\u8fd4\u56de\u65f6\uff0c\u6062\u590d\u5230\u8c03\u7528\u51fd\u6570fun()\u4e4b\u524d\u7684\u72b6\u6001 \u8fd9\u5f20\u56fe\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u5c06\u8fdb\u7a0b\u7684\u5730\u5740\u7a7a\u95f4\uff0c\u8fdb\u7a0b\u8fd0\u884c\u65f6\u523b\u7684\u72b6\u51b5\u5c55\u73b0\u51fa\u6765\u4e86\uff0c\u662f\u4e00\u4e2a\u7efc\u5408\u6027\u5730\u5185\u5bb9\u3002 \u6839\u636e\u8fd9\u7b80\u5355\u7684\u4ee3\u7801\uff0c\u7136\u540e\u5728\u89c2\u5bdf\u4e0a\u9762\u7684\u6808\u5e27\u56fe\uff0c\u6211\u4eec\u6765\u770b\u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u7684\u6808\u5e27\u539f\u7406 [html] view plain copy #include #include \u200b int fun(int x, int y) { \u200b int c = 0xcccccccc; \u200b return c; } int main() { \u200b int a = 0xaaaaaaaa; \u200b int b = 0xbbbbbbbb; \u200b int ret = fun(a, b); \u200b printf(\"You should runing here!\\n\"); \u200b system(\"pause\"); \u200b return 0; } \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u89c2\u5bdf\u4e0a\u9762\u8fd9\u4e2a\u4ee3\u7801\u548c\u6808\u5e27\u56fe\u8fd8\u6709\u4e0b\u9762\u53cd\u6c47\u7f16\u7684\u4ee3\u7801\u53d1\u73b0 \u51fd\u6570\u8c03\u7528\u8fc7\u7a0b\u5982\u4e0b\uff1a \uff081\uff09\u5148\u5c06\u8c03\u7528\u8005main()\u51fd\u6570\u7684\u5806\u6808\u7684\u57fa\u5740\uff08ebp\uff09\u5165\u6808\uff0c\u4ee5\u4fdd\u5b58\u4e4b\u524d\u4efb\u52a1\u7684\u4fe1\u606f\u3002 \uff082\uff09\u7136\u540e\u5c06\u8c03\u7528\u8005main()\u51fd\u6570\u7684\u6808\u9876\u6307\u9488\uff08esp\uff09\u7684\u503c\u8d4b\u7ed9ebp\uff0c\u4f5c\u4e3a\u65b0\u7684\u57fa\u5740\uff08\u5373\u88ab\u8c03\u7528\u8005\u51fd\u6570fun()\u7684\u6808\u5e95\uff09\u3002 \uff083\uff09\u7136\u540e\u5728\u8fd9\u4e2a\u57fa\u5740\uff08fun()\u51fd\u6570\u7684\u6808\u5e95\uff09\u4e0a\u5f00\u8f9f\uff08\u4e00\u822c\u7528sub\u547d\u4ee4\uff09\u76f8\u5e94\u7684\u7a7a\u95f4\u7528\u4f5c\u88ab\u8c03\u7528\u8005fun()\u51fd\u6570\u7684\u6808\u7a7a\u95f4\u3002 \uff084\uff09\u51fd\u6570fun()\u8fd4\u56de\u540e\uff0c\u4ece\u5f53\u524d\u6808\u5e27\u7684ebp\u5373\u6062\u590d\u4e3a\u8c03\u7528\u8005\u51fd\u6570main()\u7684\u6808\u9876\uff08esp\uff09\uff0c\u4f7f\u6808\u9876\u6062\u590d\u51fd\u6570fun()\u88ab\u8c03\u7528\u94b1\u7684\u4f4d\u7f6e\uff0c\u7136\u540e\u8c03\u7528\u8005main()\u518d\u4ece\u6062\u590d\u540e\u7684\u6808\u9876\u5f39\u51fa\u4e4b\u524d\u7684ebp\u503c(\u53ef\u4ee5\u8fd9\u4e48\u505a\u662f\u56e0\u4e3a\u8fd9\u4e2a\u503c\u5728\u51fd\u6570\u8c03\u7528\u524d\u524d\u4e00\u6b65\u88ab\u538b\u5165\u6808\u201dmain()\uff1aretaddr\u201d)\u3002\u8fd9\u6837\uff0cebp\u548cesp\u5c31\u90fd\u6062\u590d\u4e86\u8c03\u7528\u51fd\u6570fun\uff08\uff09\u4e4b\u524d\u7684\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u662f\u6808\u6062\u590d\u51fd\u6570fun()\u8c03\u7528\u524d\u7684\u72b6\u6001\u3002 \u4e0b\u9762\u662f\u4e00\u4e2a\u53cd\u6c47\u7f16\u7684\u4ee3\u7801\uff0c\u6211\u4eec\u77e5\u9053\u4efb\u4f55\u51fd\u6570\u90fd\u53ea\u6709\u4e00\u4efdebp\u548cesp\uff0c\u53ef\u4ee5\u901a\u8fc7\u5185\u5b58\u7a97\u53e3\u53d1\u73b0ebp\u548cesp\u6c38\u8fdc\u4fdd\u5b58\u6700\u65b0\u5f53\u524d\u51fd\u6570\u7684\u6570\u503c\uff0c\u7528\u51fd\u6570\u65f6\uff0c\u539f\u51fd\u6570\u7684ebp\u548cesp\u8981\u4fdd\u5b58\u8d77\u6765\uff0c\u4ee5\u4fbf\u8fd4\u56de\u65f6\u6062\u590d\u3002\u8fd8\u53ef\u4ee5\u53d1\u73b0a\u548cb\uff0c\u662fb\u5148\u5b9e\u4f8b\u5316\uff0c\u518d\u662fa\u5b9e\u4f8b\u5316\uff0c\u5f62\u53c2\u5b9e\u4f8b\u5316\u662f\u4ece\u53f3\u5f80\u5de6\u3002 call\u547d\u4ee4\u7684\u4f5c\u7528 \uff1a1.\u4fdd\u5b58\u5f53\u524d\u6b63\u5728\u8fd0\u884c\u65f6\u6307\u4ee4\u7684\u4e0b\u4e00\u6761\u6307\u4ee4\u7684\u5730\u5740\u5230\u6808\u7ed3\u6784\u4e2d \u200b 2.\u8df3\u8f6c\u5230\u6307\u5b9a\u7684\u51fd\u6570\u5165\u53e3\uff08jmp\uff09","title":"\u6808\u5e27---\u51fd\u6570\u8c03\u7528\u539f\u7406"},{"location":"CPU/Manufacturer/Intel/draft/assembly-language/the-relation-between-pc-and-stack-pointer/","text":"","title":"The relation between pc and stack pointer"},{"location":"CPU/Manufacturer/Intel/x86/x86-memory-segmentation/","text":"x86 memory segmentation x86 memory segmentation\u5df2\u7ecf\u88ab\u6dd8\u6c70\u4e86\uff0c\u4f46\u662f\u5728\u5f88\u591a\u7684\u6587\u7ae0\uff0c\u6216\u8005\u4e66\u7c4d\u4e2d\u8fd8\u662f\u4f1a\u89c1\u5230\u5b83\u7684\u8eab\u5f71\uff0c\u6240\u4ee5\u603b\u7ed3\u4e00\u4e0b\u3002 \u7ef4\u57fa\u767e\u79d1 x86 memory segmentation x86 memory segmentation refers to the implementation of memory segmentation in the Intel x86 computer instruction set architecture . Segmentation was introduced on the Intel 8086 in 1978 as a way to allow programs to address more than 64 KB (65,536 bytes ) of memory. The Intel 80286 introduced a second version of segmentation in 1982 that added support for virtual memory and memory protection . At this point the original model was renamed real mode , and the new version was named protected mode . The x86-64 architecture, introduced in 2003, has largely dropped support for segmentation in 64-bit mode. In both real and protected modes, the system uses 16-bit segment registers to derive the actual memory address. In real mode, the registers CS, DS, SS, and ES point to the currently used program code segment (CS), the current data segment (DS), the current stack segment (SS), and one extra segment determined by the programmer (ES). The Intel 80386 , introduced in 1985, adds two additional segment registers, FS and GS, with no specific uses defined by the hardware. The way in which the segment registers are used differs between the two modes. The choice of segment is normally defaulted by the processor according to the function being executed. Instructions are always fetched from the code segment . Any stack push or pop or any data reference referring to the stack uses the stack segment . All other references to data use the data segment . The extra segment is the default destination for string operations (for example MOVS or CMPS). FS and GS have no hardware-assigned uses. The instruction format allows an optional segment prefix byte which can be used to override the default segment for selected instructions if desired. Later developments The x86-64 architecture does not use segmentation in long mode (64-bit mode). Four of the segment registers, CS, SS, DS, and ES, are forced to 0, and the limit to 264. cs and eip \u5982\u679c\u91c7\u7528memory segmentation\u7684\u8bdd\uff0c\u5219\u9700\u8981\u4f7f\u7528 cs \u548c eip \u6765\u6307\u5b9anext instruction\u3002 \u53c2\u89c1 4.2.4. Hardware Handling of Interrupts and Exceptions ss and esp \u5982\u679c\u91c7\u7528memory segmentation\u7684\u8bdd\uff0c\u5219\u9700\u8981\u4f7f\u7528 ss \u548c esp \u6765\u6307\u5b9a call stack of currently executed program\u3002 \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Stack registers in x86","title":"x86-memory-segmentation"},{"location":"CPU/Manufacturer/Intel/x86/x86-memory-segmentation/#x86#memory#segmentation","text":"x86 memory segmentation\u5df2\u7ecf\u88ab\u6dd8\u6c70\u4e86\uff0c\u4f46\u662f\u5728\u5f88\u591a\u7684\u6587\u7ae0\uff0c\u6216\u8005\u4e66\u7c4d\u4e2d\u8fd8\u662f\u4f1a\u89c1\u5230\u5b83\u7684\u8eab\u5f71\uff0c\u6240\u4ee5\u603b\u7ed3\u4e00\u4e0b\u3002","title":"x86 memory segmentation"},{"location":"CPU/Manufacturer/Intel/x86/x86-memory-segmentation/#x86#memory#segmentation_1","text":"x86 memory segmentation refers to the implementation of memory segmentation in the Intel x86 computer instruction set architecture . Segmentation was introduced on the Intel 8086 in 1978 as a way to allow programs to address more than 64 KB (65,536 bytes ) of memory. The Intel 80286 introduced a second version of segmentation in 1982 that added support for virtual memory and memory protection . At this point the original model was renamed real mode , and the new version was named protected mode . The x86-64 architecture, introduced in 2003, has largely dropped support for segmentation in 64-bit mode. In both real and protected modes, the system uses 16-bit segment registers to derive the actual memory address. In real mode, the registers CS, DS, SS, and ES point to the currently used program code segment (CS), the current data segment (DS), the current stack segment (SS), and one extra segment determined by the programmer (ES). The Intel 80386 , introduced in 1985, adds two additional segment registers, FS and GS, with no specific uses defined by the hardware. The way in which the segment registers are used differs between the two modes. The choice of segment is normally defaulted by the processor according to the function being executed. Instructions are always fetched from the code segment . Any stack push or pop or any data reference referring to the stack uses the stack segment . All other references to data use the data segment . The extra segment is the default destination for string operations (for example MOVS or CMPS). FS and GS have no hardware-assigned uses. The instruction format allows an optional segment prefix byte which can be used to override the default segment for selected instructions if desired.","title":"\u7ef4\u57fa\u767e\u79d1x86 memory segmentation"},{"location":"CPU/Manufacturer/Intel/x86/x86-memory-segmentation/#later#developments","text":"The x86-64 architecture does not use segmentation in long mode (64-bit mode). Four of the segment registers, CS, SS, DS, and ES, are forced to 0, and the limit to 264.","title":"Later developments"},{"location":"CPU/Manufacturer/Intel/x86/x86-memory-segmentation/#cs#and#eip","text":"\u5982\u679c\u91c7\u7528memory segmentation\u7684\u8bdd\uff0c\u5219\u9700\u8981\u4f7f\u7528 cs \u548c eip \u6765\u6307\u5b9anext instruction\u3002 \u53c2\u89c1 4.2.4. Hardware Handling of Interrupts and Exceptions","title":"cs and  eip"},{"location":"CPU/Manufacturer/Intel/x86/x86-memory-segmentation/#ss#and#esp","text":"\u5982\u679c\u91c7\u7528memory segmentation\u7684\u8bdd\uff0c\u5219\u9700\u8981\u4f7f\u7528 ss \u548c esp \u6765\u6307\u5b9a call stack of currently executed program\u3002 \u53c2\u89c1\uff1a \u7ef4\u57fa\u767e\u79d1 Stack registers in x86","title":"ss and esp"},{"location":"CPU/Manufacturer/Intel/x86/x86/","text":"x86 x86 is a family of instruction set architectures [ a] based on the Intel 8086 microprocessor and its 8088 variant. The 8086 was introduced in 1978 as a fully 16-bit extension of Intel's 8-bit 8080 microprocessor, with memory segmentation as a solution for addressing more memory than can be covered by a plain 16-bit address. The term \"x86\" came into being because the names of several successors to Intel's 8086 processor end in \"86\", including the 80186 , 80286 , 80386 and 80486 processors. NOTE: \u76ee\u524d\u6211\u4eec\u6240\u4f7f\u7528\u7684 CORE i5 \u90fd\u662f\u91c7\u7528\u7684\u8fd9\u79cdarchitecture\u3002 Many additions and extensions have been added to the x86 instruction set over the years, almost consistently with full backward compatibility .[ b] The architecture has been implemented in processors from Intel , Cyrix , AMD , VIA and many other companies; there are also open implementations, such as the Zet SoC platform.[ 2] Nevertheless, of those, only Intel, AMD, and VIA hold x86 architectural licenses , and are producing modern 64-bit designs.[ 3] [ irrelevant citation ] The term is not synonymous with IBM PC compatibility , as this implies a multitude of other computer hardware ; embedded systems , as well as general-purpose computers, used x86 chips before the PC-compatible market started ,[ c] some of them before the IBM PC (1981) itself. As of 2018, the majority of personal computers and laptops \uff08\u7b14\u8bb0\u672c\u7535\u8111\uff09sold are based on the x86 architecture, while other categories\u2014especially high-volume[ clarification needed ] mobile categories such as smartphones or tablets \uff08\u5e73\u677f\u7535\u8111\uff09\u2014are dominated by ARM ; at the high end, x86 continues to dominate compute-intensive\uff08\u8ba1\u7b97\u5bc6\u96c6\u7684\uff09 workstation and cloud computing segments.[ 4] Overview Although the 8086 was primarily developed for embedded systems and small multi-user or single-user computers, largely as a response to the successful 8080-compatible Zilog Z80 ,[ 8] the x86 line soon grew in features and processing power. Today, x86 is ubiquitous\uff08\u65e0\u5904\u4e0d\u5728\uff09 in both stationary and portable personal computers, and is also used in midrange computers , workstations , servers and most new supercomputer clusters of the TOP500 list. A large amount of software , including a large list of x86 operating systems are using x86-based hardware. Modern x86 is relatively uncommon in embedded systems , however, and small low power applications (using tiny batteries) as well as low-cost microprocessor markets, such as home appliances and toys, lack any significant x86 presence.[ h] Simple 8-bit and 16-bit based architectures are common here, although the x86-compatible VIA C7 , VIA Nano , AMD 's Geode , Athlon Neo and Intel Atom are examples of 32- and 64-bit designs used in some relatively low power and low cost segments. Chronology\uff08\u5e74\u4ee3\u8868\uff09 History Other manufacturers Extensions of word size The instruction set architecture has twice been extended to a larger word size . In 1985, Intel released the 32-bit 80386 (later known as i386) which gradually replaced the earlier 16-bit chips in computers (although typically not in embedded systems ) during the following years; this extended programming model was originally referred to as the i386 architecture (like its first implementation) but Intel later dubbed it IA-32 when introducing its (unrelated) IA-64 architecture. In 1999-2003, AMD extended this 32-bit architecture to 64 bits and referred to it as x86-64 in early documents and later as AMD64 . Intel soon adopted AMD's architectural extensions under the name IA-32e, later using the name EM64T and finally using Intel 64. Microsoft and Sun Microsystems / Oracle also use term \"x64\", while many Linux distributions , and the BSDs also use the \"amd64\" term. Microsoft Windows, for example, designates its 32-bit versions as \"x86\" and 64-bit versions as \"x64\", while installation files of 64-bit Windows versions are required to be placed into a directory called \"AMD64\".[ 12] Overview Basic properties of the architecture The x86 architecture is a variable instruction length, primarily \" CISC \" design with emphasis on backward compatibility . The instruction set is not typical CISC, however, but basically an extended version of the simple eight-bit 8008 and 8080 architectures. Byte-addressing is enabled and words are stored in memory with little-endian byte order. Memory access to unaligned addresses is allowed for all valid word sizes. The largest native size for integer arithmetic and memory addresses (or offsets ) is 16, 32 or 64 bits depending on architecture generation (newer processors include direct support for smaller integers as well). Multiple scalar values can be handled simultaneously via the SIMD unit present in later generations, as described below.[ l] Immediate addressing offsets and immediate data may be expressed as 8-bit quantities for the frequently occurring cases or contexts where a -128..127 range is enough. Typical instructions are therefore 2 or 3 bytes in length (although some are much longer, and some are single-byte). To further conserve encoding space, most registers are expressed in opcodes using three or four bits, the latter via an opcode prefix in 64-bit mode, while at most one operand to an instruction can be a memory location.[ m] However, this memory operand may also be the destination (or a combined source and destination), while the other operand, the source , can be either register or immediate . Among other factors, this contributes to a code size that rivals eight-bit machines and enables efficient use of instruction cache memory. The relatively small number of general registers (also inherited from its 8-bit ancestors) has made register-relative addressing (using small immediate offsets) an important method of accessing operands, especially on the stack. Much work has therefore been invested in making such accesses as fast as register accesses, i.e. a one cycle instruction throughput, in most circumstances where the accessed data is available in the top-level cache. Floating point and SIMD A dedicated floating point processor with 80-bit internal registers, the 8087 , was developed for the original 8086 . This microprocessor subsequently developed into the extended 80387 , and later processors incorporated a backward compatible version of this functionality on the same microprocessor as the main processor. In addition to this, modern x86 designs also contain a SIMD -unit (see SSE below) where instructions can work in parallel on (one or two) 128-bit words, each containing two or four floating point numbers (each 64 or 32 bits wide respectively), or alternatively, 2, 4, 8 or 16 integers (each 64, 32, 16 or 8 bits wide respectively). The presence of wide SIMD registers means that existing x86 processors can load or store up to 128 bits of memory data in a single instruction and also perform bitwise operations (although not integer arithmetic[ n] ) on full 128-bits quantities in parallel. Intel's Sandy Bridge processors added the AVX (Advanced Vector Extensions) instructions, widening the SIMD registers to 256 bits. Knights Corner, the architecture used by Intel on their Xeon Phi co-processors, uses 512-bit wide SIMD registers. Current implementations During execution , current x86 processors employ a few extra decoding steps to split most instructions into smaller pieces called micro-operations. These are then handed to a control unit that buffers and schedules them in compliance with x86-semantics so that they can be executed, partly in parallel, by one of several (more or less specialized) execution units . These modern x86 designs are thus pipelined , superscalar , and also capable of out of order and speculative execution (via branch prediction , register renaming , and memory dependence prediction ), which means they may execute multiple (partial or complete) x86 instructions simultaneously, and not necessarily in the same order as given in the instruction stream.[ 13] Intel's and AMD's (starting from AMD Zen ) CPUs are also capable of simultaneous multithreading with two threads per core ( Xeon Phi has four threads per core) and in case of Intel transactional memory ( TSX ). When introduced, in the mid-1990s, this method was sometimes referred to as a \"RISC core\" or as \"RISC translation\", partly for marketing reasons, but also because these micro-operations share some properties with certain types of RISC instructions. However, traditional microcode (used since the 1950s) also inherently shares many of the same properties; the new method differs mainly in that the translation to micro-operations now occurs asynchronously. Not having to synchronize the execution units with the decode steps opens up possibilities for more analysis of the (buffered) code stream, and therefore permits detection of operations that can be performed in parallel, simultaneously feeding more than one execution unit. The latest processors also do the opposite when appropriate; they combine certain x86 sequences (such as a compare followed by a conditional jump) into a more complex micro-op which fits the execution model better and thus can be executed faster or with less machine resources involved. Another way to try to improve performance is to cache the decoded micro-operations, so the processor can directly access the decoded micro-operations from a special cache, instead of decoding them again. Intel followed this approach with the Execution Trace Cache feature in their NetBurst Microarchitecture (for Pentium 4 processors) and later in the Decoded Stream Buffer (for Core-branded processors since Sandy Bridge).[ 14] Transmeta used a completely different method in their x86 compatible CPUs. They used just-in-time translation to convert x86 instructions to the CPU's native VLIW instruction set. Transmeta argued that their approach allows for more power efficient designs since the CPU can forgo the complicated decode step of more traditional x86 implementations. Segmentation Further information: x86 memory segmentation Minicomputers during the late 1970s were running up against the 16-bit 64- KB address limit, as memory had become cheaper. Some minicomputers like the PDP-11 used complex bank-switching schemes, or, in the case of Digital's VAX , redesigned much more expensive processors which could directly handle 32-bit addressing and data. The original 8086, developed from the simple 8080 microprocessor and primarily aiming at very small and inexpensive computers and other specialized devices, instead adopted simple segment registers which increased the memory address width by only 4 bits. By multiplying a 64-KB address by 16, the 20-bit address could address a total of one megabyte (1,048,576 bytes) which was quite a large amount for a small computer at the time. The concept of segment registers was not new to many mainframes which used segment registers to swap quickly to different tasks. In practice, on the x86 it was (is) a much-criticized implementation which greatly complicated many common programming tasks and compilers. However, the architecture soon allowed linear 32-bit addressing (starting with the 80386 in late 1985) but major actors (such as Microsoft ) took several years to convert their 16-bit based systems. The 80386 (and 80486) was therefore largely used as a fast (but still 16-bit based) 8086 for many years. NOTE : 20\u4e16\u7eaa70\u5e74\u4ee3\u540e\u671f\u7684\u5c0f\u578b\u8ba1\u7b97\u673a\u572816\u4f4d64 KB\u5730\u5740\u9650\u5236\u4e0b\u8fd0\u884c\uff0c\u56e0\u4e3a\u5185\u5b58\u53d8\u5f97\u66f4\u4fbf\u5b9c\u3002\u50cfPDP-11\u8fd9\u6837\u7684\u5c0f\u578b\u8ba1\u7b97\u673a\u4f7f\u7528\u590d\u6742\u7684\u5b58\u50a8\u4f53\u5207\u6362\u65b9\u6848\uff0c\u6216\u8005\u5728Digital\u7684VAX\u60c5\u51b5\u4e0b\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u66f4\u6602\u8d35\u7684\u5904\u7406\u5668\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u740632\u4f4d\u5bfb\u5740\u548c\u6570\u636e\u3002\u6700\u521d\u76848086\u662f\u4ece\u7b80\u5355\u76848080\u5fae\u5904\u7406\u5668\u5f00\u53d1\u51fa\u6765\u7684\uff0c\u4e3b\u8981\u9488\u5bf9\u975e\u5e38\u5c0f\u800c\u4fbf\u5b9c\u7684\u8ba1\u7b97\u673a\u548c\u5176\u4ed6\u4e13\u7528\u8bbe\u5907\uff0c\u800c\u662f\u91c7\u7528\u7b80\u5355\u7684\u6bb5\u5bc4\u5b58\u5668\uff0c\u5c06\u5b58\u50a8\u5668\u5730\u5740\u5bbd\u5ea6\u4ec5\u589e\u52a0\u4e864\u4f4d\u3002\u901a\u8fc7\u5c0664-KB\u5730\u5740\u4e58\u4ee516\uff0c20\u4f4d\u5730\u5740\u53ef\u4ee5\u5904\u7406\u603b\u51711\u5146\u5b57\u8282\uff081,048,576\u5b57\u8282\uff09\uff0c\u8fd9\u5bf9\u4e8e\u5f53\u65f6\u7684\u5c0f\u578b\u8ba1\u7b97\u673a\u6765\u8bf4\u662f\u76f8\u5f53\u5927\u7684\u6570\u91cf\u3002\u5bf9\u4e8e\u4f7f\u7528\u6bb5\u5bc4\u5b58\u5668\u5feb\u901f\u4ea4\u6362\u5230\u4e0d\u540c\u4efb\u52a1\u7684\u8bb8\u591a\u5927\u578b\u673a\uff0c\u6bb5\u5bc4\u5b58\u5668\u7684\u6982\u5ff5\u5e76\u4e0d\u65b0\u9c9c\u3002\u5b9e\u9645\u4e0a\uff0c\u5728x86\u4e0a\uff0c\u5b83\u662f\u4e00\u4e2a\u5907\u53d7\u6279\u8bc4\u7684\u5b9e\u73b0\uff0c\u5b83\u4f7f\u8bb8\u591a\u5e38\u89c1\u7684\u7f16\u7a0b\u4efb\u52a1\u548c\u7f16\u8bd1\u5668\u590d\u6742\u5316\u3002\u7136\u800c\uff0c\u8be5\u67b6\u6784\u5f88\u5feb\u5c31\u5141\u8bb8\u7ebf\u602732\u4f4d\u5bfb\u5740\uff08\u4ece1985\u5e74\u672b\u768480386\u5f00\u59cb\uff09\uff0c\u4f46\u4e3b\u8981\u53c2\u4e0e\u8005\uff08\u5982\u5fae\u8f6f\uff09\u82b1\u4e86\u51e0\u5e74\u7684\u65f6\u95f4\u6765\u8f6c\u6362\u4ed6\u4eec\u768416\u4f4d\u7cfb\u7edf\u3002\u56e0\u6b64\uff0c80386\uff08\u548c80486\uff09\u5728\u5f88\u591a\u5e74\u91cc\u4e3b\u8981\u7528\u4f5c\u5feb\u901f\uff08\u4f46\u4ecd\u7136\u662f16\u4f4d\uff09\u76848086\u3002 NOTE: \u4e0a\u9762\u63d0\u53ca\u7684\u4e00\u6bb5\u8bdd\uff1ainstead adopted simple segment registers which increased the memory address width by only 4 bits\uff1b\u5fc5\u8981\u5c0f\u770b\u8fd94 bits\uff0c\u5b83\u5176\u5b9e\u662f\u6269\u5927\u4e8616\u500d\uff1b NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684However, the architecture soon allowed linear 32-bit addressing (starting with the 80386 in late 1985) \u4e2d\u7684 linear \u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u79cd\u65b9\u5f0f\uff0c\u5b83\u662f\u548csegment\u4e0d\u540c\u7684\uff1b Operating modes Real mode Main article: Real mode Real Address mode,[ 24] commonly called Real mode, is an operating mode of 8086 and later x86-compatible CPUs . Real mode is characterized by a 20-bit segmented memory address space (meaning that only 1 MiB of memory can be addressed\u2014actually, slightly more[ p] ), direct software access to peripheral hardware, and no concept of memory protection or multitasking at the hardware level. All x86 CPUs in the 80286 series and later start up in real mode at power-on; 80186 CPUs and earlier had only one operational mode, which is equivalent to real mode in later chips. (On the IBM PC platform, direct software access to the IBM BIOS routines is available only in real mode, since BIOS is written for real mode. However, this is not a characteristic of the x86 CPU but of the IBM BIOS design.) In order to use more than 64 KB of memory, the segment registers must be used. This created great complications for compiler implementors who introduced odd pointer modes such as \"near\", \"far\" and \"huge\" to leverage the implicit nature of segmented architecture to different degrees, with some pointers containing 16-bit offsets within implied segments and other pointers containing segment addresses and offsets within segments. It is technically possible to use up to 256 KB of memory for code and data, with up to 64 KB for code, by setting all four segment registers once and then only using 16-bit offsets (optionally with default-segment override prefixes) to address memory, but this puts substantial restrictions on the way data can be addressed and memory operands can be combined, and it violates the architectural intent of the Intel designers, which is for separate data items (e.g. arrays, structures, code units) to be contained in separate segments and addressed by their own segment addresses, in new programs that are not ported from earlier 8-bit processors with 16-bit address spaces. Protected mode Main article: Protected mode In addition to real mode, the Intel 80286 supports protected mode, expanding addressable physical memory to 16 MB and addressable virtual memory to 1 GB , and providing protected memory , which prevents programs from corrupting one another. This is done by using the segment registers only for storing an index into a descriptor table that is stored in memory. There are two such tables, the Global Descriptor Table (GDT) and the Local Descriptor Table (LDT), each holding up to 8192 segment descriptors, each segment giving access to 64 KB of memory. In the 80286, a segment descriptor provides a 24-bit base address , and this base address is added to a 16-bit offset to create an absolute address. The base address from the table fulfills the same role that the literal value of the segment register fulfills in real mode; the segment registers have been converted from direct registers to indirect registers. Each segment can be assigned one of four ring levels used for hardware-based computer security . Each segment descriptor also contains a segment limit field which specifies the maximum offset that may be used with the segment. Because offsets are 16 bits, segments are still limited to 64 KB each in 80286 protected mode.[ 25] Each time a segment register is loaded in protected mode, the 80286 must read a 6-byte segment descriptor from memory into a set of hidden internal registers. Therefore, loading segment registers is much slower in protected mode than in real mode, and changing segments very frequently is to be avoided. Actual memory operations using protected mode segments are not slowed much because the 80286 and later have hardware to check the offset against the segment limit in parallel with instruction execution. The Intel 80386 extended offsets and also the segment limit field in each segment descriptor to 32 bits, enabling a segment to span the entire memory space. It also introduced support in protected mode for paging , a mechanism making it possible to use paged virtual memory (with 4 KB page size). Paging allows the CPU to map any page of the virtual memory space to any page of the physical memory space. To do this, it uses additional mapping tables in memory called page tables. Protected mode on the 80386 can operate with paging either enabled or disabled; the segmentation mechanism is always active and generates virtual addresses that are then mapped by the paging mechanism if it is enabled. The segmentation mechanism can also be effectively disabled by setting all segments to have a base address of 0 and size limit equal to the whole address space; this also requires a minimally-sized segment descriptor table of only four descriptors (since the FS and GS segments need not be used).[ q] Paging is used extensively by modern multitasking operating systems. Linux , 386BSD and Windows NT were developed for the 386 because it was the first Intel architecture CPU to support paging and 32-bit segment offsets. The 386 architecture became the basis of all further development in the x86 series. x86 processors that support protected mode boot into real mode for backward compatibility with the older 8086 class of processors. Upon power-on (a.k.a. booting ), the processor initializes in real mode, and then begins executing instructions. Operating system boot code, which might be stored in ROM , may place the processor into the protected mode to enable paging and other features. The instruction set in protected mode is similar to that used in real mode. However, certain constraints that apply to real mode (such as not being able to use ax,cx,dx in addressing[ citation needed ]) do not apply in protected mode. Conversely, segment arithmetic, a common practice in real mode code, is not allowed in protected mode. x86 registers For a description of the general notion of a CPU register, see Processor register . 16-bit The original Intel 8086 and 8088 have fourteen 16- bit registers. Four of them ( AX , BX , CX , DX ) are general-purpose registers (GPRs), although each may have an additional purpose; for example, only CX can be used as a counter with the loop instruction. Each can be accessed as two separate bytes (thus BX 's high byte can be accessed as BH and low byte as BL ). Two pointer registers have special roles: SP (stack pointer) points to the \"top\" of the stack , and BP (base pointer) is often used to point at some other place in the stack, typically above the local variables (see frame pointer ). The registers SI , DI , BX and BP are address registers , and may also be used for array indexing. Four segment registers ( CS , DS , SS and ES ) are used to form a memory address. The FLAGS register contains flags such as carry flag , overflow flag and zero flag . Finally, the instruction pointer ( IP ) points to the next instruction that will be fetched from memory and then executed; this register cannot be directly accessed (read or written) by a program.[ 19] The Intel 80186 and 80188 are essentially an upgraded 8086 or 8088 CPU, respectively, with on-chip peripherals added, and they have the same CPU registers as the 8086 and 8088 (in addition to interface registers for the peripherals). The 8086, 8088, 80186, and 80188 can use an optional floating-point coprocessor, the 8087 . The 8087 appears to the programmer as part of the CPU and adds eight 80-bit wide registers, st(0) to st(7), each of which can hold numeric data in one of seven formats: 32-, 64-, or 80-bit floating point, 16-, 32-, or 64-bit (binary) integer, and 80-bit packed decimal integer.[ 7] :S-6, S-13..S-15 In the Intel 80286 , to support protected mode , three special registers hold descriptor table addresses (GDTR, LDTR, IDTR ), and a fourth task register ( TR ) is used for task switching . The 80287 is the floating-point coprocessor for the 80286 and has the same registers as the 8087 with the same data formats. 32-bit With the advent of the 32-bit 80386 processor, the 16-bit general-purpose registers, base registers, index registers, instruction pointer, and FLAGS register , but not the segment registers, were expanded to 32 bits. The nomenclature\uff08\u547d\u540d\u6cd5\uff09 represented this by prefixing an \" E \" (for \"extended\") to the register names in x86 assembly language . Thus, the AX register corresponds to the lowest 16 bits of the new 32-bit EAX register, SI corresponds to the lowest 16 bits of ESI , and so on. The general-purpose registers, base registers, and index registers can all be used as the base in addressing modes, and all of those registers except for the stack pointer can be used as the index in addressing modes. Two new segment registers ( FS and GS ) were added. With a greater number of registers, instructions and operands, the machine code format was expanded. To provide backward compatibility, segments with executable code can be marked as containing either 16-bit or 32-bit instructions. Special prefixes allow inclusion of 32-bit instructions in a 16-bit segment or vice versa . The 80386 had an optional floating-point coprocessor, the 80387 ; it had eight 80-bit wide registers: st(0) to st(7),[ 20] like the 8087 and 80287. The 80386 could also use an 80287 coprocessor.[ 21] With the 80486 and all subsequent x86 models, the floating-point processing unit (FPU) is integrated on-chip. The Pentium MMX added eight 64-bit MMX integer registers (MMX0 to MMX7, which share lower bits with the 80-bit-wide FPU stack).[ 22] With the Pentium III , Intel added a 32-bit Streaming SIMD Extensions (SSE) control/status register (MXCSR) and eight 128-bit SSE floating point registers (XMM0 to XMM7).[ 23] 64-bit Starting with the AMD Opteron processor, the x86 architecture extended the 32-bit registers into 64-bit registers in a way similar to how the 16 to 32-bit extension took place. An R -prefix identifies the 64-bit registers ( RAX , RBX , RCX , RDX , RSI , RDI , RBP , RSP , RFLAGS , RIP ), and eight additional 64-bit general registers ( R8 - R15 ) were also introduced in the creation of x86-64 . However, these extensions are only usable in 64-bit mode, which is one of the two modes only available in long mode . The addressing modes were not dramatically changed from 32-bit mode, except that addressing was extended to 64 bits, virtual addresses are now sign extended to 64 bits (in order to disallow mode bits in virtual addresses), and other selector details were dramatically reduced. In addition, an addressing mode was added to allow memory references relative to RIP (the instruction pointer ), to ease the implementation of position-independent code , used in shared libraries in some operating systems.","title":"x86"},{"location":"CPU/Manufacturer/Intel/x86/x86/#x86","text":"x86 is a family of instruction set architectures [ a] based on the Intel 8086 microprocessor and its 8088 variant. The 8086 was introduced in 1978 as a fully 16-bit extension of Intel's 8-bit 8080 microprocessor, with memory segmentation as a solution for addressing more memory than can be covered by a plain 16-bit address. The term \"x86\" came into being because the names of several successors to Intel's 8086 processor end in \"86\", including the 80186 , 80286 , 80386 and 80486 processors. NOTE: \u76ee\u524d\u6211\u4eec\u6240\u4f7f\u7528\u7684 CORE i5 \u90fd\u662f\u91c7\u7528\u7684\u8fd9\u79cdarchitecture\u3002 Many additions and extensions have been added to the x86 instruction set over the years, almost consistently with full backward compatibility .[ b] The architecture has been implemented in processors from Intel , Cyrix , AMD , VIA and many other companies; there are also open implementations, such as the Zet SoC platform.[ 2] Nevertheless, of those, only Intel, AMD, and VIA hold x86 architectural licenses , and are producing modern 64-bit designs.[ 3] [ irrelevant citation ] The term is not synonymous with IBM PC compatibility , as this implies a multitude of other computer hardware ; embedded systems , as well as general-purpose computers, used x86 chips before the PC-compatible market started ,[ c] some of them before the IBM PC (1981) itself. As of 2018, the majority of personal computers and laptops \uff08\u7b14\u8bb0\u672c\u7535\u8111\uff09sold are based on the x86 architecture, while other categories\u2014especially high-volume[ clarification needed ] mobile categories such as smartphones or tablets \uff08\u5e73\u677f\u7535\u8111\uff09\u2014are dominated by ARM ; at the high end, x86 continues to dominate compute-intensive\uff08\u8ba1\u7b97\u5bc6\u96c6\u7684\uff09 workstation and cloud computing segments.[ 4]","title":"x86"},{"location":"CPU/Manufacturer/Intel/x86/x86/#overview","text":"Although the 8086 was primarily developed for embedded systems and small multi-user or single-user computers, largely as a response to the successful 8080-compatible Zilog Z80 ,[ 8] the x86 line soon grew in features and processing power. Today, x86 is ubiquitous\uff08\u65e0\u5904\u4e0d\u5728\uff09 in both stationary and portable personal computers, and is also used in midrange computers , workstations , servers and most new supercomputer clusters of the TOP500 list. A large amount of software , including a large list of x86 operating systems are using x86-based hardware. Modern x86 is relatively uncommon in embedded systems , however, and small low power applications (using tiny batteries) as well as low-cost microprocessor markets, such as home appliances and toys, lack any significant x86 presence.[ h] Simple 8-bit and 16-bit based architectures are common here, although the x86-compatible VIA C7 , VIA Nano , AMD 's Geode , Athlon Neo and Intel Atom are examples of 32- and 64-bit designs used in some relatively low power and low cost segments.","title":"Overview"},{"location":"CPU/Manufacturer/Intel/x86/x86/#chronology","text":"","title":"Chronology\uff08\u5e74\u4ee3\u8868\uff09"},{"location":"CPU/Manufacturer/Intel/x86/x86/#history","text":"","title":"History"},{"location":"CPU/Manufacturer/Intel/x86/x86/#other#manufacturers","text":"","title":"Other manufacturers"},{"location":"CPU/Manufacturer/Intel/x86/x86/#extensions#of#word#size","text":"The instruction set architecture has twice been extended to a larger word size . In 1985, Intel released the 32-bit 80386 (later known as i386) which gradually replaced the earlier 16-bit chips in computers (although typically not in embedded systems ) during the following years; this extended programming model was originally referred to as the i386 architecture (like its first implementation) but Intel later dubbed it IA-32 when introducing its (unrelated) IA-64 architecture. In 1999-2003, AMD extended this 32-bit architecture to 64 bits and referred to it as x86-64 in early documents and later as AMD64 . Intel soon adopted AMD's architectural extensions under the name IA-32e, later using the name EM64T and finally using Intel 64. Microsoft and Sun Microsystems / Oracle also use term \"x64\", while many Linux distributions , and the BSDs also use the \"amd64\" term. Microsoft Windows, for example, designates its 32-bit versions as \"x86\" and 64-bit versions as \"x64\", while installation files of 64-bit Windows versions are required to be placed into a directory called \"AMD64\".[ 12]","title":"Extensions of word size"},{"location":"CPU/Manufacturer/Intel/x86/x86/#overview_1","text":"","title":"Overview"},{"location":"CPU/Manufacturer/Intel/x86/x86/#basic#properties#of#the#architecture","text":"The x86 architecture is a variable instruction length, primarily \" CISC \" design with emphasis on backward compatibility . The instruction set is not typical CISC, however, but basically an extended version of the simple eight-bit 8008 and 8080 architectures. Byte-addressing is enabled and words are stored in memory with little-endian byte order. Memory access to unaligned addresses is allowed for all valid word sizes. The largest native size for integer arithmetic and memory addresses (or offsets ) is 16, 32 or 64 bits depending on architecture generation (newer processors include direct support for smaller integers as well). Multiple scalar values can be handled simultaneously via the SIMD unit present in later generations, as described below.[ l] Immediate addressing offsets and immediate data may be expressed as 8-bit quantities for the frequently occurring cases or contexts where a -128..127 range is enough. Typical instructions are therefore 2 or 3 bytes in length (although some are much longer, and some are single-byte). To further conserve encoding space, most registers are expressed in opcodes using three or four bits, the latter via an opcode prefix in 64-bit mode, while at most one operand to an instruction can be a memory location.[ m] However, this memory operand may also be the destination (or a combined source and destination), while the other operand, the source , can be either register or immediate . Among other factors, this contributes to a code size that rivals eight-bit machines and enables efficient use of instruction cache memory. The relatively small number of general registers (also inherited from its 8-bit ancestors) has made register-relative addressing (using small immediate offsets) an important method of accessing operands, especially on the stack. Much work has therefore been invested in making such accesses as fast as register accesses, i.e. a one cycle instruction throughput, in most circumstances where the accessed data is available in the top-level cache.","title":"Basic properties of the architecture"},{"location":"CPU/Manufacturer/Intel/x86/x86/#floating#point#and#simd","text":"A dedicated floating point processor with 80-bit internal registers, the 8087 , was developed for the original 8086 . This microprocessor subsequently developed into the extended 80387 , and later processors incorporated a backward compatible version of this functionality on the same microprocessor as the main processor. In addition to this, modern x86 designs also contain a SIMD -unit (see SSE below) where instructions can work in parallel on (one or two) 128-bit words, each containing two or four floating point numbers (each 64 or 32 bits wide respectively), or alternatively, 2, 4, 8 or 16 integers (each 64, 32, 16 or 8 bits wide respectively). The presence of wide SIMD registers means that existing x86 processors can load or store up to 128 bits of memory data in a single instruction and also perform bitwise operations (although not integer arithmetic[ n] ) on full 128-bits quantities in parallel. Intel's Sandy Bridge processors added the AVX (Advanced Vector Extensions) instructions, widening the SIMD registers to 256 bits. Knights Corner, the architecture used by Intel on their Xeon Phi co-processors, uses 512-bit wide SIMD registers.","title":"Floating point and SIMD"},{"location":"CPU/Manufacturer/Intel/x86/x86/#current#implementations","text":"During execution , current x86 processors employ a few extra decoding steps to split most instructions into smaller pieces called micro-operations. These are then handed to a control unit that buffers and schedules them in compliance with x86-semantics so that they can be executed, partly in parallel, by one of several (more or less specialized) execution units . These modern x86 designs are thus pipelined , superscalar , and also capable of out of order and speculative execution (via branch prediction , register renaming , and memory dependence prediction ), which means they may execute multiple (partial or complete) x86 instructions simultaneously, and not necessarily in the same order as given in the instruction stream.[ 13] Intel's and AMD's (starting from AMD Zen ) CPUs are also capable of simultaneous multithreading with two threads per core ( Xeon Phi has four threads per core) and in case of Intel transactional memory ( TSX ). When introduced, in the mid-1990s, this method was sometimes referred to as a \"RISC core\" or as \"RISC translation\", partly for marketing reasons, but also because these micro-operations share some properties with certain types of RISC instructions. However, traditional microcode (used since the 1950s) also inherently shares many of the same properties; the new method differs mainly in that the translation to micro-operations now occurs asynchronously. Not having to synchronize the execution units with the decode steps opens up possibilities for more analysis of the (buffered) code stream, and therefore permits detection of operations that can be performed in parallel, simultaneously feeding more than one execution unit. The latest processors also do the opposite when appropriate; they combine certain x86 sequences (such as a compare followed by a conditional jump) into a more complex micro-op which fits the execution model better and thus can be executed faster or with less machine resources involved. Another way to try to improve performance is to cache the decoded micro-operations, so the processor can directly access the decoded micro-operations from a special cache, instead of decoding them again. Intel followed this approach with the Execution Trace Cache feature in their NetBurst Microarchitecture (for Pentium 4 processors) and later in the Decoded Stream Buffer (for Core-branded processors since Sandy Bridge).[ 14] Transmeta used a completely different method in their x86 compatible CPUs. They used just-in-time translation to convert x86 instructions to the CPU's native VLIW instruction set. Transmeta argued that their approach allows for more power efficient designs since the CPU can forgo the complicated decode step of more traditional x86 implementations.","title":"Current implementations"},{"location":"CPU/Manufacturer/Intel/x86/x86/#segmentation","text":"Further information: x86 memory segmentation Minicomputers during the late 1970s were running up against the 16-bit 64- KB address limit, as memory had become cheaper. Some minicomputers like the PDP-11 used complex bank-switching schemes, or, in the case of Digital's VAX , redesigned much more expensive processors which could directly handle 32-bit addressing and data. The original 8086, developed from the simple 8080 microprocessor and primarily aiming at very small and inexpensive computers and other specialized devices, instead adopted simple segment registers which increased the memory address width by only 4 bits. By multiplying a 64-KB address by 16, the 20-bit address could address a total of one megabyte (1,048,576 bytes) which was quite a large amount for a small computer at the time. The concept of segment registers was not new to many mainframes which used segment registers to swap quickly to different tasks. In practice, on the x86 it was (is) a much-criticized implementation which greatly complicated many common programming tasks and compilers. However, the architecture soon allowed linear 32-bit addressing (starting with the 80386 in late 1985) but major actors (such as Microsoft ) took several years to convert their 16-bit based systems. The 80386 (and 80486) was therefore largely used as a fast (but still 16-bit based) 8086 for many years. NOTE : 20\u4e16\u7eaa70\u5e74\u4ee3\u540e\u671f\u7684\u5c0f\u578b\u8ba1\u7b97\u673a\u572816\u4f4d64 KB\u5730\u5740\u9650\u5236\u4e0b\u8fd0\u884c\uff0c\u56e0\u4e3a\u5185\u5b58\u53d8\u5f97\u66f4\u4fbf\u5b9c\u3002\u50cfPDP-11\u8fd9\u6837\u7684\u5c0f\u578b\u8ba1\u7b97\u673a\u4f7f\u7528\u590d\u6742\u7684\u5b58\u50a8\u4f53\u5207\u6362\u65b9\u6848\uff0c\u6216\u8005\u5728Digital\u7684VAX\u60c5\u51b5\u4e0b\uff0c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u66f4\u6602\u8d35\u7684\u5904\u7406\u5668\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u740632\u4f4d\u5bfb\u5740\u548c\u6570\u636e\u3002\u6700\u521d\u76848086\u662f\u4ece\u7b80\u5355\u76848080\u5fae\u5904\u7406\u5668\u5f00\u53d1\u51fa\u6765\u7684\uff0c\u4e3b\u8981\u9488\u5bf9\u975e\u5e38\u5c0f\u800c\u4fbf\u5b9c\u7684\u8ba1\u7b97\u673a\u548c\u5176\u4ed6\u4e13\u7528\u8bbe\u5907\uff0c\u800c\u662f\u91c7\u7528\u7b80\u5355\u7684\u6bb5\u5bc4\u5b58\u5668\uff0c\u5c06\u5b58\u50a8\u5668\u5730\u5740\u5bbd\u5ea6\u4ec5\u589e\u52a0\u4e864\u4f4d\u3002\u901a\u8fc7\u5c0664-KB\u5730\u5740\u4e58\u4ee516\uff0c20\u4f4d\u5730\u5740\u53ef\u4ee5\u5904\u7406\u603b\u51711\u5146\u5b57\u8282\uff081,048,576\u5b57\u8282\uff09\uff0c\u8fd9\u5bf9\u4e8e\u5f53\u65f6\u7684\u5c0f\u578b\u8ba1\u7b97\u673a\u6765\u8bf4\u662f\u76f8\u5f53\u5927\u7684\u6570\u91cf\u3002\u5bf9\u4e8e\u4f7f\u7528\u6bb5\u5bc4\u5b58\u5668\u5feb\u901f\u4ea4\u6362\u5230\u4e0d\u540c\u4efb\u52a1\u7684\u8bb8\u591a\u5927\u578b\u673a\uff0c\u6bb5\u5bc4\u5b58\u5668\u7684\u6982\u5ff5\u5e76\u4e0d\u65b0\u9c9c\u3002\u5b9e\u9645\u4e0a\uff0c\u5728x86\u4e0a\uff0c\u5b83\u662f\u4e00\u4e2a\u5907\u53d7\u6279\u8bc4\u7684\u5b9e\u73b0\uff0c\u5b83\u4f7f\u8bb8\u591a\u5e38\u89c1\u7684\u7f16\u7a0b\u4efb\u52a1\u548c\u7f16\u8bd1\u5668\u590d\u6742\u5316\u3002\u7136\u800c\uff0c\u8be5\u67b6\u6784\u5f88\u5feb\u5c31\u5141\u8bb8\u7ebf\u602732\u4f4d\u5bfb\u5740\uff08\u4ece1985\u5e74\u672b\u768480386\u5f00\u59cb\uff09\uff0c\u4f46\u4e3b\u8981\u53c2\u4e0e\u8005\uff08\u5982\u5fae\u8f6f\uff09\u82b1\u4e86\u51e0\u5e74\u7684\u65f6\u95f4\u6765\u8f6c\u6362\u4ed6\u4eec\u768416\u4f4d\u7cfb\u7edf\u3002\u56e0\u6b64\uff0c80386\uff08\u548c80486\uff09\u5728\u5f88\u591a\u5e74\u91cc\u4e3b\u8981\u7528\u4f5c\u5feb\u901f\uff08\u4f46\u4ecd\u7136\u662f16\u4f4d\uff09\u76848086\u3002 NOTE: \u4e0a\u9762\u63d0\u53ca\u7684\u4e00\u6bb5\u8bdd\uff1ainstead adopted simple segment registers which increased the memory address width by only 4 bits\uff1b\u5fc5\u8981\u5c0f\u770b\u8fd94 bits\uff0c\u5b83\u5176\u5b9e\u662f\u6269\u5927\u4e8616\u500d\uff1b NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684However, the architecture soon allowed linear 32-bit addressing (starting with the 80386 in late 1985) \u4e2d\u7684 linear \u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u79cd\u65b9\u5f0f\uff0c\u5b83\u662f\u548csegment\u4e0d\u540c\u7684\uff1b","title":"Segmentation"},{"location":"CPU/Manufacturer/Intel/x86/x86/#operating#modes","text":"","title":"Operating modes"},{"location":"CPU/Manufacturer/Intel/x86/x86/#real#mode","text":"Main article: Real mode Real Address mode,[ 24] commonly called Real mode, is an operating mode of 8086 and later x86-compatible CPUs . Real mode is characterized by a 20-bit segmented memory address space (meaning that only 1 MiB of memory can be addressed\u2014actually, slightly more[ p] ), direct software access to peripheral hardware, and no concept of memory protection or multitasking at the hardware level. All x86 CPUs in the 80286 series and later start up in real mode at power-on; 80186 CPUs and earlier had only one operational mode, which is equivalent to real mode in later chips. (On the IBM PC platform, direct software access to the IBM BIOS routines is available only in real mode, since BIOS is written for real mode. However, this is not a characteristic of the x86 CPU but of the IBM BIOS design.) In order to use more than 64 KB of memory, the segment registers must be used. This created great complications for compiler implementors who introduced odd pointer modes such as \"near\", \"far\" and \"huge\" to leverage the implicit nature of segmented architecture to different degrees, with some pointers containing 16-bit offsets within implied segments and other pointers containing segment addresses and offsets within segments. It is technically possible to use up to 256 KB of memory for code and data, with up to 64 KB for code, by setting all four segment registers once and then only using 16-bit offsets (optionally with default-segment override prefixes) to address memory, but this puts substantial restrictions on the way data can be addressed and memory operands can be combined, and it violates the architectural intent of the Intel designers, which is for separate data items (e.g. arrays, structures, code units) to be contained in separate segments and addressed by their own segment addresses, in new programs that are not ported from earlier 8-bit processors with 16-bit address spaces.","title":"Real mode"},{"location":"CPU/Manufacturer/Intel/x86/x86/#protected#mode","text":"Main article: Protected mode In addition to real mode, the Intel 80286 supports protected mode, expanding addressable physical memory to 16 MB and addressable virtual memory to 1 GB , and providing protected memory , which prevents programs from corrupting one another. This is done by using the segment registers only for storing an index into a descriptor table that is stored in memory. There are two such tables, the Global Descriptor Table (GDT) and the Local Descriptor Table (LDT), each holding up to 8192 segment descriptors, each segment giving access to 64 KB of memory. In the 80286, a segment descriptor provides a 24-bit base address , and this base address is added to a 16-bit offset to create an absolute address. The base address from the table fulfills the same role that the literal value of the segment register fulfills in real mode; the segment registers have been converted from direct registers to indirect registers. Each segment can be assigned one of four ring levels used for hardware-based computer security . Each segment descriptor also contains a segment limit field which specifies the maximum offset that may be used with the segment. Because offsets are 16 bits, segments are still limited to 64 KB each in 80286 protected mode.[ 25] Each time a segment register is loaded in protected mode, the 80286 must read a 6-byte segment descriptor from memory into a set of hidden internal registers. Therefore, loading segment registers is much slower in protected mode than in real mode, and changing segments very frequently is to be avoided. Actual memory operations using protected mode segments are not slowed much because the 80286 and later have hardware to check the offset against the segment limit in parallel with instruction execution. The Intel 80386 extended offsets and also the segment limit field in each segment descriptor to 32 bits, enabling a segment to span the entire memory space. It also introduced support in protected mode for paging , a mechanism making it possible to use paged virtual memory (with 4 KB page size). Paging allows the CPU to map any page of the virtual memory space to any page of the physical memory space. To do this, it uses additional mapping tables in memory called page tables. Protected mode on the 80386 can operate with paging either enabled or disabled; the segmentation mechanism is always active and generates virtual addresses that are then mapped by the paging mechanism if it is enabled. The segmentation mechanism can also be effectively disabled by setting all segments to have a base address of 0 and size limit equal to the whole address space; this also requires a minimally-sized segment descriptor table of only four descriptors (since the FS and GS segments need not be used).[ q] Paging is used extensively by modern multitasking operating systems. Linux , 386BSD and Windows NT were developed for the 386 because it was the first Intel architecture CPU to support paging and 32-bit segment offsets. The 386 architecture became the basis of all further development in the x86 series. x86 processors that support protected mode boot into real mode for backward compatibility with the older 8086 class of processors. Upon power-on (a.k.a. booting ), the processor initializes in real mode, and then begins executing instructions. Operating system boot code, which might be stored in ROM , may place the processor into the protected mode to enable paging and other features. The instruction set in protected mode is similar to that used in real mode. However, certain constraints that apply to real mode (such as not being able to use ax,cx,dx in addressing[ citation needed ]) do not apply in protected mode. Conversely, segment arithmetic, a common practice in real mode code, is not allowed in protected mode.","title":"Protected mode"},{"location":"CPU/Manufacturer/Intel/x86/x86/#x86#registers","text":"For a description of the general notion of a CPU register, see Processor register .","title":"x86 registers"},{"location":"CPU/Manufacturer/Intel/x86/x86/#16-bit","text":"The original Intel 8086 and 8088 have fourteen 16- bit registers. Four of them ( AX , BX , CX , DX ) are general-purpose registers (GPRs), although each may have an additional purpose; for example, only CX can be used as a counter with the loop instruction. Each can be accessed as two separate bytes (thus BX 's high byte can be accessed as BH and low byte as BL ). Two pointer registers have special roles: SP (stack pointer) points to the \"top\" of the stack , and BP (base pointer) is often used to point at some other place in the stack, typically above the local variables (see frame pointer ). The registers SI , DI , BX and BP are address registers , and may also be used for array indexing. Four segment registers ( CS , DS , SS and ES ) are used to form a memory address. The FLAGS register contains flags such as carry flag , overflow flag and zero flag . Finally, the instruction pointer ( IP ) points to the next instruction that will be fetched from memory and then executed; this register cannot be directly accessed (read or written) by a program.[ 19] The Intel 80186 and 80188 are essentially an upgraded 8086 or 8088 CPU, respectively, with on-chip peripherals added, and they have the same CPU registers as the 8086 and 8088 (in addition to interface registers for the peripherals). The 8086, 8088, 80186, and 80188 can use an optional floating-point coprocessor, the 8087 . The 8087 appears to the programmer as part of the CPU and adds eight 80-bit wide registers, st(0) to st(7), each of which can hold numeric data in one of seven formats: 32-, 64-, or 80-bit floating point, 16-, 32-, or 64-bit (binary) integer, and 80-bit packed decimal integer.[ 7] :S-6, S-13..S-15 In the Intel 80286 , to support protected mode , three special registers hold descriptor table addresses (GDTR, LDTR, IDTR ), and a fourth task register ( TR ) is used for task switching . The 80287 is the floating-point coprocessor for the 80286 and has the same registers as the 8087 with the same data formats.","title":"16-bit"},{"location":"CPU/Manufacturer/Intel/x86/x86/#32-bit","text":"With the advent of the 32-bit 80386 processor, the 16-bit general-purpose registers, base registers, index registers, instruction pointer, and FLAGS register , but not the segment registers, were expanded to 32 bits. The nomenclature\uff08\u547d\u540d\u6cd5\uff09 represented this by prefixing an \" E \" (for \"extended\") to the register names in x86 assembly language . Thus, the AX register corresponds to the lowest 16 bits of the new 32-bit EAX register, SI corresponds to the lowest 16 bits of ESI , and so on. The general-purpose registers, base registers, and index registers can all be used as the base in addressing modes, and all of those registers except for the stack pointer can be used as the index in addressing modes. Two new segment registers ( FS and GS ) were added. With a greater number of registers, instructions and operands, the machine code format was expanded. To provide backward compatibility, segments with executable code can be marked as containing either 16-bit or 32-bit instructions. Special prefixes allow inclusion of 32-bit instructions in a 16-bit segment or vice versa . The 80386 had an optional floating-point coprocessor, the 80387 ; it had eight 80-bit wide registers: st(0) to st(7),[ 20] like the 8087 and 80287. The 80386 could also use an 80287 coprocessor.[ 21] With the 80486 and all subsequent x86 models, the floating-point processing unit (FPU) is integrated on-chip. The Pentium MMX added eight 64-bit MMX integer registers (MMX0 to MMX7, which share lower bits with the 80-bit-wide FPU stack).[ 22] With the Pentium III , Intel added a 32-bit Streaming SIMD Extensions (SSE) control/status register (MXCSR) and eight 128-bit SSE floating point registers (XMM0 to XMM7).[ 23]","title":"32-bit"},{"location":"CPU/Manufacturer/Intel/x86/x86/#64-bit","text":"Starting with the AMD Opteron processor, the x86 architecture extended the 32-bit registers into 64-bit registers in a way similar to how the 16 to 32-bit extension took place. An R -prefix identifies the 64-bit registers ( RAX , RBX , RCX , RDX , RSI , RDI , RBP , RSP , RFLAGS , RIP ), and eight additional 64-bit general registers ( R8 - R15 ) were also introduced in the creation of x86-64 . However, these extensions are only usable in 64-bit mode, which is one of the two modes only available in long mode . The addressing modes were not dramatically changed from 32-bit mode, except that addressing was extended to 64 bits, virtual addresses are now sign extended to 64 bits (in order to disallow mode bits in virtual addresses), and other selector details were dramatically reduced. In addition, an addressing mode was added to allow memory references relative to RIP (the instruction pointer ), to ease the implementation of position-independent code , used in shared libraries in some operating systems.","title":"64-bit"},{"location":"CPU/Manufacturer/Intel/x86/Atomicity-on-x86/","text":"Atomicity on x86 stackoverflow Atomicity on x86","title":"Introduction"},{"location":"CPU/Manufacturer/Intel/x86/Atomicity-on-x86/#atomicity#on#x86","text":"stackoverflow Atomicity on x86","title":"Atomicity on x86"},{"location":"CPU/Manufacturer/Intel/x86/Book-x86-assembly/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u662f\u9605\u8bfbwikibooks x86 Assembly \u7684\u9605\u8bfb\u7b14\u8bb0\u3002","title":"Introduction"},{"location":"CPU/Manufacturer/Intel/x86/Book-x86-assembly/#_1","text":"\u672c\u7ae0\u662f\u9605\u8bfbwikibooks x86 Assembly \u7684\u9605\u8bfb\u7b14\u8bb0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Manufacturer/Intel/x86/Book-x86-disassembly/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u662f\u9605\u8bfbwikibooks x86 Disassembly \u7684\u9605\u8bfb\u7b14\u8bb0\u3002","title":"Introduction"},{"location":"CPU/Manufacturer/Intel/x86/Book-x86-disassembly/#_1","text":"\u672c\u7ae0\u662f\u9605\u8bfbwikibooks x86 Disassembly \u7684\u9605\u8bfb\u7b14\u8bb0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/","text":"\u5173\u4e8e\u672c\u7ae0 x86 \u662f\u6bd4\u8f83\u5e38\u89c1\u7684 instruction set architecture \uff0c\u672c\u7ae0\u6574\u7406\u5b83\u7684\u76f8\u5173\u77e5\u8bc6\u3002 \u5165\u95e8\u8bfb\u7269: virginia x86 Assembly Guide Memory and Addressing Modes Declaring Static Data Regions You can declare static data regions (analogous to global variables) in x86 assembly using special assembler directives for this purpose. Data declarations should be preceded by the .DATA directive. Following this directive, the directives DB , DW , and DD can be used to declare one, two, and four byte data locations, respectively. Declared locations can be labeled with names for later reference \u2014 this is similar to declaring variables by name, but abides by some lower level rules. For example, locations declared in sequence will be located in memory next to one another. Example declarations: .DATA var DB 64 ; Declare a byte, referred to as location var, containing the value 64. var2 DB ? ; Declare an uninitialized byte, referred to as location var2. DB 10 ; Declare a byte with no label, containing the value 10. Its location is var2 + 1. X DW ? ; Declare a 2-byte uninitialized value, referred to as location X. Y DD 30000 ; Declare a 4-byte value, referred to as location Y, initialized to 30000. Unlike in high level languages where arrays can have many dimensions and are accessed by indices, arrays in x86 assembly language are simply a number of cells located contiguously in memory. An array can be declared by just listing the values, as in the first example below. Two other common methods used for declaring arrays of data are the DUP directive and the use of string literals. The DUP directive tells the assembler to duplicate an expression a given number of times. For example, 4 DUP(2) is equivalent to 2, 2, 2, 2 . Some examples: Z DD 1, 2, 3 ; Declare three 4-byte values, initialized to 1, 2, and 3. The value of location Z + 8 will be 3. bytes DB 10 DUP(?) ; Declare 10 uninitialized bytes starting at location bytes. arr DD 100 DUP(0) ; Declare 100 4-byte words starting at location arr, all initialized to 0 str DB 'hello',0 ; Declare 6 bytes starting at the address str, initialized to the ASCII character values for hello and the null (0) byte. Addressing Memory Modern x86-compatible processors are capable of addressing up to 232 bytes of memory: memory addresses are 32-bits wide. In the examples above, where we used labels to refer to memory regions, these labels are actually replaced by the assembler with 32-bit quantities that specify addresses in memory. In addition to supporting referring to memory regions by labels (i.e. constant values), the x86 provides a flexible scheme for computing and referring to memory addresses: up to two of the 32-bit registers and a 32-bit signed constant can be added together to compute a memory address. One of the registers can be optionally pre-multiplied by 2, 4, or 8. The addressing modes can be used with many x86 instructions (we'll describe them in the next section). Here we illustrate some examples using the mov instruction that moves data between registers and memory. This instruction has two operands: the first is the destination and the second specifies the source. Some examples of mov instructions using address computations are: mov eax, [ebx] ; Move the 4 bytes in memory at the address contained in EBX into EAX mov [var], ebx ; Move the contents of EBX into the 4 bytes at memory address var. (Note, var is a 32-bit constant). mov eax, [esi-4] ; Move 4 bytes at memory address ESI + (-4) into EAX mov [esi+eax], cl ; Move the contents of CL into the byte at address ESI+EAX mov edx, [esi+4*ebx] ; Move the 4 bytes of data at address ESI+4*EBX into EDX NOTE: mov \u6307\u4ee4\u53ef\u4ee5\u770b\u505a\u662f: move to DES from SRC Some examples of invalid address calculations include: mov eax, [ebx-ecx] ; Can only add register values mov [eax+esi+edi], ebx ; At most 2 registers in address computation Size Directives In general, the intended size of the of the data item at a given memory address can be inferred from the assembly code instruction in which it is referenced. For example, in all of the above instructions, the size of the memory regions could be inferred from the size of the register operand . When we were loading a 32-bit register, the assembler could infer that the region of memory we were referring to was 4 bytes wide. When we were storing the value of a one byte register to memory, the assembler could infer that we wanted the address to refer to a single byte in memory. However, in some cases the size of a referred-to memory region is ambiguous. Consider the instruction mov [ebx], 2 . Should this instruction move the value 2 into the single byte at address EBX ? Perhaps it should move the 32-bit integer representation of 2 into the 4-bytes starting at address EBX . Since either is a valid possible interpretation, the assembler must be explicitly directed as to which is correct. The size directives BYTE PTR , WORD PTR , and DWORD PTR serve this purpose, indicating sizes of 1, 2, and 4 bytes respectively. For example: mov BYTE PTR [ebx], 2 ; Move 2 into the single byte at the address stored in EBX. mov WORD PTR [ebx], 2 ; Move the 16-bit integer representation of 2 into the 2 bytes starting at the address in EBX. mov DWORD PTR [ebx], 2 ; Move the 32-bit integer representation of 2 into the 4 bytes starting at the address in EBX. Instructions Machine instructions generally fall into three categories: data movement arithmetic/logic control-flow. In this section, we will look at important examples of x86 instructions from each category. This section should not be considered an exhaustive list of x86 instructions, but rather a useful subset. For a complete list, see Intel's instruction set reference. We use the following notation: <reg32> Any 32-bit register (EAX, EBX, ECX, EDX, ESI, EDI, ESP, or EBP) <reg16> Any 16-bit register (AX, BX, CX, or DX) <reg8> Any 8-bit register (AH, BH, CH, DH, AL, BL, CL, or DL) <reg> Any register <mem> A memory address (e.g., [eax], [var + 4], or dword ptr [eax+ebx]) <con32> Any 32-bit constant <con16> Any 16-bit constant <con8> Any 8-bit constant <con> Any 8-, 16-, or 32-bit constant Data Movement Instructions mov mov \u2014 Move (Opcodes: 88, 89, 8A, 8B, 8C, 8E, ...) The mov instruction copies the data item referred to by its second operand (i.e. register contents, memory contents, or a constant value) into the location referred to by its first operand (i.e. a register or memory). While register-to-register moves are possible, direct memory-to-memory moves are not . In cases where memory transfers are desired, the source memory contents must first be loaded into a register, then can be stored to the destination memory address. Syntax mov <reg>,<reg> mov <reg>,<mem> mov <mem>,<reg> mov <reg>,<const> mov <mem>,<const> Examples mov eax, ebx \u2014 copy the value in ebx into eax mov byte ptr [var], 5 \u2014 store the value 5 into the byte at location var push push \u2014 Push stack (Opcodes: FF, 89, 8A, 8B, 8C, 8E, ...) The push instruction places its operand onto the top of the hardware supported stack in memory. Specifically, push first decrements ESP by 4, then places its operand into the contents of the 32-bit location at address [ESP] . ESP (the stack pointer) is decremented by push since the x86 stack grows down - i.e. the stack grows from high addresses to lower addresses. Syntax push <reg32> push <mem> push <con32> Examples push eax \u2014 push eax on the stack push [var] \u2014 push the 4 bytes at address var onto the stack pop pop \u2014 Pop stack The pop instruction removes the 4-byte data element from the top of the hardware-supported stack into the specified operand (i.e. register or memory location). It first moves the 4 bytes located at memory location [SP] into the specified register or memory location, and then increments SP by 4. Syntax pop <reg32> pop <mem> Examples pop edi \u2014 pop the top element of the stack into EDI. pop [ebx] \u2014 pop the top element of the stack into memory at the four bytes starting at location EBX. lea lea \u2014 Load effective address The lea instruction places the address specified by its second operand into the register specified by its first operand. Note, the contents of the memory location are not loaded, only the effective address is computed and placed into the register. This is useful for obtaining a pointer into a memory region. Syntax lea <reg32>,<mem> Examples lea edi, [ebx+4*esi] \u2014 the quantity EBX+4*ESI is placed in EDI. lea eax, [var] \u2014 the value in var is placed in EAX. lea eax, [val] \u2014 the value val is placed in EAX. Arithmetic and Logic Instructions add add \u2014 Integer Addition The add instruction adds together its two operands, storing the result in its first operand. Note, whereas both operands may be registers, at most one operand may be a memory location. Syntax add <reg>,<reg> add <reg>,<mem> add <mem>,<reg> add <reg>,<con> add <mem>,<con> Examples add eax, 10 \u2014 EAX \u2190 EAX + 10 add BYTE PTR [var], 10 \u2014 add 10 to the single byte stored at memory address var sub sub \u2014 Integer Subtraction The sub instruction stores in the value of its first operand the result of subtracting the value of its second operand from the value of its first operand. As with add Syntax sub < reg > , < reg > sub < reg > , < mem > sub < mem > , < reg > sub < reg > , < con > sub < mem > , < con > Examples sub al, ah \u2014 AL \u2190 AL - AH sub eax, 216 \u2014 subtract 216 from the value stored in EAX inc , dec inc , dec \u2014 Increment, Decrement The inc instruction increments the contents of its operand by one. The dec instruction decrements the contents of its operand by one. Syntax inc <reg> inc <mem> dec <reg> dec <mem> Examples dec eax \u2014 subtract one from the contents of EAX. inc DWORD PTR [var] \u2014 add one to the 32-bit integer stored at location var imul imul \u2014 Integer Multiplication The imul instruction has two basic formats: two-operand (first two syntax listings above) and three-operand (last two syntax listings above). The two-operand form multiplies its two operands together and stores the result in the first operand. The result (i.e. first) operand must be a register. The three operand form multiplies its second and third operands together and stores the result in its first operand. Again, the result operand must be a register. Furthermore, the third operand is restricted to being a constant value. Syntax imul <reg32>,<reg32> imul <reg32>,<mem> imul <reg32>,<reg32>,<con> imul <reg32>,<mem>,<con> Examples imul eax, [var] \u2014 multiply the contents of EAX by the 32-bit contents of the memory location var. Store the result in EAX. imul esi, edi, 25 \u2014 ESI \u2192 EDI * 25 idiv idiv \u2014 Integer Division The idiv instruction divides the contents of the 64 bit integer EDX:EAX (constructed by viewing EDX as the most significant four bytes and EAX as the least significant four bytes) by the specified operand value. The quotient result of the division is stored into EAX , while the remainder is placed in EDX . Syntax idiv <reg32> idiv <mem> Examples idiv ebx \u2014 divide the contents of EDX:EAX by the contents of EBX. Place the quotient in EAX and the remainder in EDX. idiv DWORD PTR [var] \u2014 divide the contents of EDX:EAX by the 32-bit value stored at memory location var. Place the quotient in EAX and the remainder in EDX. and , or , xor and , or , xor \u2014 Bitwise logical and, or and exclusive or These instructions perform the specified logical operation (logical bitwise and , or , and exclusive or , respectively) on their operands, placing the result in the first operand location. Syntax and <reg>,<reg> and <reg>,<mem> and <mem>,<reg> and <reg>,<con> and <mem>,<con> or <reg>,<reg> or <reg>,<mem> or <mem>,<reg> or <reg>,<con> or <mem>,<con> xor <reg>,<reg> xor <reg>,<mem> xor <mem>,<reg> xor <reg>,<con> xor <mem>,<con> Examples and eax, 0fH \u2014 clear all but the last 4 bits of EAX. xor edx, edx \u2014 set the contents of EDX to zero. not not \u2014 Bitwise Logical Not Logically negates the operand contents (that is, flips all bit values in the operand). Syntax not <reg> not <mem> Example not BYTE PTR [var] \u2014 negate all bits in the byte at the memory location var. neg neg \u2014 Negate Performs the two's complement negation of the operand contents. Syntax neg <reg> neg <mem> Example neg eax \u2014 EAX \u2192 - EAX shl , shr shl , shr \u2014 Shift Left, Shift Right These instructions shift the bits in their first operand's contents left and right, padding the resulting empty bit positions with zeros. The shifted operand can be shifted up to 31 places. The number of bits to shift is specified by the second operand, which can be either an 8-bit constant or the register CL. In either case, shifts counts of greater then 31 are performed modulo 32. Syntax shl <reg>,<con8> shl <mem>,<con8> shl <reg>,<cl> shl <mem>,<cl> shr <reg>,<con8> shr <mem>,<con8> shr <reg>,<cl> shr <mem>,<cl> Examples shl eax, 1 \u2014 Multiply the value of EAX by 2 (if the most significant bit is 0) shr ebx, cl \u2014 Store in EBX the floor of result of dividing the value of EBX by 2n wheren is the value in CL. Control Flow Instructions The x86 processor maintains an instruction pointer ( IP ) register that is a 32-bit value indicating the location in memory where the current instruction starts. Normally, it increments to point to the next instruction in memory begins after execution an instruction. The IP register cannot be manipulated directly, but is updated implicitly by provided control flow instructions. We use the notation <label> to refer to labeled locations in the program text. Labels can be inserted anywhere in x86 assembly code text by entering a label name followed by a colon. For example, mov esi, [ebp+8] begin: xor ecx, ecx mov eax, [esi] The second instruction in this code fragment is labeled begin . Elsewhere in the code, we can refer to the memory location that this instruction is located at in memory using the more convenient symbolic name begin. This label is just a convenient way of expressing the location instead of its 32-bit value. jmp jmp \u2014 Jump Transfers program control flow to the instruction at the memory location indicated by the operand. Syntax jmp <label> Example jmp begin \u2014 Jump to the instruction labeled begin. jcondition jcondition \u2014 Conditional Jump These instructions are conditional jumps that are based on the status of a set of condition codes that are stored in a special register called the machine status word. The contents of the machine status word include information about the last arithmetic operation performed. For example, one bit of this word indicates if the last result was zero. Another indicates if the last result was negative. Based on these condition codes, a number of conditional jumps can be performed. For example, the jz instruction performs a jump to the specified operand label if the result of the last arithmetic operation was zero. Otherwise, control proceeds to the next instruction in sequence. A number of the conditional branches are given names that are intuitively based on the last operation performed being a special compare instruction, cmp (see below). For example, conditional branches such as jle and jne are based on first performing a cmp operation on the desired operands. Syntax je <label> (jump when equal) jne <label> (jump when not equal) jz <label> (jump when last result was zero) jg <label> (jump when greater than) jge <label> (jump when greater than or equal to) jl <label> (jump when less than) jle <label> (jump when less than or equal to) Example cmp eax, ebx jle done If the contents of EAX are less than or equal to the contents of EBX , jump to the label done . Otherwise, continue to the next instruction. cmp cmp \u2014 Compare Compare the values of the two specified operands, setting the condition codes in the machine status word appropriately. This instruction is equivalent to the sub instruction, except the result of the subtraction is discarded instead of replacing the first operand. Syntax cmp <reg>,<reg> cmp <reg>,<mem> cmp <mem>,<reg> cmp <reg>,<con> Example cmp DWORD PTR [var], 10 jeq loop If the 4 bytes stored at location var are equal to the 4-byte integer constant 10, jump to the location labeled loop . call , ret call , ret \u2014 Subroutine call and return These instructions implement a subroutine call and return. The call instruction first pushes the current code location onto the hardware supported stack in memory (see the push instruction for details), and then performs an unconditional jump to the code location indicated by the label operand. Unlike the simple jump instructions, the call instruction saves the location to return to when the subroutine completes. The ret instruction implements a subroutine return mechanism. This instruction first pops a code location off the hardware supported in-memory stack (see the pop instruction for details). It then performs an unconditional jump to the retrieved code location. Syntax call <label> ret Assembly Language Tutorial","title":"Introduction"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#_1","text":"x86 \u662f\u6bd4\u8f83\u5e38\u89c1\u7684 instruction set architecture \uff0c\u672c\u7ae0\u6574\u7406\u5b83\u7684\u76f8\u5173\u77e5\u8bc6\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#virginia#x86#assembly#guide","text":"","title":"\u5165\u95e8\u8bfb\u7269: virginia x86 Assembly Guide"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#memory#and#addressing#modes","text":"","title":"Memory and Addressing Modes"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#declaring#static#data#regions","text":"You can declare static data regions (analogous to global variables) in x86 assembly using special assembler directives for this purpose. Data declarations should be preceded by the .DATA directive. Following this directive, the directives DB , DW , and DD can be used to declare one, two, and four byte data locations, respectively. Declared locations can be labeled with names for later reference \u2014 this is similar to declaring variables by name, but abides by some lower level rules. For example, locations declared in sequence will be located in memory next to one another. Example declarations: .DATA var DB 64 ; Declare a byte, referred to as location var, containing the value 64. var2 DB ? ; Declare an uninitialized byte, referred to as location var2. DB 10 ; Declare a byte with no label, containing the value 10. Its location is var2 + 1. X DW ? ; Declare a 2-byte uninitialized value, referred to as location X. Y DD 30000 ; Declare a 4-byte value, referred to as location Y, initialized to 30000. Unlike in high level languages where arrays can have many dimensions and are accessed by indices, arrays in x86 assembly language are simply a number of cells located contiguously in memory. An array can be declared by just listing the values, as in the first example below. Two other common methods used for declaring arrays of data are the DUP directive and the use of string literals. The DUP directive tells the assembler to duplicate an expression a given number of times. For example, 4 DUP(2) is equivalent to 2, 2, 2, 2 . Some examples: Z DD 1, 2, 3 ; Declare three 4-byte values, initialized to 1, 2, and 3. The value of location Z + 8 will be 3. bytes DB 10 DUP(?) ; Declare 10 uninitialized bytes starting at location bytes. arr DD 100 DUP(0) ; Declare 100 4-byte words starting at location arr, all initialized to 0 str DB 'hello',0 ; Declare 6 bytes starting at the address str, initialized to the ASCII character values for hello and the null (0) byte.","title":"Declaring Static Data Regions"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#addressing#memory","text":"Modern x86-compatible processors are capable of addressing up to 232 bytes of memory: memory addresses are 32-bits wide. In the examples above, where we used labels to refer to memory regions, these labels are actually replaced by the assembler with 32-bit quantities that specify addresses in memory. In addition to supporting referring to memory regions by labels (i.e. constant values), the x86 provides a flexible scheme for computing and referring to memory addresses: up to two of the 32-bit registers and a 32-bit signed constant can be added together to compute a memory address. One of the registers can be optionally pre-multiplied by 2, 4, or 8. The addressing modes can be used with many x86 instructions (we'll describe them in the next section). Here we illustrate some examples using the mov instruction that moves data between registers and memory. This instruction has two operands: the first is the destination and the second specifies the source. Some examples of mov instructions using address computations are: mov eax, [ebx] ; Move the 4 bytes in memory at the address contained in EBX into EAX mov [var], ebx ; Move the contents of EBX into the 4 bytes at memory address var. (Note, var is a 32-bit constant). mov eax, [esi-4] ; Move 4 bytes at memory address ESI + (-4) into EAX mov [esi+eax], cl ; Move the contents of CL into the byte at address ESI+EAX mov edx, [esi+4*ebx] ; Move the 4 bytes of data at address ESI+4*EBX into EDX NOTE: mov \u6307\u4ee4\u53ef\u4ee5\u770b\u505a\u662f: move to DES from SRC Some examples of invalid address calculations include: mov eax, [ebx-ecx] ; Can only add register values mov [eax+esi+edi], ebx ; At most 2 registers in address computation","title":"Addressing Memory"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#size#directives","text":"In general, the intended size of the of the data item at a given memory address can be inferred from the assembly code instruction in which it is referenced. For example, in all of the above instructions, the size of the memory regions could be inferred from the size of the register operand . When we were loading a 32-bit register, the assembler could infer that the region of memory we were referring to was 4 bytes wide. When we were storing the value of a one byte register to memory, the assembler could infer that we wanted the address to refer to a single byte in memory. However, in some cases the size of a referred-to memory region is ambiguous. Consider the instruction mov [ebx], 2 . Should this instruction move the value 2 into the single byte at address EBX ? Perhaps it should move the 32-bit integer representation of 2 into the 4-bytes starting at address EBX . Since either is a valid possible interpretation, the assembler must be explicitly directed as to which is correct. The size directives BYTE PTR , WORD PTR , and DWORD PTR serve this purpose, indicating sizes of 1, 2, and 4 bytes respectively. For example: mov BYTE PTR [ebx], 2 ; Move 2 into the single byte at the address stored in EBX. mov WORD PTR [ebx], 2 ; Move the 16-bit integer representation of 2 into the 2 bytes starting at the address in EBX. mov DWORD PTR [ebx], 2 ; Move the 32-bit integer representation of 2 into the 4 bytes starting at the address in EBX.","title":"Size Directives"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#instructions","text":"Machine instructions generally fall into three categories: data movement arithmetic/logic control-flow. In this section, we will look at important examples of x86 instructions from each category. This section should not be considered an exhaustive list of x86 instructions, but rather a useful subset. For a complete list, see Intel's instruction set reference. We use the following notation: <reg32> Any 32-bit register (EAX, EBX, ECX, EDX, ESI, EDI, ESP, or EBP) <reg16> Any 16-bit register (AX, BX, CX, or DX) <reg8> Any 8-bit register (AH, BH, CH, DH, AL, BL, CL, or DL) <reg> Any register <mem> A memory address (e.g., [eax], [var + 4], or dword ptr [eax+ebx]) <con32> Any 32-bit constant <con16> Any 16-bit constant <con8> Any 8-bit constant <con> Any 8-, 16-, or 32-bit constant","title":"Instructions"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#data#movement#instructions","text":"","title":"Data Movement Instructions"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#mov","text":"mov \u2014 Move (Opcodes: 88, 89, 8A, 8B, 8C, 8E, ...) The mov instruction copies the data item referred to by its second operand (i.e. register contents, memory contents, or a constant value) into the location referred to by its first operand (i.e. a register or memory). While register-to-register moves are possible, direct memory-to-memory moves are not . In cases where memory transfers are desired, the source memory contents must first be loaded into a register, then can be stored to the destination memory address. Syntax mov <reg>,<reg> mov <reg>,<mem> mov <mem>,<reg> mov <reg>,<const> mov <mem>,<const> Examples mov eax, ebx \u2014 copy the value in ebx into eax mov byte ptr [var], 5 \u2014 store the value 5 into the byte at location var","title":"mov"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#push","text":"push \u2014 Push stack (Opcodes: FF, 89, 8A, 8B, 8C, 8E, ...) The push instruction places its operand onto the top of the hardware supported stack in memory. Specifically, push first decrements ESP by 4, then places its operand into the contents of the 32-bit location at address [ESP] . ESP (the stack pointer) is decremented by push since the x86 stack grows down - i.e. the stack grows from high addresses to lower addresses. Syntax push <reg32> push <mem> push <con32> Examples push eax \u2014 push eax on the stack push [var] \u2014 push the 4 bytes at address var onto the stack","title":"push"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#pop","text":"pop \u2014 Pop stack The pop instruction removes the 4-byte data element from the top of the hardware-supported stack into the specified operand (i.e. register or memory location). It first moves the 4 bytes located at memory location [SP] into the specified register or memory location, and then increments SP by 4. Syntax pop <reg32> pop <mem> Examples pop edi \u2014 pop the top element of the stack into EDI. pop [ebx] \u2014 pop the top element of the stack into memory at the four bytes starting at location EBX.","title":"pop"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#lea","text":"lea \u2014 Load effective address The lea instruction places the address specified by its second operand into the register specified by its first operand. Note, the contents of the memory location are not loaded, only the effective address is computed and placed into the register. This is useful for obtaining a pointer into a memory region. Syntax lea <reg32>,<mem> Examples lea edi, [ebx+4*esi] \u2014 the quantity EBX+4*ESI is placed in EDI. lea eax, [var] \u2014 the value in var is placed in EAX. lea eax, [val] \u2014 the value val is placed in EAX.","title":"lea"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#arithmetic#and#logic#instructions","text":"","title":"Arithmetic and Logic Instructions"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#add","text":"add \u2014 Integer Addition The add instruction adds together its two operands, storing the result in its first operand. Note, whereas both operands may be registers, at most one operand may be a memory location. Syntax add <reg>,<reg> add <reg>,<mem> add <mem>,<reg> add <reg>,<con> add <mem>,<con> Examples add eax, 10 \u2014 EAX \u2190 EAX + 10 add BYTE PTR [var], 10 \u2014 add 10 to the single byte stored at memory address var","title":"add"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#sub","text":"sub \u2014 Integer Subtraction The sub instruction stores in the value of its first operand the result of subtracting the value of its second operand from the value of its first operand. As with add Syntax sub < reg > , < reg > sub < reg > , < mem > sub < mem > , < reg > sub < reg > , < con > sub < mem > , < con > Examples sub al, ah \u2014 AL \u2190 AL - AH sub eax, 216 \u2014 subtract 216 from the value stored in EAX","title":"sub"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#inc#dec","text":"inc , dec \u2014 Increment, Decrement The inc instruction increments the contents of its operand by one. The dec instruction decrements the contents of its operand by one. Syntax inc <reg> inc <mem> dec <reg> dec <mem> Examples dec eax \u2014 subtract one from the contents of EAX. inc DWORD PTR [var] \u2014 add one to the 32-bit integer stored at location var","title":"inc, dec"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#imul","text":"imul \u2014 Integer Multiplication The imul instruction has two basic formats: two-operand (first two syntax listings above) and three-operand (last two syntax listings above). The two-operand form multiplies its two operands together and stores the result in the first operand. The result (i.e. first) operand must be a register. The three operand form multiplies its second and third operands together and stores the result in its first operand. Again, the result operand must be a register. Furthermore, the third operand is restricted to being a constant value. Syntax imul <reg32>,<reg32> imul <reg32>,<mem> imul <reg32>,<reg32>,<con> imul <reg32>,<mem>,<con> Examples imul eax, [var] \u2014 multiply the contents of EAX by the 32-bit contents of the memory location var. Store the result in EAX. imul esi, edi, 25 \u2014 ESI \u2192 EDI * 25","title":"imul"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#idiv","text":"idiv \u2014 Integer Division The idiv instruction divides the contents of the 64 bit integer EDX:EAX (constructed by viewing EDX as the most significant four bytes and EAX as the least significant four bytes) by the specified operand value. The quotient result of the division is stored into EAX , while the remainder is placed in EDX . Syntax idiv <reg32> idiv <mem> Examples idiv ebx \u2014 divide the contents of EDX:EAX by the contents of EBX. Place the quotient in EAX and the remainder in EDX. idiv DWORD PTR [var] \u2014 divide the contents of EDX:EAX by the 32-bit value stored at memory location var. Place the quotient in EAX and the remainder in EDX.","title":"idiv"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#and#or#xor","text":"and , or , xor \u2014 Bitwise logical and, or and exclusive or These instructions perform the specified logical operation (logical bitwise and , or , and exclusive or , respectively) on their operands, placing the result in the first operand location. Syntax and <reg>,<reg> and <reg>,<mem> and <mem>,<reg> and <reg>,<con> and <mem>,<con> or <reg>,<reg> or <reg>,<mem> or <mem>,<reg> or <reg>,<con> or <mem>,<con> xor <reg>,<reg> xor <reg>,<mem> xor <mem>,<reg> xor <reg>,<con> xor <mem>,<con> Examples and eax, 0fH \u2014 clear all but the last 4 bits of EAX. xor edx, edx \u2014 set the contents of EDX to zero.","title":"and, or, xor"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#not","text":"not \u2014 Bitwise Logical Not Logically negates the operand contents (that is, flips all bit values in the operand). Syntax not <reg> not <mem> Example not BYTE PTR [var] \u2014 negate all bits in the byte at the memory location var.","title":"not"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#neg","text":"neg \u2014 Negate Performs the two's complement negation of the operand contents. Syntax neg <reg> neg <mem> Example neg eax \u2014 EAX \u2192 - EAX","title":"neg"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#shl#shr","text":"shl , shr \u2014 Shift Left, Shift Right These instructions shift the bits in their first operand's contents left and right, padding the resulting empty bit positions with zeros. The shifted operand can be shifted up to 31 places. The number of bits to shift is specified by the second operand, which can be either an 8-bit constant or the register CL. In either case, shifts counts of greater then 31 are performed modulo 32. Syntax shl <reg>,<con8> shl <mem>,<con8> shl <reg>,<cl> shl <mem>,<cl> shr <reg>,<con8> shr <mem>,<con8> shr <reg>,<cl> shr <mem>,<cl> Examples shl eax, 1 \u2014 Multiply the value of EAX by 2 (if the most significant bit is 0) shr ebx, cl \u2014 Store in EBX the floor of result of dividing the value of EBX by 2n wheren is the value in CL.","title":"shl, shr"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#control#flow#instructions","text":"The x86 processor maintains an instruction pointer ( IP ) register that is a 32-bit value indicating the location in memory where the current instruction starts. Normally, it increments to point to the next instruction in memory begins after execution an instruction. The IP register cannot be manipulated directly, but is updated implicitly by provided control flow instructions. We use the notation <label> to refer to labeled locations in the program text. Labels can be inserted anywhere in x86 assembly code text by entering a label name followed by a colon. For example, mov esi, [ebp+8] begin: xor ecx, ecx mov eax, [esi] The second instruction in this code fragment is labeled begin . Elsewhere in the code, we can refer to the memory location that this instruction is located at in memory using the more convenient symbolic name begin. This label is just a convenient way of expressing the location instead of its 32-bit value.","title":"Control Flow Instructions"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#jmp","text":"jmp \u2014 Jump Transfers program control flow to the instruction at the memory location indicated by the operand. Syntax jmp <label> Example jmp begin \u2014 Jump to the instruction labeled begin.","title":"jmp"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#jcondition","text":"jcondition \u2014 Conditional Jump These instructions are conditional jumps that are based on the status of a set of condition codes that are stored in a special register called the machine status word. The contents of the machine status word include information about the last arithmetic operation performed. For example, one bit of this word indicates if the last result was zero. Another indicates if the last result was negative. Based on these condition codes, a number of conditional jumps can be performed. For example, the jz instruction performs a jump to the specified operand label if the result of the last arithmetic operation was zero. Otherwise, control proceeds to the next instruction in sequence. A number of the conditional branches are given names that are intuitively based on the last operation performed being a special compare instruction, cmp (see below). For example, conditional branches such as jle and jne are based on first performing a cmp operation on the desired operands. Syntax je <label> (jump when equal) jne <label> (jump when not equal) jz <label> (jump when last result was zero) jg <label> (jump when greater than) jge <label> (jump when greater than or equal to) jl <label> (jump when less than) jle <label> (jump when less than or equal to) Example cmp eax, ebx jle done If the contents of EAX are less than or equal to the contents of EBX , jump to the label done . Otherwise, continue to the next instruction.","title":"jcondition"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#cmp","text":"cmp \u2014 Compare Compare the values of the two specified operands, setting the condition codes in the machine status word appropriately. This instruction is equivalent to the sub instruction, except the result of the subtraction is discarded instead of replacing the first operand. Syntax cmp <reg>,<reg> cmp <reg>,<mem> cmp <mem>,<reg> cmp <reg>,<con> Example cmp DWORD PTR [var], 10 jeq loop If the 4 bytes stored at location var are equal to the 4-byte integer constant 10, jump to the location labeled loop .","title":"cmp"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/#call#ret","text":"call , ret \u2014 Subroutine call and return These instructions implement a subroutine call and return. The call instruction first pushes the current code location onto the hardware supported stack in memory (see the push instruction for details), and then performs an unconditional jump to the code location indicated by the label operand. Unlike the simple jump instructions, the call instruction saves the location to return to when the subroutine completes. The ret instruction implements a subroutine return mechanism. This instruction first pops a code location off the hardware supported in-memory stack (see the pop instruction for details). It then performs an unconditional jump to the retrieved code location. Syntax call <label> ret Assembly Language Tutorial","title":"call, ret"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/Bound/","text":"BOUND felixcloutier BOUND \u2014 Check Array Index Against Bounds","title":"Bound.md"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/Bound/#bound","text":"","title":"BOUND"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/Bound/#felixcloutier#bound#check#array#index#against#bounds","text":"","title":"felixcloutier BOUND \u2014 Check Array Index Against Bounds"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/INT/","text":"INT (x86 instruction) \u7ef4\u57fa\u767e\u79d1 INT (x86 instruction) felixcloutier INT n/INTO/INT3/INT1 \u2014 Call to Interrupt Procedure Opcode Instruction Op/En 64-Bit Mode Compat/Leg Mode Description CC INT3 ZO Valid Valid Generate breakpoint trap. CD ib INT imm8 I Valid Valid Generate software interrupt with vector specified by immediate byte. CE INTO ZO Invalid Valid Generate overflow trap if overflow flag is 1. F1 INT1 ZO Valid Valid Generate debug trap. \u601d\u8003\uff1a INT1 VS INT3 \uff1f","title":"INT"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/INT/#int#x86#instruction","text":"","title":"INT (x86 instruction)"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/INT/#int#x86#instruction_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1INT (x86 instruction)"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/INT/#felixcloutier#int#nintoint3int1#call#to#interrupt#procedure","text":"Opcode Instruction Op/En 64-Bit Mode Compat/Leg Mode Description CC INT3 ZO Valid Valid Generate breakpoint trap. CD ib INT imm8 I Valid Valid Generate software interrupt with vector specified by immediate byte. CE INTO ZO Invalid Valid Generate overflow trap if overflow flag is 1. F1 INT1 ZO Valid Valid Generate debug trap. \u601d\u8003\uff1a INT1 VS INT3 \uff1f","title":"felixcloutier INT n/INTO/INT3/INT1 \u2014 Call to Interrupt Procedure"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/x86-64/","text":"x86-64","title":"x86-64"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/x86-64/#x86-64","text":"","title":"x86-64"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/x86-instruction-listings/","text":"x86 instruction listings \u7ef4\u57fa\u767e\u79d1 x86 instruction listings felixcloutier x86 and amd64 instruction reference penguin 80x86 instruction set","title":"x86-instruction-listings"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/x86-instruction-listings/#x86#instruction#listings","text":"","title":"x86 instruction listings"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/x86-instruction-listings/#x86#instruction#listings_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1x86 instruction listings"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/x86-instruction-listings/#felixcloutier#x86#and#amd64#instruction#reference","text":"","title":"felixcloutier x86 and amd64 instruction reference"},{"location":"CPU/Manufacturer/Intel/x86/Instruction/x86-instruction-listings/#penguin#80x86#instruction#set","text":"","title":"penguin 80x86 instruction set"},{"location":"CPU/Optimization/","text":"CPU optimization \u5bf9CPU\u7684optimization trick\u8fdb\u884c\u603b\u7ed3\u3002 stackoverflow What are some tricks that a processor does to optimize code?","title":"Introduction"},{"location":"CPU/Optimization/#cpu#optimization","text":"\u5bf9CPU\u7684optimization trick\u8fdb\u884c\u603b\u7ed3\u3002","title":"CPU optimization"},{"location":"CPU/Optimization/#stackoverflow#what#are#some#tricks#that#a#processor#does#to#optimize#code","text":"","title":"stackoverflow What are some tricks that a processor does to optimize code?"},{"location":"CPU-memory-access/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbaCPU\u548cmemory\u7684\u4ea4\u4e92\uff0c\u5b83\u662fCPU\u7684\u6838\u5fc3activity\u3002 Von Neumann bottleneck: \"\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\" \u53c2\u89c1 Computer-architecture\\Von-Neumann-architecture \u7ae0\u8282 Memory access processor\u548cmemory\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\u662fprocessor\u7684\u4e3b\u8981\u6d3b\u52a8\u4e4b\u4e00\uff0c\u672c\u7ae0\u4e3b\u8981\u5bf9\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\u8fdb\u884c\u6574\u7406\uff0c\u4e0b\u9762\u662f\u6d89\u53ca\u5230\u7684\u4e00\u4e9b\u5185\u5bb9\u3002 wikipedia Memory\u2013processor transfer When the processor reads from the memory subsystem into a register or writes a register's value to memory, the amount of data transferred is often a word . Historically, this amount of bits which could be transferred in one cycle was also called a catena in some environments. In simple memory subsystems, the word is transferred over the memory data bus , which typically has a width of a word or half-word. In memory subsystems that use caches , the word-sized transfer is the one between the processor and the first level of cache; at lower levels of the memory hierarchy larger transfers (which are a multiple of the word size) are normally used. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7ed9\u51fa\u4e86Memory\u2013processor transfer\u7684\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\uff0c\u8fd9\u5176\u4e2d\u4f1a\u6d89\u53ca\u4e00\u7cfb\u5217\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u672c\u7ae0\u540e\u7eed\u7ae0\u8282\u4f1a\u5bf9\u6b64\u8fdb\u884c\u5c55\u5f00\u3002 Instruction-set-architectures \u5728 CPU\\Instruction-set-architectures \u7ae0\u8282\u4ecb\u7ecd\u4e86\u4e24\u79cd\u57fa\u4e8ememory access\u7684architecture: 1\u3001 Register memory architecture 2\u3001 Load\u2013store architecture Operation on memory access \u65e0\u8bba\u591a\u4e48\u590d\u6742\uff0cmemory access\u6700\u7ec8\u90fd\u53ef\u4ee5\u5f52\u7eb3\u4e3a\u4e24\u79cdoperation: 1\u3001read 2\u3001write \u5728hardware\u4e2d\uff0c\u4e00\u822cload\u3001store\u6765\u8868\u793aread\u3001write\u3002 Memory-consistency model CPU\u7684consistency model\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u652f\u6301multiprocessing\u7684CPU\u3002\u8fd9\u662f\u6211\u5728\u9605\u8bfbWikipedia Memory ordering \u65f6\uff0c\u5176\u4e2d\u6709\u76f8\u5173\u63cf\u8ff0: There are several memory-consistency models for SMP systems NOTE: Word\u3001alignment\u3001endian\u662fprogrammer\u5e38\u5e38\u4f1a\u9047\u5230\u7684\u5173\u4e8eCPU\u7684\u4e00\u4e9b\u5185\u5bb9\u3002 Word CPU memory access(\u8bfb/\u5199)\u5355\u4f4d\u3002 Alignment CPU memory access(\u8bfb/\u5199) boundary\u3002 \u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u6982\u5ff5\uff0c\u5b83\u89e3\u91ca\u4e86\u975e\u5e38\u591a\u7684\u5185\u5bb9\u3002 Endian \u5173\u4e8ememory\u7684\u4e00\u4e2a\u91cd\u8981\u6307\u6807\u3002","title":"Introduction"},{"location":"CPU-memory-access/#_1","text":"\u672c\u7ae0\u8ba8\u8bbaCPU\u548cmemory\u7684\u4ea4\u4e92\uff0c\u5b83\u662fCPU\u7684\u6838\u5fc3activity\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/#von#neumann#bottleneck#cpu","text":"\u53c2\u89c1 Computer-architecture\\Von-Neumann-architecture \u7ae0\u8282","title":"Von Neumann bottleneck: \"\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\""},{"location":"CPU-memory-access/#memory#access","text":"processor\u548cmemory\u4e4b\u95f4\u7684\u6570\u636e\u4f20\u8f93\u662fprocessor\u7684\u4e3b\u8981\u6d3b\u52a8\u4e4b\u4e00\uff0c\u672c\u7ae0\u4e3b\u8981\u5bf9\u4e0e\u6b64\u76f8\u5173\u7684\u5185\u5bb9\u8fdb\u884c\u6574\u7406\uff0c\u4e0b\u9762\u662f\u6d89\u53ca\u5230\u7684\u4e00\u4e9b\u5185\u5bb9\u3002","title":"Memory access"},{"location":"CPU-memory-access/#wikipedia#memoryprocessor#transfer","text":"When the processor reads from the memory subsystem into a register or writes a register's value to memory, the amount of data transferred is often a word . Historically, this amount of bits which could be transferred in one cycle was also called a catena in some environments. In simple memory subsystems, the word is transferred over the memory data bus , which typically has a width of a word or half-word. In memory subsystems that use caches , the word-sized transfer is the one between the processor and the first level of cache; at lower levels of the memory hierarchy larger transfers (which are a multiple of the word size) are normally used. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7ed9\u51fa\u4e86Memory\u2013processor transfer\u7684\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\uff0c\u8fd9\u5176\u4e2d\u4f1a\u6d89\u53ca\u4e00\u7cfb\u5217\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u672c\u7ae0\u540e\u7eed\u7ae0\u8282\u4f1a\u5bf9\u6b64\u8fdb\u884c\u5c55\u5f00\u3002","title":"wikipedia Memory\u2013processor transfer"},{"location":"CPU-memory-access/#instruction-set-architectures","text":"\u5728 CPU\\Instruction-set-architectures \u7ae0\u8282\u4ecb\u7ecd\u4e86\u4e24\u79cd\u57fa\u4e8ememory access\u7684architecture: 1\u3001 Register memory architecture 2\u3001 Load\u2013store architecture","title":"Instruction-set-architectures"},{"location":"CPU-memory-access/#operation#on#memory#access","text":"\u65e0\u8bba\u591a\u4e48\u590d\u6742\uff0cmemory access\u6700\u7ec8\u90fd\u53ef\u4ee5\u5f52\u7eb3\u4e3a\u4e24\u79cdoperation: 1\u3001read 2\u3001write \u5728hardware\u4e2d\uff0c\u4e00\u822cload\u3001store\u6765\u8868\u793aread\u3001write\u3002","title":"Operation on memory access"},{"location":"CPU-memory-access/#memory-consistency#model","text":"CPU\u7684consistency model\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u652f\u6301multiprocessing\u7684CPU\u3002\u8fd9\u662f\u6211\u5728\u9605\u8bfbWikipedia Memory ordering \u65f6\uff0c\u5176\u4e2d\u6709\u76f8\u5173\u63cf\u8ff0: There are several memory-consistency models for SMP systems NOTE: Word\u3001alignment\u3001endian\u662fprogrammer\u5e38\u5e38\u4f1a\u9047\u5230\u7684\u5173\u4e8eCPU\u7684\u4e00\u4e9b\u5185\u5bb9\u3002","title":"Memory-consistency model"},{"location":"CPU-memory-access/#word","text":"CPU memory access(\u8bfb/\u5199)\u5355\u4f4d\u3002","title":"Word"},{"location":"CPU-memory-access/#alignment","text":"CPU memory access(\u8bfb/\u5199) boundary\u3002 \u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u6982\u5ff5\uff0c\u5b83\u89e3\u91ca\u4e86\u975e\u5e38\u591a\u7684\u5185\u5bb9\u3002","title":"Alignment"},{"location":"CPU-memory-access/#endian","text":"\u5173\u4e8ememory\u7684\u4e00\u4e2a\u91cd\u8981\u6307\u6807\u3002","title":"Endian"},{"location":"CPU-memory-access/CPU-cache-memory/","text":"CPU cache memory 1\u3001cache\u5bf9programmer\u662f\u900f\u660e\u7684 2\u3001\u4f46\u662f\uff0c\u5728\u8fdb\u5165\u4e86non-blocking\u3001high performance\u9886\u57df\u540e\uff0c\u53d1\u73b0\u5f88\u591atechnique\u90fd\u6d89\u53cacache\uff0c\u56e0\u6b64\uff0c\u6211\u51b3\u5b9a\u5bf9\u8fd9\u4e2atopic\u8fdb\u884c\u603b\u7ed3 3\u3001cache\u662fmemory\u4e2d\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5bf9CPU\u6027\u80fd\u7684\u5f71\u54cd\u6781\u5927 Architecture \u5efa\u7acb\u5176architecture\u5bf9\u4e8e\u7406\u89e3\u76f8\u5173\u95ee\u9898\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u6709\u7740\u975e\u5e38\u597d\u7684\u63cf\u8ff0: 1\u3001wikipedia Symmetric multiprocessing 2\u3001zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a)","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/#cpu#cache#memory","text":"1\u3001cache\u5bf9programmer\u662f\u900f\u660e\u7684 2\u3001\u4f46\u662f\uff0c\u5728\u8fdb\u5165\u4e86non-blocking\u3001high performance\u9886\u57df\u540e\uff0c\u53d1\u73b0\u5f88\u591atechnique\u90fd\u6d89\u53cacache\uff0c\u56e0\u6b64\uff0c\u6211\u51b3\u5b9a\u5bf9\u8fd9\u4e2atopic\u8fdb\u884c\u603b\u7ed3 3\u3001cache\u662fmemory\u4e2d\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u5bf9CPU\u6027\u80fd\u7684\u5f71\u54cd\u6781\u5927","title":"CPU cache memory"},{"location":"CPU-memory-access/CPU-cache-memory/#architecture","text":"\u5efa\u7acb\u5176architecture\u5bf9\u4e8e\u7406\u89e3\u76f8\u5173\u95ee\u9898\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u6709\u7740\u975e\u5e38\u597d\u7684\u63cf\u8ff0: 1\u3001wikipedia Symmetric multiprocessing 2\u3001zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a)","title":"Architecture"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/","text":"Cache zhihu \u8ba1\u7b97\u673a\u7f13\u5b58Cache\u4ee5\u53caCache Line\u8be6\u89e3","title":"Cache"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/#cache","text":"","title":"Cache"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/#zhihu#cachecache#line","text":"","title":"zhihu \u8ba1\u7b97\u673a\u7f13\u5b58Cache\u4ee5\u53caCache Line\u8be6\u89e3"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Locality/","text":"Locality \u7ef4\u57fa\u767e\u79d1 Locality of reference","title":"Locality"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Locality/#locality","text":"","title":"Locality"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Locality/#locality#of#reference","text":"","title":"\u7ef4\u57fa\u767e\u79d1Locality of reference"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/TODO/","text":"What is a cache hit and a cache miss? Why would context-switching cause cache miss?","title":"TODO"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-line/","text":"Cache line stackoverflow How do cache lines work? TODO \u5728\u9605\u8bfb stackoverflow Purpose of memory alignment # A \u65f6\uff0c\u5176\u4e2d\u7684\"Bonus: Caches\"\u6bb5\u4e2d\u63d0\u53ca\u4e86cache line\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u8fd9\u4e9b\u5185\u5bb9: Another alignment-for-performance that I alluded to previously is alignment on cache lines which are (for example, on some CPUs) 64B. For more info on how much performance can be gained by leveraging caches, take a look at Gallery of Processor Cache Effects ; from this question on cache-line sizes igoro Gallery of Processor Cache Effects stackoverflow Line size of L1 and L2 caches","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-line/#cache#line","text":"","title":"Cache line"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-line/#stackoverflow#how#do#cache#lines#work","text":"","title":"stackoverflow How do cache lines work?"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-line/#todo","text":"\u5728\u9605\u8bfb stackoverflow Purpose of memory alignment # A \u65f6\uff0c\u5176\u4e2d\u7684\"Bonus: Caches\"\u6bb5\u4e2d\u63d0\u53ca\u4e86cache line\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86\u8fd9\u4e9b\u5185\u5bb9: Another alignment-for-performance that I alluded to previously is alignment on cache lines which are (for example, on some CPUs) 64B. For more info on how much performance can be gained by leveraging caches, take a look at Gallery of Processor Cache Effects ; from this question on cache-line sizes","title":"TODO"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-line/#igoro#gallery#of#processor#cache#effects","text":"","title":"igoro Gallery of Processor Cache Effects"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-line/#stackoverflow#line#size#of#l1#and#l2#caches","text":"","title":"stackoverflow Line size of L1 and L2 caches"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-miss/","text":"Cache miss hazelcast What Is a Cache Miss?","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-miss/#cache#miss","text":"","title":"Cache miss"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-miss/#hazelcast#what#is#a#cache#miss","text":"","title":"hazelcast What Is a Cache Miss?"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/","text":"wikipedia Cache replacement policies","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/#wikipedia#cache#replacement#policies","text":"","title":"wikipedia Cache replacement policies"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/LRU-cache/","text":"LRU \u5173\u4e8e\u5b83\u7684\u539f\u7406\uff0c\u53c2\u89c1: 1\u3001labuladong \u7b97\u6cd5\u9898\u5c31\u50cf\u642d\u4e50\u9ad8\uff1a\u624b\u628a\u624b\u5e26\u4f60\u62c6\u89e3 LRU \u7b97\u6cd5","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/LRU-cache/#lru","text":"\u5173\u4e8e\u5b83\u7684\u539f\u7406\uff0c\u53c2\u89c1: 1\u3001labuladong \u7b97\u6cd5\u9898\u5c31\u50cf\u642d\u4e50\u9ad8\uff1a\u624b\u628a\u624b\u5e26\u4f60\u62c6\u89e3 LRU \u7b97\u6cd5","title":"LRU"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/LRU-cache/library-mohaps-lrucache/","text":"lrucache \u4e00\u3001\u770b\u4e86\u4e00\u4e0b\u4e24\u4e2alibrary\u7684\u5b9e\u73b0\uff0c\u539f\u7406\u548c labuladong \u7b97\u6cd5\u9898\u5c31\u50cf\u642d\u4e50\u9ad8\uff1a\u624b\u628a\u624b\u5e26\u4f60\u62c6\u89e3 LRU \u7b97\u6cd5 \u4e2d\u7684\u662f\u4e00\u81f4\u7684\uff0c\u5373hash map + double linked list \u4e8c\u3001\u4e0b\u9762\u7684\u4e24\u4e2a\u5b9e\u73b0\u7684\u5dee\u5f02\u5728\u4e8e 1\u3001 mohaps / lrucache \u4e2d\uff0c\u4f7f\u7528\u4e86custom doublelinked list\uff1b mohaps / lrucache11 \u4e2d\u4f7f\u7528\u7684\u662f std::list github mohaps / lrucache github mohaps / lrucache11","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/LRU-cache/library-mohaps-lrucache/#lrucache","text":"\u4e00\u3001\u770b\u4e86\u4e00\u4e0b\u4e24\u4e2alibrary\u7684\u5b9e\u73b0\uff0c\u539f\u7406\u548c labuladong \u7b97\u6cd5\u9898\u5c31\u50cf\u642d\u4e50\u9ad8\uff1a\u624b\u628a\u624b\u5e26\u4f60\u62c6\u89e3 LRU \u7b97\u6cd5 \u4e2d\u7684\u662f\u4e00\u81f4\u7684\uff0c\u5373hash map + double linked list \u4e8c\u3001\u4e0b\u9762\u7684\u4e24\u4e2a\u5b9e\u73b0\u7684\u5dee\u5f02\u5728\u4e8e 1\u3001 mohaps / lrucache \u4e2d\uff0c\u4f7f\u7528\u4e86custom doublelinked list\uff1b mohaps / lrucache11 \u4e2d\u4f7f\u7528\u7684\u662f std::list","title":"lrucache"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/LRU-cache/library-mohaps-lrucache/#github#mohapslrucache","text":"","title":"github mohaps/lrucache"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/Cache-replacement-policy/LRU-cache/library-mohaps-lrucache/#github#mohapslrucache11","text":"","title":"github mohaps/lrucache11"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/wikipedia-Cache-computing/","text":"Cache wikipedia Cache (computing) wikipedia CPU cache","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/wikipedia-Cache-computing/#cache","text":"","title":"Cache"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/wikipedia-Cache-computing/#wikipedia#cache#computing","text":"","title":"wikipedia Cache (computing)"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/wikipedia-Cache-computing/#wikipedia#cpu#cache","text":"","title":"wikipedia CPU cache"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE-cache/","text":"Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e cache","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E7%A7%91%E5%AD%A6%E5%87%BA%E7%89%88%E7%A4%BE-cache/#book--#cache","text":"","title":"Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e cache"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/","text":"Cache performance optimization How Cache Coherency Impacts Power Performance 1\u3001Cache Coherency\u662f\u6709\u4ee3\u4ef7\u7684\uff0c\u5b83\u4f1a\u5bf9Power\u3001Performance\u7b49\u4ea7\u751f\u91cd\u8981\u5f71\u54cd 2\u3001\u6211\u662f\u5728\u9605\u8bfb csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd \u65f6\uff0c\u8054\u60f3\u5230\u8fd9\u4e2atopic\u7684\uff0c\u5176\u4e2d\u8ba8\u8bba\u4e86cache coherency\u5bf9concurrency\u7684\u5f71\u54cd \u5176\u4e2d\u63d0\u53ca\u4e86\"cache coherence flood\"\u6982\u5ff5\u3002 3\u3001\u672c\u7ae0\u7684\u6807\u9898\u6e90\u81ea semiengineering How Cache Coherency Impacts Power, Performance semiengineering How Cache Coherency Impacts Power, Performance Cache performance optimization cache performance \u5f71\u54cd\u65b9\u65b9\u9762\u9762\uff0c\u65e0\u8bba\u662fconcurrency\u3001single thread\u7b49\uff0c\u5b83\u662foptimization\u91cd\u8981\u65b9\u5411\uff1b \u4e0b\u9762\u662f\u6211\u76ee\u524d\u9047\u5230\u8fc7\u7684\u4e00\u4e9boptimization\u65b9\u5f0f: \u4e00\u3001align-to-cache line-optimization \u4e8c\u3001\u907f\u514d \"cache sloshing-\u6643\u52a8\" \u5728 jemalloc\u7684paper A Scalable Concurrent malloc(3) Implementation for FreeBSD \u4e2d\uff0c\u5bf9\"cache sloshing-\u6643\u52a8\"\u7684\u89e3\u91ca\u4e3a: \"They attributed this to \u201ccache sloshing\u201d \u2013 the quick migration of cached data among processors during the manipulation of allocator data structures. \" \u5176\u5b9e\u5c31\u662f \"cache coherence flood-broadcast-bus traffic-interconnect contention-memory synchronization\u591a\u6838\u540c\u6b65\u5185\u5b58\"\u3002 \u5b9e\u73b0\u65b9\u5f0f: 1\u3001\u7ebf\u7a0b\u5b9a\u4f4d\u6765\u907f\u514d\"cache sloshing-\u6643\u52a8\" \u5178\u578b\u4ee3\u8868\u5c31\u662fjemalloc 2\u3001\u4f7f\u7528concurrency-friendly data structure \u53c2\u89c1 drdobbs Choose Concurrency-Friendly Data Structures \u4e09\u3001abseil B-tree Container-use array to cache optimization \u56db\u3001padding-to-cache line-optimization-avoid false sharing TODO How to improve my cache hit rate?","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/#cache#performance#optimization","text":"","title":"Cache performance optimization"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/#how#cache#coherency#impacts#power#performance","text":"1\u3001Cache Coherency\u662f\u6709\u4ee3\u4ef7\u7684\uff0c\u5b83\u4f1a\u5bf9Power\u3001Performance\u7b49\u4ea7\u751f\u91cd\u8981\u5f71\u54cd 2\u3001\u6211\u662f\u5728\u9605\u8bfb csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd \u65f6\uff0c\u8054\u60f3\u5230\u8fd9\u4e2atopic\u7684\uff0c\u5176\u4e2d\u8ba8\u8bba\u4e86cache coherency\u5bf9concurrency\u7684\u5f71\u54cd \u5176\u4e2d\u63d0\u53ca\u4e86\"cache coherence flood\"\u6982\u5ff5\u3002 3\u3001\u672c\u7ae0\u7684\u6807\u9898\u6e90\u81ea semiengineering How Cache Coherency Impacts Power, Performance","title":"How Cache Coherency Impacts Power Performance"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/#semiengineering#how#cache#coherency#impacts#power#performance","text":"","title":"semiengineering How Cache Coherency Impacts Power, Performance"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/#cache#performance#optimization_1","text":"cache performance \u5f71\u54cd\u65b9\u65b9\u9762\u9762\uff0c\u65e0\u8bba\u662fconcurrency\u3001single thread\u7b49\uff0c\u5b83\u662foptimization\u91cd\u8981\u65b9\u5411\uff1b \u4e0b\u9762\u662f\u6211\u76ee\u524d\u9047\u5230\u8fc7\u7684\u4e00\u4e9boptimization\u65b9\u5f0f: \u4e00\u3001align-to-cache line-optimization \u4e8c\u3001\u907f\u514d \"cache sloshing-\u6643\u52a8\" \u5728 jemalloc\u7684paper A Scalable Concurrent malloc(3) Implementation for FreeBSD \u4e2d\uff0c\u5bf9\"cache sloshing-\u6643\u52a8\"\u7684\u89e3\u91ca\u4e3a: \"They attributed this to \u201ccache sloshing\u201d \u2013 the quick migration of cached data among processors during the manipulation of allocator data structures. \" \u5176\u5b9e\u5c31\u662f \"cache coherence flood-broadcast-bus traffic-interconnect contention-memory synchronization\u591a\u6838\u540c\u6b65\u5185\u5b58\"\u3002 \u5b9e\u73b0\u65b9\u5f0f: 1\u3001\u7ebf\u7a0b\u5b9a\u4f4d\u6765\u907f\u514d\"cache sloshing-\u6643\u52a8\" \u5178\u578b\u4ee3\u8868\u5c31\u662fjemalloc 2\u3001\u4f7f\u7528concurrency-friendly data structure \u53c2\u89c1 drdobbs Choose Concurrency-Friendly Data Structures \u4e09\u3001abseil B-tree Container-use array to cache optimization \u56db\u3001padding-to-cache line-optimization-avoid false sharing","title":"Cache performance optimization"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/#todo","text":"How to improve my cache hit rate?","title":"TODO"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/Align-to-cache-line-optimization/","text":"Align to cache line optimization stackoverflow What does \u201ccacheline aligned\u201d mean? I read this article about PostgreSQL performance: http://akorotkov.github.io/blog/2016/05/09/scalability-towards-millions-tps/ One optimization was \"cacheline aligment\". What is this? How does it help and how to apply this in code? A CPU caches transfer data from and to main memory in chunks called cache lines ; a typical size for this seems to be 64 bytes. Data that are located closer to each other than this may end up on the same cache line. If these data are needed by different cores, the system has to work hard to keep the data consistent between the copies residing in the cores' caches. Essentially, while one thread modifies the data, the other thread is blocked by a lock from accessing the data. The article you reference talks about one such problem that was found in PostgreSQL in a data structure in shared memory that is frequently updated by different processes. By introducing padding into the structure to inflate it to 64 bytes, it is guaranteed that no two such data structures end up in the same cache line, and the processes that access them are not blocked more that absolutely necessary. This is only relevant if your program parallelizes execution and accesses a shared memory region, either by multithreading or by multiprocessing with shared memory. In this case you can benefit by making sure that data that are frequently accessed by different execution threads are not located close enough in memory that they can end up in the same cache line. The typical way to do that is by adding \u201cdead\u201d padding space at the end of a data structure. I found some interesting articles on the topic that you may want to read: http://www.drdobbs.com/parallel/maximize-locality-minimize-contention/208200273?pgno=3 http://www.drdobbs.com/tools/memory-constraints-on-thread-performance/231300494 http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206 drdobbs \u7cfb\u5217 http://www.drdobbs.com/parallel/maximize-locality-minimize-contention/208200273?pgno=3 http://www.drdobbs.com/tools/memory-constraints-on-thread-performance/231300494 http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206 \u4e0a\u8ff0\u6587\u7ae0\u73b0\u5728\u90fd\u65e0\u6cd5\u8bbf\u95ee\u4e86\u3002","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/Align-to-cache-line-optimization/#align#to#cache#line#optimization","text":"","title":"Align to cache line optimization"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/Align-to-cache-line-optimization/#stackoverflow#what#does#cacheline#aligned#mean","text":"I read this article about PostgreSQL performance: http://akorotkov.github.io/blog/2016/05/09/scalability-towards-millions-tps/ One optimization was \"cacheline aligment\". What is this? How does it help and how to apply this in code? A CPU caches transfer data from and to main memory in chunks called cache lines ; a typical size for this seems to be 64 bytes. Data that are located closer to each other than this may end up on the same cache line. If these data are needed by different cores, the system has to work hard to keep the data consistent between the copies residing in the cores' caches. Essentially, while one thread modifies the data, the other thread is blocked by a lock from accessing the data. The article you reference talks about one such problem that was found in PostgreSQL in a data structure in shared memory that is frequently updated by different processes. By introducing padding into the structure to inflate it to 64 bytes, it is guaranteed that no two such data structures end up in the same cache line, and the processes that access them are not blocked more that absolutely necessary. This is only relevant if your program parallelizes execution and accesses a shared memory region, either by multithreading or by multiprocessing with shared memory. In this case you can benefit by making sure that data that are frequently accessed by different execution threads are not located close enough in memory that they can end up in the same cache line. The typical way to do that is by adding \u201cdead\u201d padding space at the end of a data structure. I found some interesting articles on the topic that you may want to read: http://www.drdobbs.com/parallel/maximize-locality-minimize-contention/208200273?pgno=3 http://www.drdobbs.com/tools/memory-constraints-on-thread-performance/231300494 http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206","title":"stackoverflow What does \u201ccacheline aligned\u201d mean?"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/Align-to-cache-line-optimization/#drdobbs","text":"http://www.drdobbs.com/parallel/maximize-locality-minimize-contention/208200273?pgno=3 http://www.drdobbs.com/tools/memory-constraints-on-thread-performance/231300494 http://www.drdobbs.com/parallel/eliminate-false-sharing/217500206 \u4e0a\u8ff0\u6587\u7ae0\u73b0\u5728\u90fd\u65e0\u6cd5\u8bbf\u95ee\u4e86\u3002","title":"drdobbs \u7cfb\u5217"},{"location":"CPU-memory-access/CPU-cache-memory/Cache-performance-optimization/cnblogs-Geek_Ling-cache-performance-optimization/","text":"cnblogs \u5bf9\u7f13\u5b58\u7684\u601d\u8003\u2014\u2014\u63d0\u9ad8\u547d\u4e2d\u7387 cnblogs \u5bf9\u7f13\u5b58\u7684\u601d\u8003\u3010\u7eed\u3011\u2014\u2014\u7f16\u5199\u9ad8\u901f\u7f13\u5b58\u53cb\u597d\u4ee3\u7801 cnblogs \u5c40\u90e8\u6027\u539f\u7406\u6d45\u6790\u2014\u2014\u826f\u597d\u4ee3\u7801\u7684\u57fa\u672c\u7d20\u8d28","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u662f\u6839\u636eWikipedia Parallel computing \u4e2d\u7684\u603b\u7ed3\u65b9\u6cd5\u6765\u8fdb\u884c\u6574\u7406\u7684: Coordination - Memory coherency - Cache coherency \u5728CPU multiprocessing\u4e2d\uff0c\u975e\u5e38\u91cd\u8981\u7684\u4e00\u70b9\u662fcoordinate\u591a\u4e2acore/processor\u5bf9memory\u3001cache\u7684\u5229\u7528\uff0c\u5176\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2atopic\u662fcoherency: 1) Memory coherency 2) Cache coherency \u5b83\u4eec\u53ef\u4ee5\u4f7f\u7528 consistency model \u6765\u8fdb\u884c\u63cf\u8ff0\u3002","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/#_1","text":"\u672c\u7ae0\u662f\u6839\u636eWikipedia Parallel computing \u4e2d\u7684\u603b\u7ed3\u65b9\u6cd5\u6765\u8fdb\u884c\u6574\u7406\u7684: Coordination - Memory coherency - Cache coherency \u5728CPU multiprocessing\u4e2d\uff0c\u975e\u5e38\u91cd\u8981\u7684\u4e00\u70b9\u662fcoordinate\u591a\u4e2acore/processor\u5bf9memory\u3001cache\u7684\u5229\u7528\uff0c\u5176\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2atopic\u662fcoherency: 1) Memory coherency 2) Cache coherency \u5b83\u4eec\u53ef\u4ee5\u4f7f\u7528 consistency model \u6765\u8fdb\u884c\u63cf\u8ff0\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/","text":"Cache coherence Consistency model\u3001multiple model \u53ef\u4ee5\u4f7f\u7528consistency model\u6765\u8fdb\u884c\u5206\u6790\u3002 \u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u8fdb\u884c\u5206\u6790\u3002 \u8bb2\u8ff0cache coherence\u7684\u6587\u7ae0 1\u3001\"cppreference std::memory_order # Modification order\" \u6bb5 2\u3001stackoverflow Why it's termed read-modify-write but not read-write? # A 3\u3001zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a) \u8bb2\u5f97\u975e\u5e38\u597d 4\u3001\u5de5\u7a0bprogramming-language\u7684 aristeia-C++and-the-Perils-of-Double-Checked-Locking \u7ae0\u8282 5\u3001tutorialspoint Cache Coherence and Synchronization wikipedia Cache coherence In computer science , cache coherence is the consistency of shared resource data that ends up stored in multiple local caches . When clients in a system maintain caches of a common memory resource, problems may arise with inconsistent data, which is particularly the case with CPUs in a multiprocessing system. In the illustration on the right, consider both the clients have a cached copy of a particular memory block from a previous read. Suppose the client on the bottom updates/changes that memory block, the client on the top could be left with an invalid cache of memory without any notification of the change. Cache coherence is intended to manage such conflicts by maintaining a coherent view of the data values in multiple caches. NOTE: \u63cf\u8ff0\u7684\u662f\u4e00\u4e2a\u5178\u578b\u7684inconsistent \u7684\u573a\u666f Overview Cache coherence is the discipline which ensures that the changes in the values of shared operands (data) are propagated throughout the system in a timely fashion.[ 1] NOTE: \"propagate\"\u662f\u4e00\u4e2a\u975e\u5e38\u5f62\u8c61\u7684\u63cf\u8ff0 The following are the requirements for cache coherence:[ 2] NOTE:\u4e0b\u9762\u8fd9\u4e9brequirement\u662f\u4e3a\u4e86\u4fdd\u8bc1\u4ec0\u4e48\uff1f\u662f\u4e3a\u4e86\u4fdd\u8bc1coherence\uff1f Write Propagation Changes to the data in any cache must be propagated to other copies (of that cache line) in the peer caches. Transaction Serialization Reads/Writes to a single memory location must be seen by all processors in the same order. NOTE: \u4e00\u3001\u4e0b\u9762\u4f1a\u5bf9Transaction Serialization\u8fdb\u884c\u8bf4\u660e\uff0cTransaction Serialization\u662f\u975e\u5e38\u91cd\u8981\u7684\u5185\u5bb9 \u4e8c\u3001\u4e0a\u8ff0\u4e24\u4e2arequirement\u7684\u542b\u4e49\u662f\u4e0d\u540c\u7684\uff0c\u5b83\u4eec\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\uff0c\u5728\u4e0b\u9762\u7684\"Definition\"\u6bb5\u4f9d\u7136\u662f\u56f4\u7ed5\u5b83\u4eec\u800c\u5c55\u5f00 Theoretically, coherence can be performed at the load/store granularity . However, in practice it is generally performed at the granularity of cache blocks.[ 3] NOTE: unit Definition NOTE: \u4e00\u3001\u539f\u6587\u6bd4\u8f83\u957f\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u5173\u952e\u8bcd: consistency model sequential consistency memory model locality of reference \u4e8c\u3001\u539f\u6587\u5176\u5b9e\u662f\u4ece\"Overview\"\u6bb5\u7684\u63d0\u51fa\u7684\u4e24\u4e2arequirement\u5165\u624b\u6765\u8fdb\u884c\u5206\u6790\u7684 \u4e09\u3001\u539f\u6587\u8fd9\u4e00\u6bb5\u4e2d\uff0c\u82b1\u4e86\u5f88\u5927\u7bc7\u5e45\u6765\u8bba\u8ff0transaction serialization\uff0c\u5176\u5b9e\u4e4b\u524d\u5c31\u5df2\u7ecf\u63a5\u89e6\u8fc7transaction serialization\u4e86\uff0c\u53ea\u662f\u6ca1\u6709\u5b9a\u4e49\u8fd9\u6837\u7684\u6982\u5ff5 The above conditions satisfy the Write Propagation criteria required for cache coherence. However, they are not sufficient as they do not satisfy the Transaction Serialization condition. To illustrate this better, consider the following example: NOTE: \u4e00\u3001\u8fd9\u4e00\u6bb5\u662f\u627f\u4e0a\u542f\u4e0b\uff0c\u4ece\u6b64\u5c31\u5f00\u59cb\u8bba\u8ff0transaction serialization\uff0c\u7701\u7565\u4e86\u8fd9\u4e00\u6bb5\u540e\u9762\u7d27\u8ddf\u7684example\u3002 Therefore, in order to satisfy Transaction Serialization , and hence achieve Cache Coherence, the following condition along with the previous two mentioned in this section must be met: 1\u3001Writes to the same location must be sequenced. In other words, if location X received two different values A and B, in this order, from any two processors, the processors can never read location X as B and then read it as A. The location X must be seen with values A and B in that order.[ 5] NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u4ece\u5e95\u5c42\u89e3\u91ca\u4e86 \"write order to shared data different among different processor thread\"\uff0c\u5728\u5b9e\u9645CPU implementation\uff0c\u4e0a\u8ff0\u662f\u53ef\u80fd\u51fa\u73b0\u7684\uff0c\u53c2\u89c1 1\u3001aristeia C++ and the Perils of Double-Checked Locking \u5176\u4e2d\u5c31\u662f\u8be6\u7ec6\u8bf4\u660e\u4e86\u5982\u679c\u4e0d\u9075\u5faa Transaction Serialization \u7684\u5371\u5bb3 Via the definition of sequential consistency memory model The alternative definition of a coherent system is via the definition of sequential consistency memory model: \"the cache coherent system must appear to execute all threads\u2019 loads and stores to a single memory location in a total order that respects the program order of each thread\".[ 3] Thus, the only difference between the cache coherent system and sequentially consistent system is in the number of address locations the definition talks about (single memory location for a cache coherent system, and all memory locations for a sequentially consistent system). NOTE: 1\u3001\u4e24\u4e2adefinition\u7684\u672c\u8d28\u662f\u76f8\u540c\u7684\uff0c\u53ea\u662f\u8ba8\u8bba\u7684\u662f\u4e0d\u540c\u8303\u56f4\u7684\u95ee\u9898 \u4e0d\u9075\u5faatransaction serialization Rarely, but especially in algorithms, coherence can instead refer to the locality of reference . Multiple copies of same data can exist in different cache simultaneously and if processors are allowed to update their own copies freely, an inconsistent view of memory can result. NOTE: 1\u3001\u4e0a\u8ff0\"inconsistent view\"\u662f\u975e\u5e38\u597d\u7684\u603b\u7ed3 Coherency mechanisms NOTE: Snooping Main article: Bus snooping NOTE: 1\u3001\"snoop\"\u7684\u610f\u601d\"\u7aa5\u63a2\" 2\u3001\u8fd9\u79cd\u662f\u89c1\u5f97\u6bd4\u8f83\u591a\u4e00\u4e9b\u7684 First introduced in 1983,[ 7] snooping is a process where the individual caches monitor address lines for accesses to memory locations that they have cached.[ 4] The write-invalidate protocols and write-update protocols make use of this mechanism. Directory-based Main article: Directory-based cache coherence In a directory-based system , the data being shared is placed in a common directory that maintains the coherence between caches. The directory acts as a filter through which the processor must ask permission to load an entry from the primary memory to its cache. When an entry is changed, the directory either updates or invalidates the other caches with that entry. NOTE: \u5c06\u6240\u6709\u88abshared\u7684\u6570\u636e\u90fd\u653e\u5728\u4e00\u4e2a\u5171\u4eab\u7684common directory\u4e2d\uff0c\u7531\u5b83\u6765\u5b9e\u73b0coherence between caches\u3002processor\u9700\u8981\u7531\u5b83\u7684permission\u624d\u80fd\u591fcache\u4e00\u4e2a\u6570\u636e\u3002 Distributed shared memory systems mimic these mechanisms in an attempt to maintain consistency between blocks of memory in loosely coupled systems.[ 9] NOTE: \u540c\u4e00\u601d\u60f3\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528 Coherency protocols NOTE: 1\u3001\u8fd9\u90e8\u5206\u5185\u5bb9\u653e\u5230\u4e86 ./Protocol \u7ae0\u8282\u3002 How Cache Coherency Impacts Power Performance \u53c2\u89c1 Cache-performance-optimization \u7ae0\u8282\u3002","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#cache#coherence","text":"","title":"Cache coherence"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#consistency#modelmultiple#model","text":"\u53ef\u4ee5\u4f7f\u7528consistency model\u6765\u8fdb\u884c\u5206\u6790\u3002 \u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u8fdb\u884c\u5206\u6790\u3002","title":"Consistency model\u3001multiple model"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#cache#coherence_1","text":"1\u3001\"cppreference std::memory_order # Modification order\" \u6bb5 2\u3001stackoverflow Why it's termed read-modify-write but not read-write? # A 3\u3001zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a) \u8bb2\u5f97\u975e\u5e38\u597d 4\u3001\u5de5\u7a0bprogramming-language\u7684 aristeia-C++and-the-Perils-of-Double-Checked-Locking \u7ae0\u8282 5\u3001tutorialspoint Cache Coherence and Synchronization","title":"\u8bb2\u8ff0cache coherence\u7684\u6587\u7ae0"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#wikipedia#cache#coherence","text":"In computer science , cache coherence is the consistency of shared resource data that ends up stored in multiple local caches . When clients in a system maintain caches of a common memory resource, problems may arise with inconsistent data, which is particularly the case with CPUs in a multiprocessing system. In the illustration on the right, consider both the clients have a cached copy of a particular memory block from a previous read. Suppose the client on the bottom updates/changes that memory block, the client on the top could be left with an invalid cache of memory without any notification of the change. Cache coherence is intended to manage such conflicts by maintaining a coherent view of the data values in multiple caches. NOTE: \u63cf\u8ff0\u7684\u662f\u4e00\u4e2a\u5178\u578b\u7684inconsistent \u7684\u573a\u666f","title":"wikipedia Cache coherence"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#overview","text":"Cache coherence is the discipline which ensures that the changes in the values of shared operands (data) are propagated throughout the system in a timely fashion.[ 1] NOTE: \"propagate\"\u662f\u4e00\u4e2a\u975e\u5e38\u5f62\u8c61\u7684\u63cf\u8ff0 The following are the requirements for cache coherence:[ 2] NOTE:\u4e0b\u9762\u8fd9\u4e9brequirement\u662f\u4e3a\u4e86\u4fdd\u8bc1\u4ec0\u4e48\uff1f\u662f\u4e3a\u4e86\u4fdd\u8bc1coherence\uff1f","title":"Overview"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#write#propagation","text":"Changes to the data in any cache must be propagated to other copies (of that cache line) in the peer caches.","title":"Write Propagation"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#transaction#serialization","text":"Reads/Writes to a single memory location must be seen by all processors in the same order. NOTE: \u4e00\u3001\u4e0b\u9762\u4f1a\u5bf9Transaction Serialization\u8fdb\u884c\u8bf4\u660e\uff0cTransaction Serialization\u662f\u975e\u5e38\u91cd\u8981\u7684\u5185\u5bb9 \u4e8c\u3001\u4e0a\u8ff0\u4e24\u4e2arequirement\u7684\u542b\u4e49\u662f\u4e0d\u540c\u7684\uff0c\u5b83\u4eec\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\uff0c\u5728\u4e0b\u9762\u7684\"Definition\"\u6bb5\u4f9d\u7136\u662f\u56f4\u7ed5\u5b83\u4eec\u800c\u5c55\u5f00 Theoretically, coherence can be performed at the load/store granularity . However, in practice it is generally performed at the granularity of cache blocks.[ 3] NOTE: unit","title":"Transaction Serialization"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#definition","text":"NOTE: \u4e00\u3001\u539f\u6587\u6bd4\u8f83\u957f\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u5173\u952e\u8bcd: consistency model sequential consistency memory model locality of reference \u4e8c\u3001\u539f\u6587\u5176\u5b9e\u662f\u4ece\"Overview\"\u6bb5\u7684\u63d0\u51fa\u7684\u4e24\u4e2arequirement\u5165\u624b\u6765\u8fdb\u884c\u5206\u6790\u7684 \u4e09\u3001\u539f\u6587\u8fd9\u4e00\u6bb5\u4e2d\uff0c\u82b1\u4e86\u5f88\u5927\u7bc7\u5e45\u6765\u8bba\u8ff0transaction serialization\uff0c\u5176\u5b9e\u4e4b\u524d\u5c31\u5df2\u7ecf\u63a5\u89e6\u8fc7transaction serialization\u4e86\uff0c\u53ea\u662f\u6ca1\u6709\u5b9a\u4e49\u8fd9\u6837\u7684\u6982\u5ff5 The above conditions satisfy the Write Propagation criteria required for cache coherence. However, they are not sufficient as they do not satisfy the Transaction Serialization condition. To illustrate this better, consider the following example: NOTE: \u4e00\u3001\u8fd9\u4e00\u6bb5\u662f\u627f\u4e0a\u542f\u4e0b\uff0c\u4ece\u6b64\u5c31\u5f00\u59cb\u8bba\u8ff0transaction serialization\uff0c\u7701\u7565\u4e86\u8fd9\u4e00\u6bb5\u540e\u9762\u7d27\u8ddf\u7684example\u3002 Therefore, in order to satisfy Transaction Serialization , and hence achieve Cache Coherence, the following condition along with the previous two mentioned in this section must be met: 1\u3001Writes to the same location must be sequenced. In other words, if location X received two different values A and B, in this order, from any two processors, the processors can never read location X as B and then read it as A. The location X must be seen with values A and B in that order.[ 5] NOTE: \u4e00\u3001\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u4ece\u5e95\u5c42\u89e3\u91ca\u4e86 \"write order to shared data different among different processor thread\"\uff0c\u5728\u5b9e\u9645CPU implementation\uff0c\u4e0a\u8ff0\u662f\u53ef\u80fd\u51fa\u73b0\u7684\uff0c\u53c2\u89c1 1\u3001aristeia C++ and the Perils of Double-Checked Locking \u5176\u4e2d\u5c31\u662f\u8be6\u7ec6\u8bf4\u660e\u4e86\u5982\u679c\u4e0d\u9075\u5faa Transaction Serialization \u7684\u5371\u5bb3","title":"Definition"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#via#the#definition#of#sequential#consistency#memory#model","text":"The alternative definition of a coherent system is via the definition of sequential consistency memory model: \"the cache coherent system must appear to execute all threads\u2019 loads and stores to a single memory location in a total order that respects the program order of each thread\".[ 3] Thus, the only difference between the cache coherent system and sequentially consistent system is in the number of address locations the definition talks about (single memory location for a cache coherent system, and all memory locations for a sequentially consistent system). NOTE: 1\u3001\u4e24\u4e2adefinition\u7684\u672c\u8d28\u662f\u76f8\u540c\u7684\uff0c\u53ea\u662f\u8ba8\u8bba\u7684\u662f\u4e0d\u540c\u8303\u56f4\u7684\u95ee\u9898","title":"Via the definition of sequential consistency memory model"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#transaction#serialization_1","text":"Rarely, but especially in algorithms, coherence can instead refer to the locality of reference . Multiple copies of same data can exist in different cache simultaneously and if processors are allowed to update their own copies freely, an inconsistent view of memory can result. NOTE: 1\u3001\u4e0a\u8ff0\"inconsistent view\"\u662f\u975e\u5e38\u597d\u7684\u603b\u7ed3","title":"\u4e0d\u9075\u5faatransaction serialization"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#coherency#mechanisms","text":"NOTE:","title":"Coherency mechanisms"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#snooping","text":"Main article: Bus snooping NOTE: 1\u3001\"snoop\"\u7684\u610f\u601d\"\u7aa5\u63a2\" 2\u3001\u8fd9\u79cd\u662f\u89c1\u5f97\u6bd4\u8f83\u591a\u4e00\u4e9b\u7684 First introduced in 1983,[ 7] snooping is a process where the individual caches monitor address lines for accesses to memory locations that they have cached.[ 4] The write-invalidate protocols and write-update protocols make use of this mechanism.","title":"Snooping"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#directory-based","text":"Main article: Directory-based cache coherence In a directory-based system , the data being shared is placed in a common directory that maintains the coherence between caches. The directory acts as a filter through which the processor must ask permission to load an entry from the primary memory to its cache. When an entry is changed, the directory either updates or invalidates the other caches with that entry. NOTE: \u5c06\u6240\u6709\u88abshared\u7684\u6570\u636e\u90fd\u653e\u5728\u4e00\u4e2a\u5171\u4eab\u7684common directory\u4e2d\uff0c\u7531\u5b83\u6765\u5b9e\u73b0coherence between caches\u3002processor\u9700\u8981\u7531\u5b83\u7684permission\u624d\u80fd\u591fcache\u4e00\u4e2a\u6570\u636e\u3002 Distributed shared memory systems mimic these mechanisms in an attempt to maintain consistency between blocks of memory in loosely coupled systems.[ 9] NOTE: \u540c\u4e00\u601d\u60f3\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528","title":"Directory-based"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#coherency#protocols","text":"NOTE: 1\u3001\u8fd9\u90e8\u5206\u5185\u5bb9\u653e\u5230\u4e86 ./Protocol \u7ae0\u8282\u3002","title":"Coherency protocols"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/#how#cache#coherency#impacts#power#performance","text":"\u53c2\u89c1 Cache-performance-optimization \u7ae0\u8282\u3002","title":"How Cache Coherency Impacts Power Performance"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/","text":"\u5173\u4e8e\u672c\u7ae0 wikipedia Cache coherence # Coherency protocols Distributed shared memory systems employ coherency protocols to ensure the consistency between all caches, by maintaining memory coherence according to a specific consistency model . Older multiprocessors support the sequential consistency model, while modern shared memory systems typically support the release consistency or weak consistency models. The protocol must implement the basic requirements for coherence. It can be tailor-made for the target system or application. Protocols can also be classified as snoopy or directory-based . Typically, early systems used directory-based protocols where a directory would keep a track of the data being shared and the sharers. In snoopy protocols , the transaction requests (to read, write, or upgrade) are sent out to all processors. All processors snoop the request and respond appropriately. Write propagation in snoopy protocols can be implemented by either of the following methods: NOTE: \u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u5f0f Write-invalidate When a write operation is observed to a location that a cache has a copy of, the cache controller invalidates its own copy of the snooped memory location, which forces a read from main memory of the new value on its next access.[ 4] NOTE: 1\u3001Cache invalidation 2\u3001\u8fd9\u79cd\u65b9\u5f0f\u4e5f\u53eb\u505a write back (\u56de\u5199) Write-update When a write operation is observed to a location that a cache has a copy of, the cache controller updates its own copy of the snooped memory location with the new data. NOTE: 1\u3001\u8fd9\u79cd\u65b9\u5f0f\u4e5f\u53eb\u505a write-through (\u76f4\u5199) If the protocol design states that whenever any copy of the shared data is changed, all the other copies must be \"updated\" to reflect the change, then it is a write-update protocol . If the design states that a write to a cached copy by any processor requires other processors to discard or invalidate their cached copies, then it is a write-invalidate protocol . However, scalability is one shortcoming of broadcast protocols. tau Multiprocessor Programming # Lecture 7: Spin Locks and Contention Management NOTE: 1\u3001\u5176\u4e2d\u7684\"7.3 Cache Memory & Consistency\"\u4e2d\u8ba8\u8bba\u4e86write back (\u56de\u5199)\u3001write-through (\u76f4\u5199) \u5404\u79cd cache coherency protocol \u672c\u7ae0\u8ba8\u8bba\u5404\u79cd\u4e00\u4e9b cache coherency protocol\uff1a protocols states \u7ae0\u8282 MSI protocol **M**odified\u3001**S**hared\u3001**I**nvalid MESI protocol **M**odified\u3001**E**xclusive\u3001**S**hared\u3001**I**nvalid MOESI protocol **M**odified\u3001**O**wned\u3001**E**xclusive\u3001**S**hared\u3001**I**nvalid \u53ef\u4ee5\u770b\u5230\uff0c\u8fd9\u4e9bprotocol\u7684\u5dee\u5f02\u4e3b\u8981\u5728\u4e8estate\u6570\u7684\u4e0d\u540c\u3002 TODO \u4e0b\u9762\u662f\u4e00\u4e9b\u975e\u5e38\u597d\u7684\u6587\u7ae0: shahriar.svbtle Understanding write-through, write-around and write-back caching (with Python) stackoverflow Write-back vs Write-Through caching?","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/#_1","text":"","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/#wikipedia#cache#coherence#coherency#protocols","text":"Distributed shared memory systems employ coherency protocols to ensure the consistency between all caches, by maintaining memory coherence according to a specific consistency model . Older multiprocessors support the sequential consistency model, while modern shared memory systems typically support the release consistency or weak consistency models. The protocol must implement the basic requirements for coherence. It can be tailor-made for the target system or application. Protocols can also be classified as snoopy or directory-based . Typically, early systems used directory-based protocols where a directory would keep a track of the data being shared and the sharers. In snoopy protocols , the transaction requests (to read, write, or upgrade) are sent out to all processors. All processors snoop the request and respond appropriately. Write propagation in snoopy protocols can be implemented by either of the following methods: NOTE: \u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u5f0f","title":"wikipedia Cache coherence # Coherency protocols"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/#write-invalidate","text":"When a write operation is observed to a location that a cache has a copy of, the cache controller invalidates its own copy of the snooped memory location, which forces a read from main memory of the new value on its next access.[ 4] NOTE: 1\u3001Cache invalidation 2\u3001\u8fd9\u79cd\u65b9\u5f0f\u4e5f\u53eb\u505a write back (\u56de\u5199)","title":"Write-invalidate"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/#write-update","text":"When a write operation is observed to a location that a cache has a copy of, the cache controller updates its own copy of the snooped memory location with the new data. NOTE: 1\u3001\u8fd9\u79cd\u65b9\u5f0f\u4e5f\u53eb\u505a write-through (\u76f4\u5199) If the protocol design states that whenever any copy of the shared data is changed, all the other copies must be \"updated\" to reflect the change, then it is a write-update protocol . If the design states that a write to a cached copy by any processor requires other processors to discard or invalidate their cached copies, then it is a write-invalidate protocol . However, scalability is one shortcoming of broadcast protocols.","title":"Write-update"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/#tau#multiprocessor#programming#lecture#7#spin#locks#and#contention#management","text":"NOTE: 1\u3001\u5176\u4e2d\u7684\"7.3 Cache Memory & Consistency\"\u4e2d\u8ba8\u8bba\u4e86write back (\u56de\u5199)\u3001write-through (\u76f4\u5199)","title":"tau Multiprocessor Programming # Lecture 7: Spin Locks and Contention Management"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/#cache#coherency#protocol","text":"\u672c\u7ae0\u8ba8\u8bba\u5404\u79cd\u4e00\u4e9b cache coherency protocol\uff1a protocols states \u7ae0\u8282 MSI protocol **M**odified\u3001**S**hared\u3001**I**nvalid MESI protocol **M**odified\u3001**E**xclusive\u3001**S**hared\u3001**I**nvalid MOESI protocol **M**odified\u3001**O**wned\u3001**E**xclusive\u3001**S**hared\u3001**I**nvalid \u53ef\u4ee5\u770b\u5230\uff0c\u8fd9\u4e9bprotocol\u7684\u5dee\u5f02\u4e3b\u8981\u5728\u4e8estate\u6570\u7684\u4e0d\u540c\u3002","title":"\u5404\u79cd cache coherency protocol"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/#todo","text":"\u4e0b\u9762\u662f\u4e00\u4e9b\u975e\u5e38\u597d\u7684\u6587\u7ae0: shahriar.svbtle Understanding write-through, write-around and write-back caching (with Python) stackoverflow Write-back vs Write-Through caching?","title":"TODO"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/Cache-invalidation/","text":"Cache invalidation Wikipedia Cache invalidation","title":"Cache-invalidation"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/Cache-invalidation/#cache#invalidation","text":"","title":"Cache invalidation"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/Cache-invalidation/#wikipedia#cache#invalidation","text":"","title":"Wikipedia Cache invalidation"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/MOESI-protocol/","text":"MOESI protocol \u662f\u5728\u9605\u8bfb cppreference std::memory_order \u65f6\uff0c\u5176\u4e2d\u7684 \"External links\" \u7ae0\u8282\uff0c\u7ed9\u51fa\u4e86 MOESI protocol \u3002 wikipedia MOESI protocol","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/MOESI-protocol/#moesi#protocol","text":"\u662f\u5728\u9605\u8bfb cppreference std::memory_order \u65f6\uff0c\u5176\u4e2d\u7684 \"External links\" \u7ae0\u8282\uff0c\u7ed9\u51fa\u4e86 MOESI protocol \u3002","title":"MOESI protocol"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/MOESI-protocol/#wikipedia#moesi#protocol","text":"","title":"wikipedia MOESI protocol"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/MSI-protocol/","text":"MSI protocol Wikipedia MSI protocol","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/MSI-protocol/#msi#protocol","text":"","title":"MSI protocol"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Cache-coherence/Protocol/MSI-protocol/#wikipedia#msi#protocol","text":"","title":"Wikipedia MSI protocol"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Memory-coherence/","text":"Memory coherence 1\u3001\"coherence\"\u7684\u610f\u601d\u662f\"\u4e00\u81f4\u6027\"\u3002 2\u3001\u8bb2\u5f97\u5176\u5b9e\u5c31\u662fcache coherence Wikipedia Memory coherence Memory coherence is an issue that affects the design of computer systems in which two or more processors or cores share a common area of memory .[ 1] [ 2] [ 3] [ 4] NOTE: \u4eceone( uniprocessor ) to many( multiprocessor (or multicore ) )\uff0c\u590d\u6742\u5ea6\u63d0\u5347\u4e86\u975e\u5e38\u591a\uff0c\u56e0\u6b64\u9700\u8981\u76f8\u5e94\u7684\u7406\u8bba\u6765\u5bf9\u8fd9\u79cd\u60c5\u51b5\u8fdb\u884c\u8bf4\u660e\u3002 In a uniprocessor system (whereby, in today's terms, there exists only one core), there is only one processing element doing all the work and therefore only one processing element that can read or write from/to a given memory location. As a result, when a value is changed, all subsequent read operations of the corresponding memory location will see the updated value, even if it is cached . NOTE: uniprocessor \u662f\u975e\u5e38\u7b80\u5355\u7684\uff1b Conversely, in multiprocessor (or multicore ) systems, there are two or more processing elements (\u5bf9\u5e94\u7684\u662fmultiple model\u4e2d\u7684entity) working at the same time, and so it is possible that they simultaneously access the same memory location. Provided none of them changes the data in this location, they can share it indefinitely and cache it as they please. But as soon as one updates the location, the others might work on an out-of-date copy that, e.g., resides in their local cache. Consequently, some scheme is required to notify all the processing elements of changes to shared values ; such a scheme is known as a \" memory coherence protocol \", and if such a protocol is employed the system is said to have a \" coherent memory \". NOTE: \u4f7f\u7528multiple model\u8fdb\u884c\u63cf\u8ff0\u3002 The exact nature and meaning of the memory coherency is determined by the consistency model that the coherence protocol implements. In order to write correct concurrent programs, programmers must be aware of the exact consistency model that is employed by their systems. NOTE: \u53ef\u4ee5\u4f7f\u7528consistency model\u6765\u63cf\u8ff0\u8fd9\u79cd\u95ee\u9898 When implemented in hardware, the coherency protocol can, e.g., be directory based or employ snooping (a.k.a. \"sniffing\"). Examples of specific protocols are the MSI protocol and its derivatives MESI , MOSI and MOESI .","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Memory-coherence/#memory#coherence","text":"1\u3001\"coherence\"\u7684\u610f\u601d\u662f\"\u4e00\u81f4\u6027\"\u3002 2\u3001\u8bb2\u5f97\u5176\u5b9e\u5c31\u662fcache coherence","title":"Memory coherence"},{"location":"CPU-memory-access/CPU-cache-memory/Coordination/Memory-coherence/#wikipedia#memory#coherence","text":"Memory coherence is an issue that affects the design of computer systems in which two or more processors or cores share a common area of memory .[ 1] [ 2] [ 3] [ 4] NOTE: \u4eceone( uniprocessor ) to many( multiprocessor (or multicore ) )\uff0c\u590d\u6742\u5ea6\u63d0\u5347\u4e86\u975e\u5e38\u591a\uff0c\u56e0\u6b64\u9700\u8981\u76f8\u5e94\u7684\u7406\u8bba\u6765\u5bf9\u8fd9\u79cd\u60c5\u51b5\u8fdb\u884c\u8bf4\u660e\u3002 In a uniprocessor system (whereby, in today's terms, there exists only one core), there is only one processing element doing all the work and therefore only one processing element that can read or write from/to a given memory location. As a result, when a value is changed, all subsequent read operations of the corresponding memory location will see the updated value, even if it is cached . NOTE: uniprocessor \u662f\u975e\u5e38\u7b80\u5355\u7684\uff1b Conversely, in multiprocessor (or multicore ) systems, there are two or more processing elements (\u5bf9\u5e94\u7684\u662fmultiple model\u4e2d\u7684entity) working at the same time, and so it is possible that they simultaneously access the same memory location. Provided none of them changes the data in this location, they can share it indefinitely and cache it as they please. But as soon as one updates the location, the others might work on an out-of-date copy that, e.g., resides in their local cache. Consequently, some scheme is required to notify all the processing elements of changes to shared values ; such a scheme is known as a \" memory coherence protocol \", and if such a protocol is employed the system is said to have a \" coherent memory \". NOTE: \u4f7f\u7528multiple model\u8fdb\u884c\u63cf\u8ff0\u3002 The exact nature and meaning of the memory coherency is determined by the consistency model that the coherence protocol implements. In order to write correct concurrent programs, programmers must be aware of the exact consistency model that is employed by their systems. NOTE: \u53ef\u4ee5\u4f7f\u7528consistency model\u6765\u63cf\u8ff0\u8fd9\u79cd\u95ee\u9898 When implemented in hardware, the coherency protocol can, e.g., be directory based or employ snooping (a.k.a. \"sniffing\"). Examples of specific protocols are the MSI protocol and its derivatives MESI , MOSI and MOESI .","title":"Wikipedia Memory coherence"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/","text":"False sharing \u662f\u5728\u9605\u8bfb zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a) \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86false sharing\uff0c\u5e76\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a) cacheline\u5bfc\u81f4\u7684false sharing struct test { uint32_t a ; uint32_t b ; } test t ; t \u662f\u4e00\u4e2a\u5171\u4eab\u53d8\u91cf\uff0ccore1\u4e0a\u7ebf\u7a0bt1\u8bbf\u95ee t.a \uff0c core2\u7ebf\u7a0bt2\u8bbf\u95ee t.b ,\u5982\u679ccacheline\u5927\u5c0f\u662f16Bytes\uff0c\u90a3\u4e48\u5f53t1\u8bbf\u95ee t.a \u65f6cache miss\uff0c\u7136\u540e\u4f1a\u5c06\u8fd9\u4e2a t.a \u8bfb\u53d6\u5230cache\u4e2d\uff0c\u4f46\u662f\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0ccore\u6bcf\u6b21\u8bfb\u53d6\u7684\u957f\u5ea6\u662fcacheline\u7684\u957f\u5ea6\uff0c t.a \u957f\u5ea6\u662f4\uff0c t.b \u957f\u5ea6\u662f4\uff0c\u6240\u4ee5\u5728\u4e00\u6b21\u8bfb\u53d6\u4e2d\u4f1a\u5c06 t.a \u548c t.b \u4e00\u6b21\u6027\u8bfb\u5230\u4e86\u81ea\u5df1cache\u4e2d\u4e86\uff0c\u90a3\u4e48t2\u4e5f\u540c\u6837\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\uff0c\u5982\u679ct1\u4fee\u6539\u5f53\u524dcacheline\u4e2d\u7684\u503c\uff0c\u4ed6\u9700\u8981\u4e0ecore2\u5bf9\u5e94\u7684cacheline\u540c\u6b65\uff0c\u90a3\u4e48\u672c\u6765\u4e24\u4e2a\u53ef\u4ee5\u5e76\u53d1\u7684memory location\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u3002\u5bf9\u6027\u80fd\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u5f71\u54cd\u3002 \u90a3\u4e48\u4e3a\u4e86\u907f\u514d**false sharing**\uff0c\u5c31\u9700\u8981\u8ba9\u8fd9\u4e24\u4e2amemory location\u8fdb\u884ccacheline\u5bf9\u9f50\u3002C++ \u63d0\u4f9b\u4e86**alignas**\u7684\u65b9\u6cd5\u3002 dzone False Sharing Memory is stored within the cache system in units know as cache lines . Cache lines are a power of 2 of contiguous bytes which are typically 32-256 in size. The most common cache line size is 64 bytes. False sharing is a term which applies when threads unwittingly(\u4e0d\u77e5\u4e0d\u89c9\u5730) impact the performance of each other while modifying independent variables sharing the same cache line . Write contention on cache lines is the single most limiting factor on achieving scalability for parallel threads of execution in an SMP system. I\u2019ve heard false sharing described as the silent performance killer because it is far from obvious when looking at code. NOTE: \u8fd9\u6bb5\u8bdd\u975e\u5e38\u5177\u6709\u542f\u793a\u4f5c\u7528 To achieve linear scalability with number of threads NOTE: \u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u597d 1\u3001\u5982\u4f55\u4fdd\u8bc1\"no two threads write to the same variable or cache line \"? \u4fdd\u8bc1\"no two threads write to the same variable\"\u662f\u76f8\u5bf9\u5bb9\u6613\u7684\uff0c\u4f46\u662f\u540e\u8005\u7684\u5b9e\u73b0\u5219\u76f8\u5bf9\u9ebb\u70e6\u4e00\u4e9b\u3002\u5176\u5b9e\u8fd9\u5c31\u662f\u5982\u4f55detect false sharing\u3002 To achieve linear scalability with number of threads, we must ensure no two threads write to the same variable or cache line . Two threads writing to the same variable can be tracked down at a code level. To be able to know if independent variables share the same cache line we need to know the memory layout , or we can get a tool to tell us. Intel VTune is such a profiling tool. In this article I\u2019ll explain how memory is laid out for Java objects and how we can pad(\u8865\u9f50) out our cache lines to avoid false sharing. NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u7684\u7ffb\u8bd1\u5982\u4e0b: \"\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5c06\u89e3\u91ca\u5982\u4f55\u4e3aJava\u5bf9\u8c61\u5e03\u5c40\u5185\u5b58\uff0c\u4ee5\u53ca\u5982\u4f55\u6269\u5c55\u7f13\u5b58\u7ebf\u4ee5\u907f\u514d\u9519\u8bef\u5171\u4eab\u3002\" Figure 1. above illustrates the issue of false sharing . A thread running on core 1 wants to update variable X while a thread on core 2 wants to update variable Y . Unfortunately these two hot variables reside in the same cache line . Each thread will race for ownership of the cache line so they can update it. If core 1 gets ownership then the cache sub-system will need to invalidate the corresponding cache line for core 2. When Core 2 gets ownership and performs its update, then core 1 will be told to invalidate its copy of the cache line. This will ping pong back and forth via the L3 cache greatly impacting performance. The issue would be further exacerbated(\u52a0\u91cd) if competing cores are on different sockets and additionally have to cross the socket interconnect. Java Memory Layout For the Hotspot JVM, all objects have a 2-word header. First is the \u201cmark\u201d word which is made up of 24-bits for the hash code and 8-bits for flags such as the lock state , or it can be swapped for lock objects. The second is a reference to the class of the object. Arrays have an additional word for the size of the array. Every object is aligned to an 8-byte granularity boundary for performance. Therefore to be efficient when packing, the object fields are re-ordered from declaration order to the following order based on size in bytes: 1\u3001doubles (8) and longs (8) 2\u3001ints (4) and floats (4) 3\u3001shorts (2) and chars (2) 4\u3001booleans (1) and bytes (1) 5\u3001references (4/8) With this knowledge we can pad a cache line between any fields with 7 longs. Within the Disruptor we pad cache lines around the RingBuffer cursor and BatchEventProcessor sequences. To show the performance impact let\u2019s take a few threads each updating their own independent counters. These counters will be *volatile long*s so the world can see their progress. public final class FalseSharing implements Runnable { public final static int NUM_THREADS = 4 ; // change public final static long ITERATIONS = 500L * 1000L * 1000L ; private final int arrayIndex ; private static VolatileLong [] longs = new VolatileLong [ NUM_THREADS ]; static { for ( int i = 0 ; i < longs . length ; i ++ ) { longs [ i ] = new VolatileLong (); } } public FalseSharing ( final int arrayIndex ) { this . arrayIndex = arrayIndex ; } public static void main ( final String [] args ) throws Exception { final long start = System . nanoTime (); runTest (); System . out . println ( \"duration = \" + ( System . nanoTime () - start )); } private static void runTest () throws InterruptedException { Thread [] threads = new Thread [ NUM_THREADS ]; for ( int i = 0 ; i < threads . length ; i ++ ) { threads [ i ] = new Thread ( new FalseSharing ( i )); } for ( Thread t : threads ) { t . start (); } for ( Thread t : threads ) { t . join (); } } public void run () { long i = ITERATIONS + 1 ; while ( 0 != -- i ) { longs [ arrayIndex ]. value = i ; } } public final static class VolatileLong { public volatile long value = 0L ; public long p1 , p2 , p3 , p4 , p5 , p6 ; // comment out } } Results Running the above code while ramping(\u63d0\u5347) the number of threads and adding/removing the cache line padding , I get the results depicted in Figure 2. below. This is measuring the duration of test runs on my 4-core Nehalem at home. Figure 2. The impact of false sharing can clearly be seen by the increased execution time required to complete the test. Without the cache line contention we achieve near linear scale up with threads. This is not a perfect test because we cannot be sure where the VolatileLongs will be laid out in memory. They are independent objects. However experience shows that objects allocated at the same time tend to be co-located. So there you have it. False sharing can be a silent performance killer. ustc.edu False Sharing False sharing is a common problem in shared memory parallel processing . It occurs when two or more cores hold a copy of the same memory cache line . NOTE: 1\u3001\u603b\u7ed3\u5f97\u6bd4\u8f83\u597d If one core writes, the cache line holding the memory line is invalidated on other cores. Even though another core may not be using that data (reading or writing), it may be using another element of data on the same cache line. The second core will need to reload the line before it can access its own data again. Detect false sharing The cache hardware ensures data coherency , but at a potentially high performance cost if false sharing is frequent. A good technique to identify false sharing problems is to catch unexpected sharp increases in last-level cache misses using hardware counters or other performance tools. NOTE: 1\u3001\u8fd9\u6bb5\u8bdd\u8ba9\u6211\u60f3\u5230\u4e86cache Miss \u548c false sharing\u7684\u5173\u8054 Simple example As a simple example, consider a spawned function with a for loop that increments array values. The array is volatile to force the compiler to generate store instructions rather than hold values in registers or optimize the loop. NOTE: 1\u3001\u6b64\u5904\u5bf9 volatile \u7684\u89e3\u91ca\u3001\u4f7f\u7528\u662f\u975e\u5e38\u597d\u7684 volatile int x [ 32 ]; void f ( volatile int * p ) { for ( int i = 0 ; i < 100000000 ; i ++ ) { ++ p [ 0 ]; ++ p [ 16 ]; } } int main () { cilk_spawn f ( & x [ 0 ]); cilk_spawn f ( & x [ 1 ]); cilk_spawn f ( & x [ 2 ]); cilk_spawn f ( & x [ 3 ]); cilk_sync ; return 0 ; } NOTE: 1\u3001\u4e0a\u8ff0\u4f8b\u5b50\u4f7f\u7528\u4e86 Cilk \u8bed\u8a00\uff0c\u53c2\u89c1 : a\u3001 https://cilk.mit.edu/programming/ b\u3001wikipedia Cilk 2\u3001 cilk_spawn \u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e \u521b\u5efa\u4e86\u4e00\u4e2athread\uff0c\u5b83\u5bf9\u5e94\u7684\u7684 pthread_create cilk_sync \u5bf9\u5e94\u7684\u662f pthrread_join \u663e\u7136\u4e0a\u8ff0\u4f8b\u5b50\u662f\u5178\u578b\u7684: fork\u2013join idiom 3\u3001\u4e0a\u8ff0\u4f8b\u5b50\u975e\u5e38\u597d\u5730\u5c55\u793a\u4e86false sharing: \u901a\u8fc7\u4e0b\u9762\u7684\u63cf\u8ff0\"64-byte cache line\"\u53ef\u77e5\uff0ccache line\u662f64-byte\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u662f16\u4e2aint\uff0c\u800c void f(volatile int *p) \u7684\u5b9e\u73b0\u4e3a: ++ p [ 0 ]; ++ p [ 16 ]; \u663e\u7136\u8fd9\u662f17\u4e2aint\uff0c\u663e\u7136\u5b83\u5c31\u4f1a\u89e6\u53d1\"cache line contention\"\u3002 The x[] elements are four bytes wide, and a 64-byte cache line would hold 16 elements. There are no data races, and the results will be correct when the loop completes. However, cache line contention as the individual strands update adjacent array elements can degrade performance, sometimes significantly. wikipedia False sharing In computer science , false sharing is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism. When a system participant attempts to periodically access data that will never be altered by another party, but those data share a cache block with data that are altered, the caching protocol may force the first participant to reload the whole unit despite a lack of logical necessity. The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource. NOTE: 1\u3001\u76f8\u6bd4\u4e8e\u524d\u9762\u4e24\u7bc7\u6587\u7ae0\u7684\u5177\u4f53\uff0cwikipedia False sharing \u7684\u4e0a\u8ff0\u5185\u5bb9\u5c31\u975e\u5e38\u62bd\u8c61\u4e86: a\u3001\"the size of the smallest resource block managed by the caching mechanism\" \u5bf9\u5e94\u7684\u662f size of cache line\uff0c\u5176\u5b9e\u5c31\u662funit TODO Intel . Avoiding and Identifying False Sharing Among Threads .","title":"Introduction"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#false#sharing","text":"\u662f\u5728\u9605\u8bfb zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a) \u65f6\uff0c\u5176\u4e2d\u63d0\u53ca\u4e86false sharing\uff0c\u5e76\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002","title":"False sharing"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#zhuanlanzhihu#--","text":"","title":"zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a)"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#cachelinefalse#sharing","text":"struct test { uint32_t a ; uint32_t b ; } test t ; t \u662f\u4e00\u4e2a\u5171\u4eab\u53d8\u91cf\uff0ccore1\u4e0a\u7ebf\u7a0bt1\u8bbf\u95ee t.a \uff0c core2\u7ebf\u7a0bt2\u8bbf\u95ee t.b ,\u5982\u679ccacheline\u5927\u5c0f\u662f16Bytes\uff0c\u90a3\u4e48\u5f53t1\u8bbf\u95ee t.a \u65f6cache miss\uff0c\u7136\u540e\u4f1a\u5c06\u8fd9\u4e2a t.a \u8bfb\u53d6\u5230cache\u4e2d\uff0c\u4f46\u662f\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0ccore\u6bcf\u6b21\u8bfb\u53d6\u7684\u957f\u5ea6\u662fcacheline\u7684\u957f\u5ea6\uff0c t.a \u957f\u5ea6\u662f4\uff0c t.b \u957f\u5ea6\u662f4\uff0c\u6240\u4ee5\u5728\u4e00\u6b21\u8bfb\u53d6\u4e2d\u4f1a\u5c06 t.a \u548c t.b \u4e00\u6b21\u6027\u8bfb\u5230\u4e86\u81ea\u5df1cache\u4e2d\u4e86\uff0c\u90a3\u4e48t2\u4e5f\u540c\u6837\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\uff0c\u5982\u679ct1\u4fee\u6539\u5f53\u524dcacheline\u4e2d\u7684\u503c\uff0c\u4ed6\u9700\u8981\u4e0ecore2\u5bf9\u5e94\u7684cacheline\u540c\u6b65\uff0c\u90a3\u4e48\u672c\u6765\u4e24\u4e2a\u53ef\u4ee5\u5e76\u53d1\u7684memory location\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u3002\u5bf9\u6027\u80fd\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u5f71\u54cd\u3002 \u90a3\u4e48\u4e3a\u4e86\u907f\u514d**false sharing**\uff0c\u5c31\u9700\u8981\u8ba9\u8fd9\u4e24\u4e2amemory location\u8fdb\u884ccacheline\u5bf9\u9f50\u3002C++ \u63d0\u4f9b\u4e86**alignas**\u7684\u65b9\u6cd5\u3002","title":"cacheline\u5bfc\u81f4\u7684false sharing"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#dzone#false#sharing","text":"Memory is stored within the cache system in units know as cache lines . Cache lines are a power of 2 of contiguous bytes which are typically 32-256 in size. The most common cache line size is 64 bytes. False sharing is a term which applies when threads unwittingly(\u4e0d\u77e5\u4e0d\u89c9\u5730) impact the performance of each other while modifying independent variables sharing the same cache line . Write contention on cache lines is the single most limiting factor on achieving scalability for parallel threads of execution in an SMP system. I\u2019ve heard false sharing described as the silent performance killer because it is far from obvious when looking at code. NOTE: \u8fd9\u6bb5\u8bdd\u975e\u5e38\u5177\u6709\u542f\u793a\u4f5c\u7528","title":"dzone False Sharing"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#to#achieve#linear#scalability#with#number#of#threads","text":"NOTE: \u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u597d 1\u3001\u5982\u4f55\u4fdd\u8bc1\"no two threads write to the same variable or cache line \"? \u4fdd\u8bc1\"no two threads write to the same variable\"\u662f\u76f8\u5bf9\u5bb9\u6613\u7684\uff0c\u4f46\u662f\u540e\u8005\u7684\u5b9e\u73b0\u5219\u76f8\u5bf9\u9ebb\u70e6\u4e00\u4e9b\u3002\u5176\u5b9e\u8fd9\u5c31\u662f\u5982\u4f55detect false sharing\u3002 To achieve linear scalability with number of threads, we must ensure no two threads write to the same variable or cache line . Two threads writing to the same variable can be tracked down at a code level. To be able to know if independent variables share the same cache line we need to know the memory layout , or we can get a tool to tell us. Intel VTune is such a profiling tool. In this article I\u2019ll explain how memory is laid out for Java objects and how we can pad(\u8865\u9f50) out our cache lines to avoid false sharing. NOTE: \u6700\u540e\u4e00\u6bb5\u8bdd\u7684\u7ffb\u8bd1\u5982\u4e0b: \"\u5728\u672c\u6587\u4e2d\uff0c\u6211\u5c06\u89e3\u91ca\u5982\u4f55\u4e3aJava\u5bf9\u8c61\u5e03\u5c40\u5185\u5b58\uff0c\u4ee5\u53ca\u5982\u4f55\u6269\u5c55\u7f13\u5b58\u7ebf\u4ee5\u907f\u514d\u9519\u8bef\u5171\u4eab\u3002\" Figure 1. above illustrates the issue of false sharing . A thread running on core 1 wants to update variable X while a thread on core 2 wants to update variable Y . Unfortunately these two hot variables reside in the same cache line . Each thread will race for ownership of the cache line so they can update it. If core 1 gets ownership then the cache sub-system will need to invalidate the corresponding cache line for core 2. When Core 2 gets ownership and performs its update, then core 1 will be told to invalidate its copy of the cache line. This will ping pong back and forth via the L3 cache greatly impacting performance. The issue would be further exacerbated(\u52a0\u91cd) if competing cores are on different sockets and additionally have to cross the socket interconnect.","title":"To achieve linear scalability with number of threads"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#java#memory#layout","text":"For the Hotspot JVM, all objects have a 2-word header. First is the \u201cmark\u201d word which is made up of 24-bits for the hash code and 8-bits for flags such as the lock state , or it can be swapped for lock objects. The second is a reference to the class of the object. Arrays have an additional word for the size of the array. Every object is aligned to an 8-byte granularity boundary for performance. Therefore to be efficient when packing, the object fields are re-ordered from declaration order to the following order based on size in bytes: 1\u3001doubles (8) and longs (8) 2\u3001ints (4) and floats (4) 3\u3001shorts (2) and chars (2) 4\u3001booleans (1) and bytes (1) 5\u3001references (4/8) With this knowledge we can pad a cache line between any fields with 7 longs. Within the Disruptor we pad cache lines around the RingBuffer cursor and BatchEventProcessor sequences. To show the performance impact let\u2019s take a few threads each updating their own independent counters. These counters will be *volatile long*s so the world can see their progress. public final class FalseSharing implements Runnable { public final static int NUM_THREADS = 4 ; // change public final static long ITERATIONS = 500L * 1000L * 1000L ; private final int arrayIndex ; private static VolatileLong [] longs = new VolatileLong [ NUM_THREADS ]; static { for ( int i = 0 ; i < longs . length ; i ++ ) { longs [ i ] = new VolatileLong (); } } public FalseSharing ( final int arrayIndex ) { this . arrayIndex = arrayIndex ; } public static void main ( final String [] args ) throws Exception { final long start = System . nanoTime (); runTest (); System . out . println ( \"duration = \" + ( System . nanoTime () - start )); } private static void runTest () throws InterruptedException { Thread [] threads = new Thread [ NUM_THREADS ]; for ( int i = 0 ; i < threads . length ; i ++ ) { threads [ i ] = new Thread ( new FalseSharing ( i )); } for ( Thread t : threads ) { t . start (); } for ( Thread t : threads ) { t . join (); } } public void run () { long i = ITERATIONS + 1 ; while ( 0 != -- i ) { longs [ arrayIndex ]. value = i ; } } public final static class VolatileLong { public volatile long value = 0L ; public long p1 , p2 , p3 , p4 , p5 , p6 ; // comment out } } Results Running the above code while ramping(\u63d0\u5347) the number of threads and adding/removing the cache line padding , I get the results depicted in Figure 2. below. This is measuring the duration of test runs on my 4-core Nehalem at home. Figure 2. The impact of false sharing can clearly be seen by the increased execution time required to complete the test. Without the cache line contention we achieve near linear scale up with threads. This is not a perfect test because we cannot be sure where the VolatileLongs will be laid out in memory. They are independent objects. However experience shows that objects allocated at the same time tend to be co-located. So there you have it. False sharing can be a silent performance killer.","title":"Java Memory Layout"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#ustcedu#false#sharing","text":"False sharing is a common problem in shared memory parallel processing . It occurs when two or more cores hold a copy of the same memory cache line . NOTE: 1\u3001\u603b\u7ed3\u5f97\u6bd4\u8f83\u597d If one core writes, the cache line holding the memory line is invalidated on other cores. Even though another core may not be using that data (reading or writing), it may be using another element of data on the same cache line. The second core will need to reload the line before it can access its own data again.","title":"ustc.edu False Sharing"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#detect#false#sharing","text":"The cache hardware ensures data coherency , but at a potentially high performance cost if false sharing is frequent. A good technique to identify false sharing problems is to catch unexpected sharp increases in last-level cache misses using hardware counters or other performance tools. NOTE: 1\u3001\u8fd9\u6bb5\u8bdd\u8ba9\u6211\u60f3\u5230\u4e86cache Miss \u548c false sharing\u7684\u5173\u8054","title":"Detect false sharing"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#simple#example","text":"As a simple example, consider a spawned function with a for loop that increments array values. The array is volatile to force the compiler to generate store instructions rather than hold values in registers or optimize the loop. NOTE: 1\u3001\u6b64\u5904\u5bf9 volatile \u7684\u89e3\u91ca\u3001\u4f7f\u7528\u662f\u975e\u5e38\u597d\u7684 volatile int x [ 32 ]; void f ( volatile int * p ) { for ( int i = 0 ; i < 100000000 ; i ++ ) { ++ p [ 0 ]; ++ p [ 16 ]; } } int main () { cilk_spawn f ( & x [ 0 ]); cilk_spawn f ( & x [ 1 ]); cilk_spawn f ( & x [ 2 ]); cilk_spawn f ( & x [ 3 ]); cilk_sync ; return 0 ; } NOTE: 1\u3001\u4e0a\u8ff0\u4f8b\u5b50\u4f7f\u7528\u4e86 Cilk \u8bed\u8a00\uff0c\u53c2\u89c1 : a\u3001 https://cilk.mit.edu/programming/ b\u3001wikipedia Cilk 2\u3001 cilk_spawn \u5176\u5b9e\u5c31\u76f8\u5f53\u4e8e \u521b\u5efa\u4e86\u4e00\u4e2athread\uff0c\u5b83\u5bf9\u5e94\u7684\u7684 pthread_create cilk_sync \u5bf9\u5e94\u7684\u662f pthrread_join \u663e\u7136\u4e0a\u8ff0\u4f8b\u5b50\u662f\u5178\u578b\u7684: fork\u2013join idiom 3\u3001\u4e0a\u8ff0\u4f8b\u5b50\u975e\u5e38\u597d\u5730\u5c55\u793a\u4e86false sharing: \u901a\u8fc7\u4e0b\u9762\u7684\u63cf\u8ff0\"64-byte cache line\"\u53ef\u77e5\uff0ccache line\u662f64-byte\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u662f16\u4e2aint\uff0c\u800c void f(volatile int *p) \u7684\u5b9e\u73b0\u4e3a: ++ p [ 0 ]; ++ p [ 16 ]; \u663e\u7136\u8fd9\u662f17\u4e2aint\uff0c\u663e\u7136\u5b83\u5c31\u4f1a\u89e6\u53d1\"cache line contention\"\u3002 The x[] elements are four bytes wide, and a 64-byte cache line would hold 16 elements. There are no data races, and the results will be correct when the loop completes. However, cache line contention as the individual strands update adjacent array elements can degrade performance, sometimes significantly.","title":"Simple example"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#wikipedia#false#sharing","text":"In computer science , false sharing is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism. When a system participant attempts to periodically access data that will never be altered by another party, but those data share a cache block with data that are altered, the caching protocol may force the first participant to reload the whole unit despite a lack of logical necessity. The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource. NOTE: 1\u3001\u76f8\u6bd4\u4e8e\u524d\u9762\u4e24\u7bc7\u6587\u7ae0\u7684\u5177\u4f53\uff0cwikipedia False sharing \u7684\u4e0a\u8ff0\u5185\u5bb9\u5c31\u975e\u5e38\u62bd\u8c61\u4e86: a\u3001\"the size of the smallest resource block managed by the caching mechanism\" \u5bf9\u5e94\u7684\u662f size of cache line\uff0c\u5176\u5b9e\u5c31\u662funit","title":"wikipedia False sharing"},{"location":"CPU-memory-access/CPU-cache-memory/False-sharing/#todo","text":"Intel . Avoiding and Identifying False Sharing Among Threads .","title":"TODO"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/","text":"zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a) NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u5199\u5f97\u975e\u5e38\u597d \"Multithreading is just one damn thing after, before, or simultaneous with another. \"[2] 0 \u5199\u5728\u524d\u9762 \u5728\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u4e86\u63d0\u9ad8**\u53ef\u9760\u6027**\uff0c\u901a\u5e38\u4f1a\u5f15\u5165\u591a\u4e2a\u526f\u672c\uff0c\u591a\u4e2a\u526f\u672c\u9700\u8981\u5411\u7528\u6237\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5185\u5bb9\u3002\u8fd9\u5f88\u81ea\u7136\u7684\u8ba9\u4eba\u60f3\u5230\u5982\u4f55\u786e\u4fdd\u591a\u526f\u672c\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\u4e5f\u6709\u4e86paxos\u548craft\u7b49\u4fdd\u8bc1\u591a\u526f\u672c\u4e4b\u95f4\u4e00\u81f4\u6027\u7684\u534f\u8bae\u3002\u5f53\u6211\u4eec\u5728\u4e00\u4e2a\u591a\u5904\u7406\u5668\u673a\u5668\u4e0a\u7f16\u7a0b\u65f6\u6211\u4eec\u901a\u5e38\u4f1a\u5ffd\u7565\u5728\u591a\u5904\u7406\u5668\u73af\u5883\u4e0b\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5df2\u7ecf\u4e3a\u6211\u4eec\u505a\u597d\u4e86\u57fa\u672c\u7684**\u4e00\u81f4\u6027**\u4fdd\u8bc1\uff0c\u5f53\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\u7684\u65f6\u5019\u4e0a\u5c42\u7f16\u7a0b\u8bed\u8a00\u4e5f\u63d0\u4f9b\u4e86\u5177\u5907\u4e00\u81f4\u6027\u8bed\u610f\u7684\u63a5\u53e3\uff0c\u53ea\u662f\u6211\u4eec\u5728\u7f16\u7a0b\u4e2d\u5e76\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e9b**\u63a5\u53e3**\u4e0e**\u4e00\u81f4\u6027**\u7684\u5173\u7cfb\u3002\u65e0\u8bba\u662f\u5206\u5e03\u5f0f\u5b58\u50a8\u8fd8\u662f\u591a\u5904\u7406\u5668\u7f16\u7a0b\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u70b9\uff0c\u5c31\u662f\u4f1a\u6d89\u53ca\u5171\u4eab\u5bf9\u8c61\u7684\u64cd\u4f5c\u3002 NOTE: 1\u3001\"\u53ef\u9760\u6027\"\uff0c\u5176\u5b9e\u5c31\u662fHA \"\u591a\u4e2a\u526f\u672c\"\uff0c\u5176\u5b9e\u5c31\u662fmaster-slave \"\u4e00\u81f4\u7684\u5185\u5bb9\"\uff0c\u5176\u5b9e\u5c31\u662fconsistency \"\u5171\u4eab\u5bf9\u8c61\"\u5176\u5b9e\u5c31\u662fmultiple model\u4e2d\u7684shared data 2\u3001\u4e0a\u8ff0\u5176\u5b9e\u53ef\u4ee5\u4f7f\u7528multiple model-shared data\u6765\u8fdb\u884c\u63cf\u8ff0 \u4e00\u65e6\u51fa\u73b0\u5171\u4eab\uff0c\u5c31\u4f1a\u51fa\u73b0\u6b63\u786e\u6027\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u5982\u4f55\u5b9a\u4e49\u5728\u5e76\u53d1\u4e2d\u64cd\u4f5c\u5171\u4eab\u5bf9\u8c61\u7684\u6b63\u786e\u6027\uff0c\u8fd9\u5c31\u662f\u4e00\u81f4\u6027\u534f\u8bae\u7684\u4efb\u52a1\u4e86\u3002 \u672c\u6587\u4e3b\u8981\u9488\u5bf9\u591a\u5904\u7406\u5668\u7cfb\u7edf\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u8fdb\u884c\u4e86\u4e00\u4e9b\u603b\u7ed3\uff0c\u5bf9\u4e8e\u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u4f1a\u5728\u540e\u9762\u6587\u7ae0\u4e2d\u603b\u7ed3\u3002 \u591a\u5904\u7406\u5668\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u6e90\u4e8e\u5e76\u53d1\uff0c\u6e90\u4e8e\u5171\u4eab\u3002\u5bf9\u4e8e\u5171\u4eab\u5185\u5b58\u5e76\u53d1\u64cd\u4f5c\u4e0b\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\u662f\u786c\u4ef6\u8bbe\u8ba1\u8005\u9700\u8981\u63d0\u4f9b\u7ed9\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6700\u91cd\u8981\u7684\u4fdd\u8bc1\u3002\u5bf9\u4e8e\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6765\u8bf4\uff0c\u7cfb\u7edf\u5185\u90e8\u7684\u4e00\u81f4\u6027\u662f\u900f\u660e\u7684\uff0c\u4f46\u662f\u53bb\u4e86\u89e3\u7cfb\u7edf\u5185\u90e8\u4e00\u81f4\u6027\u7684\u8bbe\u8ba1\u53ca\u539f\u7406\u6709\u5229\u4e8e\u6211\u4eec\u66f4\u80fd\u591f\u9762\u5411\u673a\u5668\u7f16\u7a0b\uff0c\u5199\u51fa\u6b63\u786e\u7684\u4ee3\u7801\u3002 \u7531\u4e8e\u6c34\u5e73\u6709\u9650\uff0c\u6587\u4e2d\u5982\u5b58\u5728\u95ee\u9898\uff0c\u8bf7\u5e2e\u5fd9\u63d0\u51fa\u3002 1 \u672c\u6587\u4e3b\u8981\u5185\u5bb9 \u672c\u6587\u4e3b\u8981\u5305\u62ec\u4ee5\u4e0b\u5185\u5bb9\uff1a 1\u3001cache coherence cache design cache coherence protocol store buffer & invalidate message queue memory barrier/ fence 2\u3001memory consistency consistency \u5206\u7c7b\u5b9a\u4e49 SC x86 TSO relaxed memory model 3\u3001C++ memory order memory model and memory order memory order release/acquire memory order sequence memory order relaxed 4\u3001Synchronize insight lock 2 cache coherence[4] \u4e00\u81f4\u6027\u7684\u4efb\u52a1\u5c31\u662f\u4fdd\u8bc1\u5e76\u53d1\u64cd\u4f5c\u5171\u4eab\u5185\u5b58\u7684\u6b63\u786e\u6027\uff0c\u5728\u8ba8\u8bba\u5185\u5b58\u64cd\u4f5c\u6b63\u786e\u6027\u7684\u65f6\u5019\uff0c\u901a\u5e38\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1amemory consistency \u548c cache coherence[3]\u3002 \u672c\u8282\u7740\u91cd\u8ba8\u8bbacache coherence\u7684\u5b9e\u73b0\u3002 2.1 cache vs buffer \u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u901a\u5e38\u4f1a\u6709cache \u53ca buffer\u7684\u8bbe\u8ba1\uff0c\u4e24\u8005\u90fd\u6709\u7f13\u5b58\u7684\u610f\u601d\uff0c\u90a3\u4e48\u771f\u6b63\u7684\u533a\u522b\u5728\u54ea\u91cc\u5462\uff0c\u77e5\u4e4e\u4e0a\u6709\u4e2a\u56de\u7b54\u6211\u611f\u89c9\u633a\u597d\u7684\u3002 Cache \u548c Buffer \u90fd\u662f\u7f13\u5b58\uff0c\u4e3b\u8981\u533a\u522b\u662f\u4ec0\u4e48\uff1f Buffer \uff08\u7f13\u51b2\u533a\uff09\u662f\u7cfb\u7edf\u4e24\u7aef\u5904\u7406**\u901f\u5ea6\u5e73\u8861**\uff08\u4ece\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u770b\uff09\u65f6\u4f7f\u7528\u7684\u3002\u5b83\u7684\u5f15\u5165\u662f\u4e3a\u4e86\u51cf\u5c0f\u77ed\u671f\u5185\u7a81\u53d1I/O\u7684\u5f71\u54cd\uff0c\u8d77\u5230**\u6d41\u91cf\u6574\u5f62**\u7684\u4f5c\u7528\u3002\u6bd4\u5982\u751f\u4ea7\u8005\u2014\u2014\u6d88\u8d39\u8005\u95ee\u9898\uff0c\u4ed6\u4eec\u4ea7\u751f\u548c\u6d88\u8017\u8d44\u6e90\u7684\u901f\u5ea6\u5927\u4f53\u63a5\u8fd1\uff0c\u52a0\u4e00\u4e2abuffer\u53ef\u4ee5\u62b5\u6d88\u6389\u8d44\u6e90\u521a\u4ea7\u751f/\u6d88\u8017\u65f6\u7684\u7a81\u7136\u53d8\u5316\u3002 Cache \uff08\u7f13\u5b58\uff09\u5219\u662f\u7cfb\u7edf\u4e24\u7aef\u5904\u7406**\u901f\u5ea6\u4e0d\u5339\u914d**\u65f6\u7684\u4e00\u79cd**\u6298\u8877\u7b56\u7565**\u3002\u56e0\u4e3aCPU\u548cmemory\u4e4b\u95f4\u7684\u901f\u5ea6\u5dee\u5f02\u8d8a\u6765\u8d8a\u5927\uff0c\u6240\u4ee5\u4eba\u4eec\u5145\u5206\u5229\u7528\u6570\u636e\u7684\u5c40\u90e8\u6027\uff08locality\uff09\u7279\u5f81\uff0c\u901a\u8fc7\u4f7f\u7528\u5b58\u50a8\u7cfb\u7edf\u5206\u7ea7\uff08memory hierarchy\uff09\u7684\u7b56\u7565\u6765\u51cf\u5c0f\u8fd9\u79cd\u5dee\u5f02\u5e26\u6765\u7684\u5f71\u54cd\u3002 2.2 cache\u8bbe\u8ba1 \u4ececache vs buffer\u7684\u533a\u522b\u4e2d\u6211\u4eec\u53ef\u770b\u51fa\uff0ccache\u7684\u51fa\u73b0\u5176\u5b9e\u5c31\u662f\u4e3a\u4e86\u7f13\u89e3CPU\u4e0e\u4e3b\u5b58\u4e4b\u95f4\u7684\u901f\u5ea6\u6781\u5ea6\u4e0d\u5e73\u8861\u3002\u56e0\u6b64\u901a\u5e38CPU\u8bbe\u8ba1\u4e2d\u5f15\u5165\u4e86cache\uff0c\u5206\u4e3aL1 cache\uff0cL2 cache\u548cL3 cache\uff0c\u5176\u6027\u80fd\u6307\u6570\u5927\u81f4\u5982\u4e0b[11]\uff1a \u901a\u5e38L1 cache\u53c8\u4f1a\u5206\u4e3aI cache\u548cD cache\uff0c\u5206\u522b\u5b58\u653e\u6307\u4ee4\u4e0e\u6570\u636e\uff0cL1 cache\u901a\u5e38\u662fcore\u72ec\u5360\u7684\uff0cL2\u53ef\u4ee5\u662f\u4e24\u4e2acore\u5171\u4eab\uff0cL3 cache\u5219\u662f\u6240\u6709core\u5171\u4eab\u3002 2.2.0 CPU cache\u67b6\u6784 \u5982\u4e0b\u56fe[4]\uff0c \u76ee\u524d\u7684CPU\u67b6\u6784\u4e2d\u7684cache\u901a\u5e38\u67b6\u6784\u5982\u6b64\uff0c\u8fd9\u91cc\u7684cache\u662fL1 cache\u3002\u6211\u4eec\u8fd9\u91cc\u8ba8\u8bba\u7684cache\u57fa\u672c\u90fd\u662f\u57fa\u4e8eL1 cache\u7684\uff0c\u56e0\u4e3aL1 cache\u901a\u5e38\u662fcore\u72ec\u5360\u7684\uff0c\u6240\u4ee5L1 cache\u7684\u8bbe\u8ba1\u53ca\u6570\u636e\u4e00\u81f4\u6027\u662f\u96be\u70b9\u4e5f\u662f\u91cd\u70b9\u3002 cache**\u4e2d\u7684\u6570\u636e\u5b58\u653e\u5355\u4f4d\u662f**cache line \uff0c\u901a\u5e38\u4e00\u4e2a**cache line**\u957f\u5ea6\u5927\u6982\u572816-256Bytes\u4e4b\u95f4\u3002CPU\u8bbf\u95eememory\u7684\u65f6\u5019\u9996\u5148\u4f1a\u5148\u67e5\u770bcache\u4e2d\u662f\u5426\u5df2\u7ecf\u7f13\u5b58\u8be5address\uff0c\u5982\u679c\u5df2\u7f13\u5b58\u5f53\u524daddress\u5e76\u4e14\u662f\u53ef\u7528\u7684\uff0c\u90a3\u4e48CPU\u5c31\u4f1a\u76f4\u63a5\u4ececache\u8bfb\u53d6\u8be5\u503c\uff0c\u8fd9\u6837\u6781\u5927\u7684\u63d0\u9ad8performance\u3002\u5982\u679c\u6240\u8981\u8bbf\u95ee\u7684address\u4e0d\u5728cache\u4e2d\uff0cCPU\u5219\u8981\u4ecememory\u8bfb\u53d6\uff0cCPU\u4f1a\u8bfb\u53d6cache line\u5927\u5c0f\u7684\u6570\u636e\uff0c\u5e76\u5c06\u5176\u653e\u5165cache\u4e2d\u8fd9\u6837\u4e0b\u6b21\u8bfb\u53d6\u7684\u65f6\u5019\u53ef\u4ee5\u76f4\u63a5\u4ececache\u4e2d\u8bfb\u3002L1 cache\u901a\u5e38\u572832KB\u5927\u5c0f\uff0c\u5f53cache\u88ab\u586b\u6ee1\u4e4b\u540e\u5c31\u9700\u8981\u6309\u7167\u4e00\u5b9a\u7b56\u7565\u5c06\u5143\u7d20\u8e22\u51fa\u53bb\uff0c\u8fd9\u4e2a\u7b56\u7565\u53ef\u4ee5\u662f**LRU**\u6216\u8005**\u968f\u673a**\u7b49\u7b49\u3002 NOTE: \u8fd9\u8ba9\u6211\u60f3\u5230\u4e86virtual memory\u7684page swap \u8fd9\u91cc\u6709\u4e24\u4e2a\u5c0f\u70b9\u9700\u8981\u6ce8\u610f\uff1a 1\u3001L1 cache\u72ec\u5360\u5bfc\u81f4\u7684\u6570\u636e\u4e0d\u4e00\u81f4 \u56e0\u4e3a\u6bcf\u4e2acore\u90fd\u6709\u81ea\u5df1\u72ec\u7acb\u7684L1 cache\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u5171\u4eab\u7684memory location\u4e24\u4e2acache\u53ef\u4ee5\u6709\u81ea\u5df1\u7684copy\u3002\u90a3\u4e48\u8fd9\u5c31\u4f1a\u51fa\u73b0\u4e86\u6570\u636e\u4e0d\u4e00\u81f4\u7684\u72b6\u51b5\uff0c\u4e24\u4e2acore\u53ef\u80fd\u540c\u65f6\u8bbf\u95ee\u8fd9\u4e2amemory location\uff0c\u4e14\u5982\u679c\u4e00\u4e2acore\u662f\u5bf9\u8fd9\u4e2amemory location\u8fdb\u884c\u4fee\u6539\uff0c\u90a3\u4e48\u8fd9\u5c31\u9700\u8981\u4e24\u8fb9\u7684cache\u8fdb\u884c\u540c\u6b65\uff0c\u9632\u6b62\u6570\u636e\u4e0d\u4e00\u81f4\u3002\u8fd9\u4e2a\u5de5\u4f5c\u5c31\u662f**cache coherence protocol**\u8981\u505a\u7684\u4e8b\u60c5\u3002 2\u3001cacheline\u5bfc\u81f4\u7684false sharing struct test { uint32_t a ; uint32_t b ; } test t ; t\u662f\u4e00\u4e2a\u5171\u4eab\u53d8\u91cf\uff0ccore1\u4e0a\u7ebf\u7a0bt1\u8bbf\u95ee t.a \uff0c core2\u7ebf\u7a0bt2\u8bbf\u95ee t.b ,\u5982\u679ccacheline\u5927\u5c0f\u662f16Bytes\uff0c\u90a3\u4e48\u5f53t1\u8bbf\u95eet.a\u65f6cache miss\uff0c\u7136\u540e\u4f1a\u5c06\u8fd9\u4e2at.a\u8bfb\u53d6\u5230cache\u4e2d\uff0c\u4f46\u662f\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0ccore\u6bcf\u6b21\u8bfb\u53d6\u7684\u957f\u5ea6\u662fcacheline\u7684\u957f\u5ea6\uff0ct.a\u957f\u5ea6\u662f4\uff0c t.b\u957f\u5ea6\u662f4\uff0c\u6240\u4ee5\u5728\u4e00\u6b21\u8bfb\u53d6\u4e2d\u4f1a\u5c06t.a\u548ct.b\u4e00\u6b21\u6027\u8bfb\u5230\u4e86\u81ea\u5df1cache\u4e2d\u4e86\uff0c\u90a3\u4e48t2\u4e5f\u540c\u6837\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\uff0c\u5982\u679ct1\u4fee\u6539\u5f53\u524dcacheline\u4e2d\u7684\u503c\uff0c\u4ed6\u9700\u8981\u4e0ecore2\u5bf9\u5e94\u7684cacheline\u540c\u6b65\uff0c\u90a3\u4e48\u672c\u6765\u4e24\u4e2a\u53ef\u4ee5\u5e76\u53d1\u7684memory location\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u3002\u5bf9\u6027\u80fd\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u5f71\u54cd\u3002 NOTE: \u4e0a\u8ff0\u95ee\u9898\u548cbyte assignment \u975e\u5e38\u7c7b\u4f3c \u90a3\u4e48\u4e3a\u4e86\u907f\u514dfalse sharing\uff0c\u5c31\u9700\u8981\u8ba9\u8fd9\u4e24\u4e2amemory location\u8fdb\u884ccacheline\u5bf9\u9f50\u3002C++ \u63d0\u4f9b\u4e86alignas\u7684\u65b9\u6cd5\u3002 2.3 cache coherence protocol \u524d\u9762\u5c0f\u8282\u63d0\u5230\u4e86\u4e3a\u4ec0\u4e48\u9700\u8981coherence protocol\uff0c\u90a3\u4e48coherence protocol\u662f\u5982\u4f55\u4fdd\u8bc1cache\u4e4b\u95f4\u7684\u6570\u636e\u4e00\u81f4\u6027\u7684\u5462\uff1f \u8fd9\u91cc\u5c31\u9700\u8981\u5f15\u5165MESI\u534f\u8bae\u4e86\uff0cMESI\u662f\u4e00\u79cd\u7ef4\u62a4cacheline\u72b6\u6001\u4e00\u81f4\u6027\u7684\u534f\u8bae\u3002MESI: \u7531Modified,Exclusive,Share,Invalid\u56db\u79cd\u72b6\u6001\u7ec4\u5408\u3002 2.3.1 cacheline state 1\u3001Modified(M): \u5f53\u4e00\u4e2acore \u7684cacheline\u7684\u72b6\u6001\u662fM\u65f6\uff0c\u8bf4\u660e\u5f53\u524dcore\u6700\u8fd1\u4fee\u6539\u4e86\u8fd9\u4e2acache\uff0c\u90a3\u4e48\u5176\u4ed6core\u7684cache\u4e0d\u80fd\u518d\u4fee\u6539\u5f53\u524dcache line\u5bf9\u5e94\u7684memory location\uff0c\u9664\u975e\u8be5cache\u5c06\u8fd9\u4e2a\u4fee\u6539\u540c\u6b65\u5230\u4e86memory\u3002\u8fd9\u4e2acore\u5bf9\u8fd9\u4e2amemory location\u53ef\u4ee5\u7406\u89e3\u4e3aOwned\u3002 NOTE: \u5176\u4ed6core\u7684cache\u5982\u4f55\u77e5\u9053\"\u4e0d\u80fd\u518d\u4fee\u6539\u5f53\u524dcache line\u5bf9\u5e94\u7684memory location\"\uff1f\u53c2\u89c1\u4e0b\u9762\u7684share\u3002 2\u3001Exclusive(E): NOTE: \"exclusive\"\u7684\u610f\u601d\u662f\u6392\u4ed6\u7684\u3001\u72ec\u6709\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u8fd9\u4e2acache\u4e2d\u7684memory location\u662f\u72ec\u6709\u7684\uff0c\u5176\u4ed6\u7684cache\u4e2d\u90fd\u4e0d\u5305\u542b\uff1b\u663e\u7136\u5b83\u548c\"share\"\u662f\u76f8\u53cd\u7684 E\u8fd9\u4e2a\u72b6\u6001\u4e0eM\u5f88\u50cf\uff0c\u533a\u522b\u5728\u4e8e\u5f53\u524dcore\u5e76\u6ca1\u6709\u4fee\u6539\u5f53\u524d\u7684cacheline\uff0c\u8fd9\u610f\u5473\u7740\u5f53\u524dcacheline\u5b58\u50a8\u7684memory location\u7684\u503c\u662f\u6700\u65b0\u7684\u3002\u5f53\u524dcore\u53ef\u4ee5\u5bf9\u8be5cacheline\u8fdb\u884cmodify\u4e14\u4e0d\u9700\u8981\u4e0e\u5176\u4ed6core\u7684cache\u540c\u6b65\u3002\u8fd9\u4e2acore\u5bf9\u8fd9\u4e2amemory location\u53ef\u4ee5\u7406\u89e3\u4e3aOwned\u3002 3\u3001Share(S): S\u8868\u793a\u5f53\u524dcacheline\u5728\u5176\u4ed6core\u7684cache\u4e5f\u5b58\u5728copy\uff0c\u5f53\u524dcore\u5982\u679c\u9700\u8981\u4fee\u6539\u8be5cacheline\u5219\u9700\u8981\u4e0e\u5176\u4ed6core\u7684cache\u8fdb\u884c\u63d0\u524d\u6c9f\u901a\u3002 4\u3001Invalid(I): I\u8868\u793a\u5f53\u524dcacheline\u662f\u7a7a\u7684\u3002 cacheline\u7684\u72b6\u6001\u53d8\u5316\u9700\u8981\u5728\u5404\u4e2acore\u4e4b\u95f4\u540c\u6b65\uff0c\u90a3\u4e48\u5982\u4f55\u8fdb\u884c\u540c\u6b65\u5462\uff0cMESI\u4f7f\u7528protocol message\u3002 NOTE: \u8fd9\u8ba9\u6211\u60f3\u5230\u4e86gossip protocol 2.3.2 protocol message \u4f5c\u4e3acore\u4e4b\u95f4\u7684\u6c9f\u901a\u5de5\u5177\uff0cprotocol message\u5206\u4e3a\u4ee5\u4e0b\u51e0\u79cd\u6d88\u606f\u7c7b\u578b\uff1a 1\u3001Read \u5f53\u4e00\u4e2acache\u9700\u8981\u8bfb\u53d6\u67d0\u4e2acacheline\u6d88\u606f\u7684\u65f6\u5019\u5c31\u4f1a\u53d1\u8d77read\u6d88\u606f\u3002 2\u3001Read Response read response\u662fread\u7684\u56de\u5e94\uff0c\u8fd9response\u53ef\u4ee5\u6765\u81ea\u5176\u4ed6core\u7684cache\u4e5f\u53ef\u4ee5\u6765\u81eamemory\u3002\u5f53\u5176\u4ed6core\u4e2d\u5bf9\u5f53\u524dcacheline\u662fM\u72b6\u6001\u65f6\uff0c\u5219\u4f1a\u53d1\u8d77read response\u3002 3\u3001Invalidate Invalidate\u6d88\u606f\u5305\u542b\u5bf9\u5e94\u7684memory location\uff0c\u63a5\u6536\u5230\u8fd9\u4e2a\u6d88\u606f\u7684cache\u9700\u8981\u5c06\u81ea\u5df1cacheline\u5185\u5bb9\u5254\u9664\uff0c\u5e76\u54cd\u5e94\u3002 4\u3001Invalidate acknowledge \u63a5\u6536\u5230Invalidate\u540e\u5220\u9664cacheline\u4e2d\u7684\u6570\u636e\u5c31\u5411\u53d1\u8d77\u8005\u56de\u590dinvalidate ack\u3002 5\u3001Read Invalidate \u8fd9\u4e2a\u6d88\u606f\u5305\u542b\u4e24\u4e2a\u64cd\u4f5c\uff0cread\u548cinvalidate\uff0c\u90a3\u4e48\u5b83\u4e5f\u9700\u8981\u63a5\u6536read response\u548c\u591a\u4e2ainvalidate ack\u54cd\u5e94\u3002 6\u3001write back writeback\u5305\u542b\u6570\u636e\u548c\u5730\u5740\uff0c\u4f1a\u5c06\u8fd9\u4e2a\u5730\u5740\u5bf9\u5e94\u7684\u6570\u636e\u5237\u5230\u5185\u5b58\u4e2d\u3002 NOTE: commit 2.4 store buffer and invalidate queue MESI\u4e2d\u7684\u56db\u79cd\u72b6\u6001\u53ef\u4ee5\u4e92\u76f8\u8f6c\u5316\uff0c\u5177\u4f53\u72b6\u6001\u8fc1\u79fb\u6761\u4ef6\u8bf7\u53c2\u8003[4]\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002 2.4.1 store buffer NOTE: \u5176\u5b9e\u5c31\u662f\u5c06write\u7684\u503c\uff0c\u653e\u5230\u7f13\u5b58\u4e2d CPU\u8bbe\u8ba1\u8005\u90fd\u662f\u5728\u6781\u81f4\u538b\u69a8\u5176\u6027\u80fd\u3002cache\u5c31\u662fCPU\u8bbe\u8ba1\u8005\u538b\u69a8CPU\u6027\u80fd\u7684\u4e00\u79cd\u4f53\u73b0\uff0c\u4f46\u662f\u8fd9\u6837\u4ed6\u4eec\u89c9\u5f97\u8fd8\u4e0d\u591f\u3002\u8003\u8651\u4e00\u79cd\u72b6\u51b5\uff0c\u5047\u8bbe\u5f53core1\u6267\u884c\u4e00\u6b21store\u64cd\u4f5c\uff0c\u4e14\u8fd9\u4e2astore\u64cd\u4f5c\u7684memory location\u5bf9\u5e94\u7684cacheline\u5728\u53e6\u5916\u4e00\u4e2acore2\u4e0a\u662fowned\u72b6\u6001\uff0c\u90a3\u4e48core1\u9700\u8981\u5411core2\u53d1\u9001Invalidate message\uff0c\u5e76\u4e14\u9700\u8981\u7b49\u5230core2\u8fd4\u56deinvalidate ack\u4e4b\u540e\u624d\u80fd\u7ee7\u7eed\u5411\u4e0b\u6267\u884c\uff0c\u90a3\u4e48\u5728\u8fd9\u4e2a\u671f\u95f4core1\u5c31\u5904\u4e8e**\u76f2\u7b49**\u9636\u6bb5\uff0c\u90a3\u4e48core1\u5fc5\u987b\u8981\u7b49\u8fd9\u4e48\u4e45\u5417\uff1f\u4e8e\u662fCPU\u8bbe\u8ba1\u8005\u5f15\u5165\u4e86store buffer\uff0c\u8fd9\u4e2abuffer\u5904\u4e8eCPU\u4e0ecache\u4e4b\u95f4\u3002\u5982\u4e0b\u56fe[4]. NOTE: core1\u6267\u884c\u4e86\u4fee\u6539\uff0c\u56e0\u6b64\u5176\u4ed6\u7684cache\u9700\u8981\u66f4\u65b0\u6700\u65b0\u503c\uff1b \u6709\u4e86store buffer\u4e4b\u540ecore1\u5982\u679c\u6267\u884cstore\u64cd\u4f5c\u5c31\u4e0d\u7528\u7acb\u523b\u5411core2\u53d1\u9001invalidate message\u4e86\uff0ccore1\u53ea\u9700\u8981\u5c06store\u503c\u6dfb\u52a0\u5230store buffer\u4e2d\u5373\u53ef\u3002\u4f46\u662f\u5f15\u5165store buffer\u4f1a\u5e26\u6765\u4e24\u4e2a\u95ee\u9898\u3002 1\u3001\u8003\u8651\u4ee5\u4e0b\u573a\u666f\uff1a // a, b init to 0. a=1; b = a + 1; assert(b == 2); \u4e0a\u8ff0\u4ee3\u7801\u4e2d\uff0ccore\u6267\u884ca=1\u540e\uff0ca=1\u8fd9\u4e2a\u503c\u88ab\u653e\u5230core\u7684storebuffer\u91cc\u4e86\uff0c\u7136\u540e\u7ee7\u7eed\u6267\u884cb=a+1,\u8fd9\u65f6\u5019core\u7684cacheline\u4e2d\u4fdd\u5b58\u7684a\u8fd8\u662f\u539f\u6765\u76840.\u8fd9\u4e2a\u65f6\u5019\u5c31\u4f1a\u5bfc\u81f4assert\u5931\u8d25\u3002\u56e0\u4e3a\u6211\u4eec\u5728storebuffer\u91cc\u548ccacheline\u4e2d\u7684a\u662f\u4e24\u4e2a\u72ec\u7acb\u7684\u62f7\u8d1d\uff0c\u6240\u4ee5\u5bfc\u81f4\u8fd9\u79cd\u9519\u8bef\u7684\u7ed3\u679c\uff0c\u56e0\u6b64CPU\u8bbe\u8ba1\u8005\u901a\u8fc7\u4f7f\u7528\"store Forwarding\"\u7684\u65b9\u5f0f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u5728\u6267\u884cload\u64cd\u4f5c\u65f6\u5148\u53bbstorebuffer\u4e2d\u67e5\u627e\u5bf9\u5e94\u7684memory location\uff0c\u5982\u679c\u67e5\u5230\u5c31\u4f7f\u7528storebuffer\u4e2d\u7684\u6700\u65b0\u503c\u3002 2\u3001\u8003\u8651\u53e6\u5916\u4e00\u4e2a\u573a\u666f void foo ( void ) { a = 1 ; b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; assert ( a == 1 ); } foo\u548cbar\u4e24\u4e2a\u51fd\u6570\u5982\u679c\u5728\u4e24\u4e2a\u4e0d\u540c\u7684core\u4e0a\u6267\u884c\uff0c\u5047\u8bbecore1\u6267\u884cfoo\uff0ccore2\u6267\u884cbar\uff0ca\u4e0d\u5728core1\u7684cache\u4e2d\uff0cb\u5728core1\u7684cache\u4e2d\uff0c\u4e14b\u5bf9\u5e94\u7684cacheline\u72b6\u6001\u662fM\u3002 NOTE: \u4e24\u4e2athread\u5206\u522b\u6267\u884cfoo\u3001bar \u90a3\u4e48core1\u6267\u884ca=1\u7684\u65f6\u5019\u4f1a\u5c06a=1\u653e\u5230storebuffer\u4e2d\uff0c\u7136\u540e\u518d\u6267\u884cb=1\uff0c\u56e0\u4e3ab\u5728core1\u4e0a\u662fM\u72b6\u6001\uff0c\u6240\u4ee5\u4fee\u6539b\u4e0d\u9700\u8981\u4e0e\u5176\u4ed6core\u8fdb\u884c\u540c\u6b65\uff0cb\u7684\u4fee\u6539\u76f4\u63a5\u5c31\u5728cacheline\u4e2d\u8fdb\u884c\u4e86\uff0c\u6240\u4ee5\u4e5f\u4e0d\u4f1a\u8fdbstorebuffer\u3002\u8fd9\u65f6\u5019core2\u6267\u884cwhile(b==0)\u5224\u65ad\u7684\u65f6\u5019\u53d1\u73b0b=1\u4e86\uff0c\u90a3\u4e48\u5c31\u4f1a\u8fdb\u5165\u5230assert\uff0c\u4f46\u8fd9\u4e2a\u65f6\u5019\u5982\u679ca=1\u7684storebuffer\u8fd8\u6ca1\u6709\u66f4\u65b0core1\u4e2d\u7684a\u7684cacheline\u7684\u8bdd\uff0ccore2\u83b7\u5f97\u7684a\u7684\u503c\u4e3a0\uff0c\u90a3\u4e48\u8fd9\u4e2a\u65f6\u5019\u7ed3\u679c\u4e5f\u662f\u4e0d\u7b26\u5408\u9884\u671f\u7684\u3002 NOTE: a\u4e0d\u662fcore1 owner\u7684\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u5165store buffer \u4f46\u662f\u5728CPU\u8bbe\u8ba1\u5c42\u9762\u662f\u65e0\u6cd5\u5224\u65ad\u5f53\u524dcore\u4e2d\u6267\u884c\u7684\u53d8\u91cf\u662f\u5426\u4e0e\u5176\u4ed6\u7684core\u4e2d\u7684\u53d8\u91cf\u5b58\u5728\u5173\u7cfb\uff0c\u56e0\u4e3aCPU\u5728\u6267\u884c\u4ee3\u7801\u7684\u65f6\u5019\u4ed6\u8ba4\u4e3a\u8fd9\u4e2a\u5f53\u524d\u6240\u6267\u884c\u7a0b\u5e8f\u5c31\u662f\u4e00\u4e2a\u5355\u7ebf\u7a0b\u7684\uff0c\u4ed6\u65e0\u6cd5\u611f\u77e5\u591a\u7ebf\u7a0b\u7684\u5b58\u5728\u3002\u56e0\u6b64\u8fd9\u4e2a\u95ee\u9898\u65e0\u6cd5\u5728CPU\u8bbe\u8ba1\u5c42\u9762\u89e3\u51b3\uff0c\u8fd9\u4e2a\u5c31\u9700\u8981\u7f16\u7801\u4eba\u5458\u4ecb\u5165\u4e86\uff0c\u7f16\u7801\u4eba\u5458\u9700\u8981\u544a\u8bc9CPU\u73b0\u5728\u9700\u8981\u5c06storebuffer flush\u5230cache\u91cc\uff0c\u4e8e\u662fCPU\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u53eb**memory barrier**\u7684\u5de5\u5177\u3002 void foo ( void ) { a = 1 ; smp_mb (); // memory barrier b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; assert ( a == 1 ); } smp_mb()\u4f1a\u5728\u6267\u884c\u7684\u65f6\u5019\u5c06storebuffer\u4e2d\u7684\u6570\u636e\u5168\u90e8\u5237\u8fdbcache\u3002\u8fd9\u6837assert\u5c31\u4f1a\u6267\u884c\u6210\u529f\u4e86\u3002 2.4.2 invalidate message queue storebuffer\u5e2e\u52a9core\u5728\u8fdb\u884cstore\u64cd\u4f5c\u7684\u65f6\u5019\u5c3d\u5feb\u8fd4\u56de\uff0c\u8fd9\u91cc\u7684buffer\u548ccache\u90fd\u662f\u786c\u4ef6\u5143\u7d20\uff0c\u6240\u4ee5\u8fd9\u4e9bbuffer\u4e00\u822c\u90fd\u6bd4\u8f83\u5c0f\uff08\u6216\u8bb8\u5c31\u53ea\u6709\u51e0\u5341\u4e2a\u5b57\u8282\u8fd9\u4e48\u5927\uff09\uff0c\u5f53storebuffer\u6ee1\u4e86\u4e4b\u540e\u5c31\u9700\u8981\u5c06buffer\u4e2d\u7684\u5185\u5bb9\u5237\u5230cache\uff0c\u5237\u5230cache\u5c31\u4f1a\u89e6\u53d1cacheline\u7684invalidate message\uff0c\u8fd9\u4e9bmessage\u4f1a\u4e00\u8d77\u53d1\u9001\u7ed9\u5176\u4ed6\u7684core\uff0c\u7136\u540e\u7b49\u5230\u5176\u4ed6\u7684core\u8fd4\u56deinvalidate ack\u4e4b\u540e\u624d\u80fd\u7ee7\u7eed\u5411\u4e0b\u6267\u884c\u3002\u90a3\u4e48\u8fd9\u4e2a\u65f6\u5019\u95ee\u9898\u53c8\u6765\u4e86\uff0c\u8fd9\u4e9bmessage\u53d1\u5230\u53e6\u5916\u7684core\uff0c\u8fd9\u4e9bcore\u9700\u8981\u5148invlidate\uff0c\u7136\u540e\u5728\u8fd4\u56deack\uff0c\u5982\u679c\u8fd9\u4e9bcore\u672c\u6765\u5c31\u5f88\u5fd9\u7684\u8bdd\u5c31\u4f1a\u5bfc\u81f4message\u5904\u7406\u88ab\u5ef6\u540e\uff0c\u8fd9\u5bf9\u4e8eCPU\u8bbe\u8ba1\u8005\u6765\u8bf4\u540c\u6837\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002\u56e0\u6b64CPU\u8bbe\u8ba1\u8005\u53c8\u5f15\u5165\u4e86invalidate queue\u3002\u5982\u4e0b\u56fe[4]. \u540c\u6837\uff0c\u7c7b\u4f3c\u4e8estorebuffer\uff0c\u6709\u4e86invalidate queue\u4e4b\u540e\uff0c\u53d1\u9001\u7684invalidate message\u53ea\u9700\u8981push\u5230\u5bf9\u5e94core\u7684invalidate queue\u5373\u53ef\uff0c\u7136\u540e\u8fd9\u4e2acore\u5c31\u4f1a\u8fd4\u56de\u5bf9\u5e94\u7684invalidate ack\u4e86\uff0c\u4e2d\u95f4\u4e0d\u9700\u8981\u7b49\u5f85\u3002\u8fd9\u6837cache\u4e4b\u95f4\u7684\u6c9f\u901a\u5c31\u4e0d\u4f1a\u6709\u5f88\u5927\u7684\u963b\u585e\u4e86\uff0c\u4f46\u662f\u8fd9\u540c\u6837\u5e26\u6765\u4e86\u95ee\u9898\u3002 1\u3001\u8003\u8651\u4ee5\u4e0b\u573a\u666f \u8fd9\u4e2a\u4f8b\u5b50\u4e0estore buffer\u4e2d\u7684\u4f8b\u5b50\u4e00\u6837\u3002 void foo ( void ) { a = 1 ; smp_mb (); // memory barrier b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; assert ( a == 1 ); } core1\u6267\u884cfoo\uff0ccore2\u6267\u884cbar\uff0c\u540c\u6837\u5f53core1\u6267\u884c\u5230smp_mb()\u4f1a\u5c06storebuffer\u4e2d\u7684\u6570\u636e\u5168\u90e8\u5237\u5230cache\uff0c\u7136\u540ecache\u4f1a\u5411core2\u53d1\u9001invalidate message\uff0c\u8fd9\u4e2amessage\u4f1apush\u5230core2\u7684invalidate queue, \u7136\u540e\u6267\u884cb=1\u4e4b\u540e\uff0ccore2\u7684bar\u5224\u65adb==0\u5931\u8d25\u7136\u540e\u6267\u884cassert\uff0c\u4f46\u662f\u8fd9\u65f6\u5019\u5982\u679ccore2\u4e2da\u7684cacheline\u4e0d\u4e3aI\uff0c\u4e14invalidate queue\u8fd8\u6ca1\u6709\u5237\u5230core2\u7684cache\uff0c\u8fd9\u65f6\u5019assert\u8fd8\u4f1a\u5931\u8d25\u3002\u8fd9\u4e5f\u662f\u4e0d\u7b26\u5408\u7a0b\u5e8f\u8bed\u610f\u7684\u3002\u4f46\u662f\u540c\u6837core2\u5e76\u4e0d\u77e5\u9053\u5f53\u524d\u6267\u884c\u7684a\u4e0e\u5176\u4ed6core\u4e2d\u7684\u53d8\u91cf\u6709\u4ec0\u4e48\u5173\u7cfb\uff0cCPU\u8bbe\u8ba1\u5c42\u9762\u4f9d\u7136\u4e0d\u80fd\u76f4\u63a5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u4e8e\u662f\u8fd8\u662f\u9700\u8981\u52a0\u4e0amemory_barrier. void foo ( void ) { a = 1 ; smp_mb (); // memory barrier b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; smp_mb (); // memory_barrier assert ( a == 1 ); } \u540c\u6837\uff0c\u5728core2\u4e2d\u6dfb\u52a0smp_mb()\uff0c\u8fd9\u4e2amemory_barrier\u4f1a\u5c06\u6240\u5728core\u7684storebuffer\u548cinvalidate queue\u90fd flush\u3002 2.5 memory barrier \u4e0a\u9762\u4e24\u5c0f\u8282\u4e2d\u53ef\u5df2\u770b\u51famemory barrier\u7684\u4f5c\u7528\uff0c\u4e0a\u9762\u63d0\u5230\u7684smp_mb()\u662f\u4e00\u79cdfull memory barrier\uff0c\u4ed6\u4f1a\u5c06store buffer\u548cinvalidate queue\u90fdflush\u4e00\u904d\u3002\u4f46\u662f\u5c31\u50cf\u4e0a\u9762\u4f8b\u5b50\u4e2d\u4f53\u73b0\u7684\u90a3\u6837\u6709\u65f6\u5019\u6211\u4eec\u4e0d\u7528\u4e24\u4e2a\u90fdflush\uff0c\u4e8e\u662f\u786c\u4ef6\u8bbe\u8ba1\u8005\u5f15\u5165\u4e86**read memory barrier**\u548c**write memory barrier**\u3002 read memory barrier\u4f1a\u5c06invalidate queue flush\u3002 write memory barrier\u4f1a\u5c06storebuffer flush\u3002 NOTE: \u4e3a\u4ec0\u4e48\u53eb\u505aread\u3001write\uff1f write\u662f\u6307\u5c06storebuffer\u4e2d\u7684\u5185\u5bb9write\u5230 cache\u4e2d read\u662f\u6307cache\u8bfb\u53d6invalid queue \u4e2d\u7684\u5185\u5bb9\uff0c\u5176\u5b9e\u8054\u7cfb\u4e0a\u8ff0\u56fe\u5c31\u53ef\u4ee5\u77e5\u9053 2.6 \u603b\u7ed3 \u4ece\u4e0a\u51e0\u8282\u8ba8\u8bba\u53ef\u4ee5\u770b\u51fa\uff0ccache coherence\u4e3b\u8981\u96c6\u4e2d\u5728\u5bf9\u4e00\u4e2amemory location\u7684\u4e00\u81f4\u6027\u4fdd\u8bc1\uff0cMESI\u534f\u8bae\u662f\u5bf9\u540c\u4e00\u4e2amemory location\u5728\u4e0d\u540ccache\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u4fdd\u8bc1\u3002 3 memory consistency[3] memory model, memory model consistency, memory consistency\u4e09\u4e2a\u6307\u7684\u662f\u540c\u4e00\u4e2a\u4e1c\u897f\u3002 memory consistency\u662f\u4ec0\u4e48[8]\uff1a \\1. The guarantees provided by the runtime environment to a multithreaded program, regarding the order of memory operations. \\2. Each level of the environment might have a different memory model \u2013 CPU, virtual machine, language. \\3. The correctness of parallel algorithms depends on the memory model. \u8fd9\u8282\u6211\u4eec\u8ba8\u8bba\u7684\u662fCPU memory model\u3002 3.1 consistency\u5206\u7c7b \u4e00\u81f4\u6027\u4ece\u5f3a\u5230\u5f31\u6709\u591a\u79cd\u5206\u7c7b\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5e38\u7528lineraziblity consistency\uff08\u7ebf\u6027\u4e00\u81f4\u6027\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5728\u5bf9\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u4e00\u81f4\u6027\u4fdd\u8bc1\u4e2d\u53c8\u6709**Serializability** consistency\u8fd9\u4e5f\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u65f6\u53c8\u6709sequence consistency\uff08SC\uff09\u4e00\u81f4\u6027\u6a21\u578b\uff0csequence consistency\u4e5f\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u4e0d\u8fc7\u6bd4\u7ebf\u6027\u4e00\u81f4\u6027\u7a0d\u5f31\u3002\u5f53\u7136\u8fd8\u6709\u82e5\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u6bd4\u5982\u5185\u5b58\u4e00\u81f4\u6027\u4e2d\u7684relaxed memory consistency model\u3002 \u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u65f6\uff0c\u4e00\u81f4\u6027\u8bbe\u8ba1\u4e3b\u8981\u662f\u5728SC\u548cXC\u7684\u4e00\u79cd\u8bbe\u8ba1\uff0c\u4e3a\u4ec0\u4e48\u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u65f6\u4e0d\u662f\u5bf9lineraziblity consistency\u7684\u5b9e\u73b0\u800c\u662f\u5bf9\u76f8\u6bd4\u8f83\u8f83\u5f31\u7684SC\u6216\u8005XC\u7684\u5b9e\u73b0\uff1f\u8fd9\u4e2a\u5728\u672c\u8282\u672b\u5c3e\u4f1a\u8c08\u4e00\u4e0b\u4e2a\u4eba\u7684\u89c2\u70b9\u3002 \u672c\u6587\u4e3b\u8981\u662f\u5bf9memory consistency\u7684\u4e00\u81f4\u6027\u6a21\u578b\u7684\u603b\u7ed3\uff0c\u6240\u4ee5\u5bf9lineraziblity\u548c**Serializability** consistency\u4e0d\u4f1a\u6d89\u53ca\u5f88\u591a\u3002 3.2 Sequence Consistency 3.2.0 reorder \u5728\u8c08memory consistency\u4e4b\u524d\u6211\u4eec\u5148\u4e86\u89e3\u4e0breorder\uff0c\u6211\u4eec\u7f16\u7801\u5e76\u53d1\u5e03\u8fd0\u884c\u9700\u8981\u7ecf\u8fc7\u7f16\u8bd1\u5668\u7f16\u8bd1\u540e\u7136\u540e\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u901a\u5e38\u6211\u4eec\u8ba4\u4e3a\u6211\u4eec\u6240\u5199\u7684\u4ee3\u7801\u662f\u6309\u7167\u987a\u5e8f\u6267\u884c\u4e0b\u53bb\u7684\uff0c\u5c31\u662f\u8bf4\u4e0a\u4e00\u4e2a\u8bed\u53e5\u4e00\u5b9a\u5728\u4e0b\u4e00\u4e2a\u8bed\u53e5\u6267\u884c\u4e4b\u524d\u6267\u884c\u3002\u8fd9\u662f\u6211\u4eec\u7684\u6f5c\u610f\u8bc6\uff0c\u7136\u800c\u4e8b\u5b9e\u53ef\u80fd\u5e76\u4e0d\u662f\u8fd9\u6837\uff0c\u56e0\u4e3a\u4e2d\u95f4\u7ecf\u8fc7\u4e86\u7f16\u8bd1\u5668\u4e5f\u7ecf\u8fc7\u4e86CPU\u3002\u7f16\u8bd1\u5668\u548cCPU\u4e3a\u4e86\u5145\u5206\u63d0\u9ad8\u7a0b\u5e8f\u8fd0\u884c\u6027\u80fd\u4f1a\u5728\u5185\u90e8\u8fdb\u884c\u4e00\u7cfb\u5217\u4f18\u5316\uff0c\u8fd9\u4e9b\u4f18\u5316\u65b9\u6cd5\u6709\u5f88\u591a\uff0c\u4e5f\u5f88\u590d\u6742\u3002\u6bd4\u8f83\u5178\u578b\u7684\u6709reorder\uff0cSpeculative execution\u7b49\u3002\u7f16\u8bd1\u5668\u4f1a\u5bf9\u6211\u4eec\u5199\u7684\u4ee3\u7801\u987a\u5e8f\u8fdb\u884creorder\uff0cCPU\u6267\u884c\u7684\u65f6\u5019\u4e5f\u4f1a\u8fdb\u884creorder\uff0c\u4e5f\u5c31\u662f\u8bf4\u5728\u6267\u884c\u65f6\uff0c\u6211\u4eec\u5199\u7684\u4ee3\u7801\u5e76\u4e0d\u662f\u4e00\u5b9a\u6309\u7167\u6211\u4eec\u6240\u770b\u5230\u987a\u5e8f\u3002\u4f46\u662f\u4e0d\u7528\u62c5\u5fc3\uff0cCPU\u6216\u8005\u7f16\u8bd1\u5668\u5728reorder\u7684\u65f6\u5019\u5e76\u4e0d\u4f1a\u65e0\u5398\u5934\u7684reorder\uff0c\u4ed6\u4eec\u81f3\u5c11\u8981\u4fdd\u8bc1\u7684\u662f\uff0c\u5728reorder\u4e4b\u540e\uff0c\u7a0b\u5e8f\u6240\u8868\u73b0\u51fa\u6765\u7684\u884c\u4e3a\u6548\u679c\u4e0e\u5355\u7ebf\u7a0b\u6267\u884c\u6548\u679c\u662f\u4e00\u81f4\u7684\u3002\u8fd9\u91cc\u63d0\u5230\u7684\u662f\u5355\u7ebf\u7a0b\uff0c\u4e5f\u5c31\u662f\u8bf4CPU\u548c\u7f16\u8bd1\u5668\u5e76\u4e0d\u80fd\u611f\u77e5\u9053\u4f60\u7684\u4ee3\u7801\u662f\u591a\u7ebf\u7a0b\u8fd8\u662f\u5355\u7ebf\u7a0b\uff0c\u4ed6\u53ea\u80fd\u4fdd\u8bc1\u5355\u7ebf\u7a0b\u72b6\u51b5\u65f6\u6b63\u786e\u7684\uff0c\u591a\u7ebf\u7a0b\u5c31\u4e0d\u5f97\u800c\u77e5\u4e86\u3002 \u6240\u4ee5 *C++ and the Perils of Double-Checked Locking.*\u8fd9\u7bc7\u8bba\u6587\u7684\u4f5c\u8005\u6709\u8fd9\u4e48\u4e00\u53e5\u8bdd\uff1a\"Multithreading is just one damn thing after, before, or simultaneous with another. \"\u3002 \u5176\u5b9e\u86ee\u6709\u9053\u7406\u7684\u3002 \u90a3\u4e48\u65e2\u7136CPU\u548ccompiler\u4e0d\u80fd\u611f\u77e5**\u591a\u7ebf\u7a0b**\uff0c\u90a3\u4f1a\u51fa\u73b0\u4ec0\u4e48\u95ee\u9898\u5462\uff1f\u5982\u4e0b\u3002 NOTE: \u65e0\u6cd5\u8fdb\u884ccontrol 3.2.1 memory consistency motivation \u5728memory \u7cfb\u7edf\u8bbe\u8ba1\u65f6\u4e3a\u4ec0\u4e48\u9700\u8981memory consistency\u8fd9\u6837\u7684\u7ea6\u675f\uff1f\u8003\u8651\u4e0b\u9762\u8868\u683c\u9501\u63cf\u8ff0\u7684\u4e00\u79cd\u573a\u666f\u3002 core C1\u4e0e core C2\u662f\u4e24\u4e2a\u5355\u72ec\u7684core\uff0c\u5404\u81ea\u6267\u884c\u81ea\u5df1\u7684\u7a0b\u5e8f\uff0c\u5c31\u50cf\u8868\u683c\u6807\u9898r2\u4f1a\u4e00\u76f4\u88ab\u7f6e\u4e3aNEW\u5417\uff1f\u7b54\u6848\u5f53\u7136\u4e0d\u662f\uff0c\u9996\u5148CPU\u662f\u5b58\u5728out-of-order[6]\u4f18\u5316\u7684\uff0c\u53e6\u5916\u5bf9\u4e8ecore\u6765\u8bf4\uff0c\u4ed6\u5e76\u4e0d\u77e5\u9053\u81ea\u5df1\u5f53\u524d\u6267\u884c\u7684\u662f\u4e00\u4e2a\u591a\u7ebf\u7a0b\u7a0b\u5e8f\u3002\u4e00\u4e2aCPU core\u5728\u8bbe\u8ba1\u65f6\u81f3\u5c11\u9700\u8981\u4fdd\u8bc1\u5f53\u524dcore\u4e0a\u6267\u884c\u7684\u7a0b\u5e8f\u662f\u9075\u5faa\u5176program order\u7684(program order\u5c31\u662f\u4ee3\u7801\u987a\u5e8f)\u3002\u4f46\u662f\u4e3a\u4e86\u63d0\u9ad8\u6027\u80fd,CPU\u4f18\u5316\u5f15\u5165\u4e86out-of-order-excution\u673a\u5236\u548cSpeculative execution\u673a\u5236\uff0c\u90a3\u4e48\u5bf9\u4e8eC1\u800c\u8a00\uff0cCPU\u53ef\u4ee5\u6267\u884cS2->S1,\u4e5f\u53ef\u4ee5\u6267\u884cS1->S2, \u5bf9\u4e8e\u8fd9\u4e24\u79cd\u6267\u884c\u65b9\u5f0f\u5728C1\u770b\u6765\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u5355\u7ebf\u7a0b\u800c\u8a00\u8fd9\u4e24\u79cd\u6267\u884c\u65b9\u5f0f\u6700\u540e\u8fbe\u5230\u7684\u6548\u679c\u662f\u4e00\u6837\u7684\u3002\uff08\u56e0\u4e3aS2\u548cS1\u662f\u5bf9\u4e0d\u540c\u7684memory location\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5\u4f1areorder\uff0c\u5982\u679c\u662f\u5bf9\u540c\u4e00\u4e2amemory location\u64cd\u4f5c\u662f\u4e0d\u5141\u8bb8\u51fa\u73b0\u8fd9\u79cdreorder\u7684\uff0c\u5f53\u7136TSO\u5141\u8bb8\u8fd9\u79cdreorder\uff0c\u4f46\u662f\u5bf9\u540c\u4e00location\u800c\u8a00reorder\u524d\u540e\u6548\u679c\u4e00\u81f4\uff0c\u5177\u4f53\u4f1a\u5728\u540e\u9762\u7ae0\u8282\u8be6\u7ec6\u63cf\u8ff0\u3002\uff09 NOTE: \u5173\u4e8eTSO\uff0c\u53c2\u89c1 wikipedia Memory ordering : TSO Total store order (default) \u90a3\u4e48\u5982\u679cC1\u7684\u6267\u884c\u987a\u5e8f\u662fS2->L1->L2->S1\uff0c\u90a3\u4e48\u5f97\u5230\u7684\u7ed3\u679cr2 = 0\uff0c\u800c\u4e0d\u662f\u5411\u6211\u4eec\u9884\u671f\u7684r2 = NEW\u3002\u5bf9\u6211\u4eec\u800c\u8a00\u8fd9\u79cd\u7ed3\u679c\u662f\u8d85\u51fa\u9884\u671f\u7684\uff0c\u662f\u9519\u7684\u3002 \u90a3\u4e48\u4e3a\u4e86\u4fdd\u8bc1\u4e0d\u4f1a\u51fa\u73b0\u8fd9\u79cd\u8d85\u51fa\u9884\u671f\u7684\u884c\u4e3a\uff0c\u6211\u4eec\u5c31\u9700\u8981\u4e00\u79cd\u89c4\u5219\u6765\u7ea6\u675f\u8fd9\u79cd\u884c\u4e3a\u4e0d\u80fd\u51fa\u73b0\u3002\u8fd9\u4e2a\u4efb\u52a1\u5c31\u662fmemory consistency\u9700\u8981\u4fdd\u8bc1\u7684\uff08\u8fd9\u91cc\u6307\u7684\u662f\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff1aSC(sequence consistency)/TSO\uff0c XC\u7684memory consistency\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u8fd9\u70b9\uff09\u3002 3.2.2 program order vs memory order \u5728\u5f15\u5165SC\u5b9a\u4e49\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u660e\u786e\u4ec0\u4e48\u662fprogram order\u4ec0\u4e48\u662fmemory order\u3002\u8fd9\u91cc\u6211\u4eec\u8c08\u7684\u90fd\u662f\u9488\u5bf9\u5171\u4eab\u5185\u5b58\u7684\u64cd\u4f5c\uff0c\u5bf9\u90a3\u4e9b\u5c40\u90e8\u53d8\u91cf\uff0c\u5bf9core\u6765\u8bf4\u662f\u79c1\u6709\u7684\uff0c\u4e0d\u5b58\u5728\u5171\u4eab\uff0c\u6240\u4ee5\u4e0d\u4f1a\u5b58\u5728consistency\u7684\u95ee\u9898\u3002 NOTE: shared data program order: \u5c31\u662f\u6211\u4eec\u5199\u7684\u4ee3\u7801\u7684\u987a\u5e8f\uff0c\u8fd9\u4e2a\u662f\u9759\u6001\u7684\u4e5f\u662f\u6bcf\u4e2aCPU core\u5404\u81ea\u62e5\u6709\u7684\u3002 memory order: \u5c31\u662f\u4ee3\u7801\u6267\u884c\u7684\u987a\u5e8f\uff0c\u8fd9\u4e2a\u662f\u5168\u5c40\u7684\uff0c\u6bcf\u4e2aCPU core\u5bf9\u5171\u4eab\u5185\u5b58\u7684\u6267\u884c\u90fd\u4f1a\u51fa\u73b0\u5728memory order\u4e2d\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6bcf\u4e2acore\u7684\u4ee3\u7801\u90fd\u4f1a\u5bf9\u5e94\u5230memory order\u8fd9\u6761\u6267\u884c\u7ebf\u4e0a\u3002 3.2.3 SC definition Lamport\u5f15\u5165\u4e86\u5bf9SC\u7684\u5b9a\u4e49\uff0c\u4ed6\u8fd9\u6837\u5b9a\u4e49multiprocessor\u4e0b\u7684SC: \"the result of any execution is the same as if the operations of all processors (cores) were executed in some sequential order, and the operations of each individual processor (core) appear in this sequence in the order specified by its program.\"[3] \u5982\u4e0a\u56fe\u4e2d\uff0cS1 \u4e0e S2\u7684program order\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a S1 <p S2, S1\u4e0eL2\u53ef\u4ee5\u8868\u793a\u4e3a S1 <m L2. \u7528<p \u8868\u793aprogram order\u7684\u5148\u4e8e\u987a\u5e8f\uff0c<m\u8868\u793amemory order\u7684\u5148\u4e8e\u987a\u5e8f\u3002 \u90a3\u4e48SC\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u5982\u4e0b[3]\uff1a (1) All cores insert their loads and stores into the order <m respecting their program order, regardless of whether they are to the same or different addresses (i.e., a=b or a=\u0338b). There are four cases: If L(a) <p L(b) \u21d2 L(a) <m L(b) /* Load\u2192Load */ If L(a) <p S(b) \u21d2 L(a) <m S(b) /* Load\u2192Store */ ***If S(a) <p S(b) \u21d2 S(a) <m S(b) /* Store\u2192Store */** ***If S(a) <p L(b) \u21d2 S(a) <m L(b) /* Store\u2192Load */** (\u6240\u6709\u5bf9\u5171\u4eab\u5185\u5b58\u7684\u64cd\u4f5c\u90fd\u53ef\u4ee5\u62bd\u8c61\u6210load(\u8bfb\u53d6)\u548cstore(\u5199\u5165)\uff0c\u6bcf\u4e00core\u6267\u884cload\u548cstore\u662f\u6309\u7167\u5176program order\uff0c\u90a3\u4e48\u5c31\u6709S1 <p S2\u80af\u5b9a\u4f1a\u63a8\u51fa S1 <m S2\uff0cSC\u7684\u5b9a\u4e49\u4e5f\u7531\u6b64\u5f15\u5165\u4e86load\u548cstore\u7684\u56db\u79cd\u5173\u7cfb\u3002\u5728SC\u7684\u5b9a\u4e49\u4e2d\u8fd9\u56db\u79cd\u5173\u7cfb\u662f\u4e0d\u5141\u8bb8\u88abreorder\u7684\uff0c\u5373\u4f7f\u662f\u5bf9\u4e0d\u540cmemory location\u7684\u64cd\u4f5c\u3002) (2) Every load gets its value from the last store before it (in global memory order) to the same address: Value of L(a) = Value of MAX <m {S(a) | S(a) <m L(a)}, where MAX <m denotes \u201clatest in memory order.\u201d \uff08SC\u4e2d\u5b9a\u4e49\u6bcf\u4e2a\u5171\u4eab\u5185\u5b58\u7684\u8bfb\u53d6\u80af\u5b9a\u662f\u5176\u5728memory order\u4e2d\u6700\u8fd1\u7684\u4e00\u6b21\u5199\u5165\u7684\u503c\uff09\u3002 \u53ea\u8981\u7b26\u5408\u4e0a\u8ff0\u4e24\u4e2a\u6761\u4ef6\uff0c\u90a3\u4e48\u6211\u4eec\u5c31\u53ef\u4ee5\u8bf4\u8fd9\u4e2amemory\u64cd\u4f5c\u662f\u7b26\u5408\u987a\u5e8f\u4e00\u81f4\u6027\u7684\u3002 \u4e0b\u8868\u7ed9\u51fa\u4e86SC\u7684\u5b9a\u4e49\u4e2d\u7ea6\u675f\u7684\u884c\u4e3a\u3002operation1 <p operation2\u3002 3.2.4 SC example \u8fd8\u662f\u75283.2.1\u8282\u4e2d\u7684\u4f8b\u5b50\uff0c\u6765\u9a8c\u8bc1\u4e00\u4e0bSC\u5bf9memory\u64cd\u4f5c\u7684\u7ea6\u675f\u3002 \u7a0b\u5e8fmemory operation\u5982\u4e0b \u7a0b\u5e8f\u53ef\u80fd\u51fa\u73b0\u7684SC\u6267\u884c\u7ed3\u679c \u4e0b\u56fe\u4e2d\u524d\u4e09\u79cd\u7ed3\u679c\u662f\u7b26\u5408SC\u5b9a\u4e49\u7684\uff0c\u6700\u540e\u4e00\u79cd\u662f\u4e0d\u7b26\u5408SC\u5b9a\u4e49\u7684\uff0c\u53e6\u5916\u4e00\u65b9\u9762\u53ef\u4ee5\u770b\u51faSC\u4e2d\u662f\u4e0d\u5141\u8bb8\u6267\u884c\u7ebf\uff08\u865a\u7ebf\uff09\u4ea4\u53c9\u7684\u3002\u6bcf\u4e2a\u6267\u884c\u8bed\u53e5\u843d\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u7684\u4f4d\u7f6e\u8868\u793a\u5f53\u524d\u7684\u6267\u884c\u5df2\u7ecf\u53d1\u751f\u4e86\u3002\uff08\u5728lineraziblity consistency\u5b9a\u4e49\u4e2d\u6bcf\u4e2a\u64cd\u4f5c\u4f1a\u5212\u5206\u4e3ainvocation\u548cresponse\u4e24\u4e2a\u9636\u6bb5\uff0c\u4e14\u64cd\u4f5c\u4f1a\u5728\u8fd9\u4e24\u4e2a\u9636\u6bb5\u4e2d\u95f4\u4efb\u4e00\u4e2a\u77ac\u95f4\u53d1\u751f\uff0c\u8fd9\u91cc\u8bed\u53e5\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u7684\u843d\u70b9\u5c31\u662f\u8be5\u64cd\u4f5c\u6267\u884c\u7684\u77ac\u95f4\u3002\uff09 \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\u5728\u4fdd\u8bc1program order\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0cmemory order\u7684\u987a\u5e8f\u53ef\u4ee5\u968f\u610f\u6392\u5217\u3002\u8fd9\u4e2a\u7279\u522b\u50cf\u6211\u4eec\u5e73\u5e38\u6d17\u724c\u7684\u8fc7\u7a0b\uff0c\u4e24\u526f\u6251\u514b\u724c\u4e92\u76f8\u4ea4\u53e0\uff0c\u4f46\u662f\u6bcf\u526f\u724c\u7684\u987a\u5e8f\uff08program order\uff09\u662f\u4e0d\u53d8\u7684\uff0c\u4f46\u662f\u5728\u6d17\u5b8c\u4e4b\u540e\u4e24\u526f\u724c\u5408\u6210\u4e00\u526f\u724c\u7684\u65f6\u5019\u8fd9\u4e2a\u987a\u5e8f\u662f\u968f\u673a\u7684\uff0c\u4e0d\u786e\u5b9a\u3002 \u5728\u7b26\u5408SC\u5b9a\u4e49\u4e0b\u7684\u6267\u884c\u4e2d\u4f1a\u51fa\u73b0\u591a\u79cd\u6b63\u786e\u7684\u7ed3\u679c\uff0c\u8fd9\u4e9b\u7ed3\u679c\u7b26\u5408SC\u5b9a\u4e49\uff0c\u5728\u7f16\u7801\u4eba\u5458\u5f00\u6765\uff0c\u8fd9\u6837\u4e00\u4e2a\u591a\u7ebf\u7a0b\u7a0b\u5e8f\u6267\u884c\u7684\u7ed3\u679c\u4e5f\u662f\u6b63\u786e\u7684\u3002\u5e76\u53d1\u5728\u4e0d\u5f3a\u52a0\u5e72\u6d89\u7684\u60c5\u51b5\u4e0b\u662f\u4e0d\u80fd\u9884\u6d4b\u6267\u884c\u987a\u5e8f\u7684\u3002 3.3 TSO 3.3.0 SC\u5e26\u6765\u7684\u95ee\u9898 SC\u4e25\u683c\u5b9a\u4e49\u4e86\u5bf9\u4e8e\u5171\u4eab\u5185\u5b58\u7684load\u548cstore\u64cd\u4f5c\uff0cloadload\uff0cstorestore\uff0cloadstore\uff0cstoreload\u56db\u79cd\u6267\u884c\u987a\u5e8f\u662f\u4e0d\u5141\u8bb8reorder\u7684\u3002\u5f53\u4e0bCPU\u7684\u6267\u884c\u901f\u5ea6\u5df2\u7ecf\u7529DRAM\uff08memory\uff09\u597d\u51e0\u4e2a\u91cf\u7ea7\uff0c\u5982\u679c\u6bcf\u6b21store\uff0cload\u64cd\u4f5c\u90fd\u4eceDRAM\u8bfb\u53d6\u4f1a\u62d6\u6162CPU\u7684\u6267\u884c\u901f\u5ea6\uff0c\u5728\u8fd9\u4e2a\u6781\u5ea6\u538b\u69a8\u786c\u4ef6\u6027\u80fd\u7684\u65f6\u4ee3\uff0c\u662f\u4e0d\u80fd\u63a5\u53d7\u8fd9\u79cd\u884c\u4e3a\u7684\u3002\u56e0\u6b64\u5728x86\u7684\u67b6\u6784\u5b9e\u73b0\u4e2d\u5f15\u5165\u4e86TSO\u3002 However, current commercial compilers and most current commercial hardware do not preserve sequential consistency. [9] 3.3.1 TSO\u5982\u4f55\u89e3\u51b3SC\u7684\u95ee\u9898 TSO\u5168\u79f0Total Store Order\uff0c\u6211\u4eec\u5728\u8ba8\u8bbaTSO\u7684\u65f6\u5019\u5148\u5ffd\u7565cache\u8fd9\u4e00\u5c42\u3002 TSO\u5728CPU\u4e0ememory\u4e4b\u95f4\u5f15\u5165\u4e86write buffer\u3002CPU\u5199\u5165\u7684\u65f6\u5019\u5148\u5199\u5165write buffer\u7136\u540e\u5c31\u8fd4\u56de\u4e86\uff0c\u8fd9\u6837\u5c31\u5c06cpu\u4e0ememory\u4e4b\u95f4\u7684\u5dee\u8ddd\u9690\u85cf\u4e86\uff0c\u4f46\u662f\u8fd9\u6837\u540c\u6837\u5e26\u6765\u4e86\u4e00\u4e2a\u95ee\u9898\u3002 \u8fd8\u662f\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\uff0cS1\u5c06x=NEW\u653e\u5230\u4e86core C1\u7684write buffer\u4e2d\uff0cS2\u5c06y=NEW\u653e\u5230\u4e86C2\u7684write buffer\u4e2d\uff0c\u90a3\u4e48\u5728\u6267\u884cL1,L2\u7684\u65f6\u5019\uff0cr1\u4e0er2\u8fd9\u65f6\u5019\u4ecememory\u8bfb\u5230\u662f0\u3002\u8fd9\u4e2a\u662f\u8fdd\u80cc\u4e86SC\u7684\uff0c\u4f46\u662f\u8fd9\u6837\u7684\u8bbe\u8ba1\u786e\u5b9e\u5e26\u6765\u4e86\u6027\u80fd\u7684\u63d0\u5347\u3002 \u90a3\u4e48\u5728TSO\u6a21\u578b\u4e0b\u7684\u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff1a \u524d\u4e09\u79cd\u4e0eSC\u4e00\u81f4\uff0c\u7b2c\u56db\u4e2a\u6267\u884c\u7ed3\u679c\u5219\u662fTSO\u72ec\u6709\u7684\uff0c\u53ef\u4ee5\u770b\u51fa\uff0cTSO\u4e2d\u5141\u8bb8\u6267\u884c\u7ebf\u4ea4\u53c9\u3002 3.3.2 TSO definition TSO\u5728\u5b9e\u73b0SC\u7684\u8fc7\u7a0b\u4e2d\u505a\u4e86\u4e00\u4e9b\u6539\u52a8\uff0c\u5e26\u6765\u4e86\u6027\u80fd\u63d0\u5347\u3002SC\u662fTSO\u7684\u4e00\u4e9b\u7279\u4f8b\uff0cx86\u901a\u8fc7\u5728\u6bcf\u4e2acore\u4e0a\u5f15\u5165FIFO \u7684write buffer\u5b9e\u73b0\u4e86TSO[3]\u3002 TSO\u4e0eSC\u7684\u5b9a\u4e49\u5f88\u50cf\uff0c\u5176\u5b9a\u4e49\u5982\u4e0b\uff1a \uff081\uff09 All cores insert their loads and stores into the memory order <m respecting their program order, regardless of whether they are to the same or different addresses (i.e., a==b or a!=b). There are four cases: If L(a) <p L(b) \u21d2 L(a) <m L(b) /* Load\u2192Load */ If L(a) <p S(b) \u21d2 L(a) <m S(b) /* Load\u2192Store */ If S(a) <p S(b) \u21d2 S(a) <m S(b) /* Store\u2192Store */ // If S(a) S(a) Load*/ *//\u8fd9\u4e2a\u662fSC\u7684\uff0cTSO\u6ca1\u6709* \uff082\uff09Every load gets its value from the last store before it to the same address: // Value of L(a) = Value of MAX <m {S(a) | S(a) <m L(a)} *// \u8fd9\u4e2a\u662fSC\u7684\uff0c TSO\u6ca1\u6709* Value of L(a) = Value of MAX <m {S(a) | S(a) <m L(a) or S(a) <p L(a)} TSO\u7684\u5b9a\u4e49\u4e0eSC\u7684\u5b9a\u4e49\u6709\u4e24\u4e2a\u53d8\u5316\uff1a \u53d8\u5316\u4e00\uff1a \u4e0d\u4fdd\u8bc1storeload\u987a\u5e8f \u4e3e\u4e2a\u4f8b\u5b50\uff1aCore C1\u4e2dS1\u548cL1\uff0c S1\u5148\u53bbL1\u6267\u884c\uff0c\u4f46\u662fS1\u53ea\u662f\u5c06\u503c\u9001\u5165\u4e86write buffer\u5c31\u8fd4\u56de\u4e86\uff0c\u7d27\u63a5\u7740\u6267\u884cL1\uff0cL1\u5728memory order\u4e2d\u7684\u70b9\u6267\u884c\u5b8c\u4e4b\u540e\uff0cS1\u7684write buffer\u8fd9\u65f6\u5019flush\u5230\u5185\u5b58\uff0c\u90a3\u4e48S1\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u771f\u6b63\u6267\u884c\u7684\u70b9\u5728L1\u4e4b\u540e\u4e86\uff0c\u90a3\u4e48\u8fd9\u65f6\u5019S1\u4e0eL1\u5c31\u51fa\u73b0\u4e86reorder\u4e86\u3002 \u53d8\u5316\u4e8c\uff1a load\u7684\u6700\u65b0\u503c\u4e0d\u4e00\u5b9a\u662fmemory order\u4e2d\u6700\u8fd1\u7684\uff0c\u6709\u53ef\u80fd\u662fprogram order\u6700\u8fd1\u7684store \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u65e0\u8bba\u662fTSO\u8fd8\u662fSC\u90fd\u9700\u8981\u81f3\u5c11\u4fdd\u8bc1\u4e00\u70b9\uff0c\u5373\u4f7f\u5141\u8bb8reorder\u4e5f\u8981\u4fdd\u8bc1program\u6267\u884c\u7684\u7ed3\u679c\u4e0e\u5355\u7ebf\u7a0b\u6267\u884c\u7684\u7ed3\u679c\u662f\u4e00\u81f4\u7684\u3002\u6bd4\u5982\u4e00\u5bf9\u64cd\u4f5c\uff1a S1: x = new L1: y = x. \u65e0\u8bba\u662fTSO\u8fd8\u662fSC\u90fd\u9700\u8981\u4fdd\u8bc1y\u8bfb\u5230\u7684\u662fx=new\u7684\u503c\uff08\u6392\u9664\u5176\u4ed6\u7ebf\u7a0b\u5728\u8fd9\u4e24\u4e2a\u8bed\u53e5\u4e4b\u524d\u5bf9x\u8fdb\u884cstore\u64cd\u4f5c\u3002\uff09 \u56e0\u4e3aTSO\u5f15\u5165\u4e86write buffer\uff0c\u90a3\u4e48\u4e0a\u8ff0x=new\u4f1a\u5199\u5165buffer\uff0c\u5982\u4f55\u786e\u4fddL1\u4f1a\u8bfb\u5230\u6700\u65b0\u7684\u503c\u5462\uff0cTSO\u5f15\u5165\u4e86\u4e00\u79cd\u53eb\u201cbypass\u201d\u7684\u6982\u5ff5\uff0c\u5c31\u662f\u5bf9\u4e8e**\u540c\u4e00memory location**\u7684\u8bfb\u5199\u4f1a\u4fdd\u969cload\u4f1a\u8bfb\u5230store\u7684\u6700\u65b0\u503c\u65e0\u8bba\u8fd9\u4e2astore\u4f1a\u4e0d\u4f1a\u8fdb\u5165write buffer\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1a L1\u8bfb\u53d6\u7684\u662fS1\u7684\u503c\uff0c\u5373\u4f7fL1 <m S1 \u4e14 S1 <p L1. 3.3.3 FENCE 3.3.1\u5c0f\u8282\u63d0\u5230\u7684\u4f8b\u5b50\u4e2d\uff0c\u51fa\u73b0\u4e86\u4e00\u79cd\u8d85\u51fa\u9884\u671f\u7684\u6267\u884c\u72b6\u51b5\uff0c\u5982\u679c\u6211\u4eec\u60f3\u907f\u514d\u8fd9\u79cd\u95ee\u9898\uff0c\u90a3\u4e48\u9700\u8981\u5728\u4e0a\u5c42\u4ee3\u7801\u4e2d\u6dfb\u52a0FENCE\uff0c\u8fd9\u4e2afence\u53ef\u4ee5\u7406\u89e3\u4e3amemory barrier\uff0c\u4ed6\u7684\u4f5c\u7528\u662f\u5c06write buffer\u4e2d\u7684\u8bb0\u5f55flush\u5230\u5185\u5b58\u3002 FENCE\u4f1a\u5f3a\u5236\u4fdd\u8bc1program order\u3002 If S(a) <p FENCE \u21d2 S(a) <m FENCE /* Store \u2192 FENCE */ If FENCE <p L(a) \u21d2 FENCE <m L(a) /* FENCE \u2192 Load */ \u5982\u679c\u518dS1\u4e0eL1\u4e4b\u95f4\u52a0\u4e0aFENCE\uff0c\u5c31\u4fdd\u8bc1\u4e86S1 <p L1 \u548c S1 <m L1. x86\u7cfb\u7edf\u4e2d\u5e76\u6ca1\u6709\u4e3a\u6211\u4eec\u52a0\u4e0aFENCE\uff0c\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u6dfb\u52a0FENCE\u53ea\u6709\u5f00\u53d1\u4eba\u5458\u77e5\u9053\uff0c\u6240\u4ee5\u4e3a\u4e86\u907f\u514d\u7a0b\u5e8f\u51fa\u73b0\u83ab\u540d\u5176\u5999\u7684\u9519\u8bef\uff0c\u8bb0\u5f97\u5728store\u548cload\u5171\u4eab\u5185\u5b58\u7684\u65f6\u5019\u52a0\u4e0aFENCE\u3002 \u56e0\u6b64TSO\u4e0b\u7684operation order\u5982\u4e0b: (operation1 <p operation2) 3.4 relaxed memory consistency SC\u548cTSO\u4e25\u683c\u610f\u4e49\u4e0a\u6765\u8bf4\u90fd\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u56e0\u4e3a\u4ed6\u4eec\u90fd\u5bf9\u7a0b\u5e8f\u7684\u6267\u884c\u987a\u5e8f\u505a\u4e86\u4e00\u5b9a\u7684\u7ea6\u675f\uff0c\u65e2\u7136\u5b58\u5728\u7ea6\u675f\u90a3\u4e48\u5c31\u4f1a\u5e26\u6765\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u8017\u3002 \u90a3\u4e48\u6709\u6ca1\u6709\u4e00\u79cd\u6ca1\u8fd9\u4e48\u591a\u7684\u7ea6\u675f\u7684\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u80fd\u591f\u4f7f\u673a\u5668\u8fdb\u884c\u6df1\u5ea6\u7684\u4f18\u5316\u5e76\u53d1\u6325\u6781\u81f4\u6027\u80fd\u3002\u90a3\u4e48\u6267\u884c\u987a\u5e8f\u7684\u6b63\u786e\u6027\u5c31\u53ea\u80fd\u6709\u7f16\u7801\u4eba\u5458\u6765\u4fdd\u8bc1\u4e86\u3002 relaxed memory consistency\u5b9e\u73b0\u5bf9\u4e8eload\u4e0estore\u987a\u5e8f\u5b8c\u5168\u653e\u5f00\uff0c\u9664\u4e86\u5bf9\u540c\u4e00memory location\u7684\u64cd\u4f5c\u4fdd\u8bc1load\u770b\u5230\u662f\u6700\u65b0\u7684store\u4ee5\u5916\u5176\u4ed6\u90fd\u4e0d\u8fdb\u884c\u7ea6\u675f\uff0c\u7f16\u7801\u4eba\u5458\u5982\u679c\u60f3\u5f3a\u52a0order\u53ef\u4ee5\u901a\u8fc7\u4e0a\u8ff0\u7684FENCE\u3002 3.5 Linearizability consistency **Linearizability**\u662f\u6bd4SC\u66f4\u5f3a\u7684\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5728SC\u5b9a\u4e49\u4e2d\u4e0d\u540ccore\u7684\u6267\u884c\u8bed\u53e5\u5728memory order\u7684\u65f6\u95f4\u7ebf\u4e2d\u53ef\u4ee5\u968f\u610f\u63d2\u5165\uff0c\u800c\u5bf9\u4e8eLinearizability consistency\u4e0d\u4ec5\u9650\u5236\u4e86\u5355\u7ebf\u7a0b\u4e2d\u7684\u6267\u884c\u987a\u5e8f\uff0c\u540c\u65f6\u5bf9\u4e8e\u591a\u7ebf\u7a0b\u4e2d\u6267\u884c\u987a\u5e8f\u4e5f\u505a\u4e86\u4e00\u4e9b\u9650\u5236\u3002\u5177\u4f53 Linearizability consistency\u7684\u76f8\u5173\u77e5\u8bc6\u5728\u540e\u9762\u6587\u7ae0\u518d\u8be6\u7ec6\u603b\u7ed3\u4e00\u4e0b\u3002 3.6 memory consistency\u603b\u7ed3 1\u3001\u4e3a\u4ec0\u4e48memory consistency\u53ea\u8c08sequence consistency(SC)\u800c\u4e0d\u8c08linearizability(LIN) \u4e00\u65b9\u9762\uff0c\u56e0\u4e3aSC\u5728\u5e95\u5c42\u5df2\u7ecf\u5b9a\u4e49\u4e00\u4e2a\u591a\u7ebf\u7a0b\u80fd\u591f\u5bf9\u5171\u4eab\u5185\u5b58\u5e76\u53d1\u64cd\u4f5c\u7684\u6b63\u786e\u6027\uff0cLIN\u6bd4SC\u66f4\u4e25\u683c\uff0cLIN\u662fSC\u7684\u4e00\u79cd\u5f62\u5f0f\uff0cLIN\u66f4\u8d34\u8fd1\u4e0e\u4e0a\u5c42\uff0c\u5728SC\u63d0\u4f9b\u7684\u591a\u79cd\u6b63\u786e\u6027\u7684\u6267\u884c\u5e8f\u5217\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u7b26\u5408\u6211\u4eec\u4e1a\u52a1\u903b\u8f91\u6b63\u786e\u6027\u7684\u4fdd\u969c\u624b\u6bb5\uff0c\u800cLIN\u5c31\u662f\u8fd9\u79cd\u624b\u6bb5\u3002 \u53e6\u4e00\u65b9\u9762\u662f\uff0c\u5728\u5e95\u5c42\u7cfb\u7edf\u4e2dCPU\u548c\u7f16\u8bd1\u5668\u662f\u4e0d\u80fd\u610f\u8bc6\u5230\u591a\u7ebf\u7a0b\u7684\u5b58\u5728\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4CPU\u53ea\u662f \u77e5\u9053\u5f53\u524d\u7684\u6307\u4ee4\uff0c\u5e76\u4e0d\u77e5\u9053\u5f53\u524d\u6307\u4ee4\u4e0e\u5176\u4ed6CPU\u6307\u4ee4\u4e4b\u95f4\u7684\u5173\u7cfb\uff0cLIN\u4e2d\u7ea6\u675f\u4e86\u4e24\u4e2aprocess\uff08\u4e5f\u5c31\u662f\u4e24\u4e2a\u6267\u884c\u4f53\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u7ebf\u7a0b\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u8fdb\u7a0b\u7b49\uff09\u4e4b\u95f4\u7684partial order\uff08\u504f\u5e8f\u5173\u7cfb\uff09\uff0c\u800c\u8fd9\u79cd\u5173\u7cfb\u5728\u5e95\u5c42\u662f\u4e0d\u5f97\u800c\u77e5\u7684\u3002 \u5728\u4e00\u4e2a\u539f\u56e0\uff0cLIN\u7684\u6210\u672c\u66f4\u9ad8\uff0c\u5982\u679c\u518dCPU\u5c42\u9762\u5b9e\u73b0LIN\uff0c\u90a3\u4e48\u539f\u672c\u7528\u6765\u63d0\u9ad8\u6027\u80fd\u7684cache\u548cstore buffer\u7684\u4f5c\u7528\u5c31\u4f1a\u88ab\u5927\u5927\u524a\u5f31\uff0c\u6240\u4ee5\u5728\u80fd\u591f\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u57fa\u7840\u4e0a\u53c8\u80fd\u5145\u5206\u5229\u7528CPU\u8d44\u6e90\u7684\u524d\u63d0\u4e0b\uff0cLIN\u5e76\u6ca1\u6709\u5728CPU\u5c42\u9762\u5b9e\u73b0\u3002 **SC**\u5728\u5b9a\u4e49\u4e2d\u6ca1\u6709\u7ea6\u5b9a\u6267\u884c\u65f6\u95f4\u8fd9\u4e2a\u6982\u5ff5\uff0c\u53ea\u662f\u5f3a\u8c03\u4e86program order\u4e0eexecution order\uff0c\u4e5f\u5c31\u662fmemory order\u3002 **Linearizability**\u5728\u5b9a\u4e49\u4e2d\u7ea6\u5b9a\u4e86\u4e24\u4e2a\u6267\u884c\u4f53\u4e4b\u95f4\u7684\u6267\u884c\u987a\u5e8f\uff0c\u4e5f\u5c31\u662f\u6267\u884c\u65f6\u95f4\u5148\u540e\u88ab\u7ea6\u5b9a\u4e86\u3002 2\u3001memory consistency\u4e0ecache coherence\u7684\u5173\u7cfb \u4ece\u4ee5\u4e0a\u8ba8\u8bba\u53ef\u4ee5\u770b\u51fa\uff0cmemory consistency\u6ce8\u91cd\u7684\u662f\u5168\u5c40\u7684memory order\uff0c\u800ccache coherence\u5219\u662f\u5173\u6ce8\u4e8e\u4e00\u4e2amemory location\u3002 memory consistency\u662f\u4fdd\u8bc1\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u6b63\u786e\u6027\uff0c\u6211\u4eec\u5728\u8ba8\u8bba\u8fd9\u4e2a\u7684\u65f6\u5019\u53ef\u4ee5\u628acache\u5f53\u505a\u4e00\u4e2a\u9ed1\u76d2\u5b50\u6765\u5904\u7406\uff0c\u4e5f\u5c31\u662f\u8bf4\u5373\u4f7f\u6ca1\u6709cache\uff0c\u6211\u4eec\u4e5f\u540c\u6837\u9700\u8981memory consistency\u6765\u4fdd\u8bc1\u6b63\u786e\u6027\u3002 3\u3001memory barrier\u4e0ememory order\u7684\u5173\u7cfb \u7b2c2\u8282\u4e2d\u6700\u540e\u63d0\u5230\u4e86memory barrier\uff0c\u8fd9\u4e2amemory barrier\u4f1aflush storebuffer\u548cinvalidate queue\u3002\u90a3\u4e48memory barrier\u4e0ememory order\u6709\u5565\u5173\u7cfb\u5462\u3002 memory order\u662f\u5168\u5c40\u7684\u591a\u5904\u7406\u5668\u5bf9\u5171\u4eab\u53d8\u91cf\u7684\u64cd\u4f5c\u7684\u4e00\u4e2a\u6392\u5e8f\u3002\u5bf9\u5171\u4eab\u53d8\u91cf\u64cd\u4f5c\u843d\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u7684\u70b9\u5c31\u662f\u5bf9\u5916\u8868\u73b0\u51fa\u7684\u6267\u884c\u987a\u5e8f\u3002 memory barrier\u4f1a\u5c06storebuffer\u4e2d\u7684\u5185\u5bb9flush\u5230cache\u7136\u540ecache\uff0c\u90a3\u4e48\u53ea\u6709\u6267\u884c\u8fd9\u4e2amemory barrier\u4e4b\u540e\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u624d\u7b97\u771f\u6b63\u7684\u6267\u884c\uff0c\u624d\u80fd\u771f\u6b63\u843d\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u3002\u6240\u4ee5memory barrier\u8d77\u5230\u4e00\u4e2aorder\u7684\u4f5c\u7528\uff0c\u8fd9\u4e2aorder\u662f\u4e00\u4e2a\u52a8\u8bcd\uff0c\u5c31\u662f\u4ec0\u4e48\u65f6\u5019\u771f\u6b63\u6267\u884c\u8fd9\u4e2a\u52a8\u4f5c\u3002 4 \u7bc7\u5e45\u9650\u5236\uff0c\u540e\u7eed\u90e8\u5206\u653e\u5728\u540e\u4e00\u7bc7 7 References M. Mizuno, M. Raynal, J.Z. Zhou. Sequential Consistency in Distributed Systems. Scott Meyers and Andrei Alexandrescu. C++ and the Perils of Double-Checked Locking. Daniel J. Sorin, Mark D. Hill, and David A. Wood. A Primer on Memory Consistency and Cache Coherence. Paul E. McKenney . Memory Barriers: a Hardware View for Software Hackers. C++ memory order out of order execution The New C++: Lay down your guns, knives, and clubs c++ memory model H. Boehm, S. V. Adve. Foundations of the C++ Concurrency Memory Model C++ Standard - 2012-01-16 - Working Draft (N3337).pdf CPU Cache and Memory Ordering think cell talk memory model Acquire and Release Semantics C++ Memory model","title":1},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#zhuanlanzhihu#--","text":"NOTE: 1\u3001\u8fd9\u7bc7\u6587\u7ae0\u5199\u5f97\u975e\u5e38\u597d \"Multithreading is just one damn thing after, before, or simultaneous with another. \"[2]","title":"zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0a)"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#0","text":"\u5728\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u4e86\u63d0\u9ad8**\u53ef\u9760\u6027**\uff0c\u901a\u5e38\u4f1a\u5f15\u5165\u591a\u4e2a\u526f\u672c\uff0c\u591a\u4e2a\u526f\u672c\u9700\u8981\u5411\u7528\u6237\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u5185\u5bb9\u3002\u8fd9\u5f88\u81ea\u7136\u7684\u8ba9\u4eba\u60f3\u5230\u5982\u4f55\u786e\u4fdd\u591a\u526f\u672c\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u4e3a\u6b64\u4e5f\u6709\u4e86paxos\u548craft\u7b49\u4fdd\u8bc1\u591a\u526f\u672c\u4e4b\u95f4\u4e00\u81f4\u6027\u7684\u534f\u8bae\u3002\u5f53\u6211\u4eec\u5728\u4e00\u4e2a\u591a\u5904\u7406\u5668\u673a\u5668\u4e0a\u7f16\u7a0b\u65f6\u6211\u4eec\u901a\u5e38\u4f1a\u5ffd\u7565\u5728\u591a\u5904\u7406\u5668\u73af\u5883\u4e0b\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u7cfb\u7edf\u5df2\u7ecf\u4e3a\u6211\u4eec\u505a\u597d\u4e86\u57fa\u672c\u7684**\u4e00\u81f4\u6027**\u4fdd\u8bc1\uff0c\u5f53\u5b58\u5728\u4e00\u81f4\u6027\u95ee\u9898\u7684\u65f6\u5019\u4e0a\u5c42\u7f16\u7a0b\u8bed\u8a00\u4e5f\u63d0\u4f9b\u4e86\u5177\u5907\u4e00\u81f4\u6027\u8bed\u610f\u7684\u63a5\u53e3\uff0c\u53ea\u662f\u6211\u4eec\u5728\u7f16\u7a0b\u4e2d\u5e76\u6ca1\u6709\u610f\u8bc6\u5230\u8fd9\u4e9b**\u63a5\u53e3**\u4e0e**\u4e00\u81f4\u6027**\u7684\u5173\u7cfb\u3002\u65e0\u8bba\u662f\u5206\u5e03\u5f0f\u5b58\u50a8\u8fd8\u662f\u591a\u5904\u7406\u5668\u7f16\u7a0b\u90fd\u6709\u4e00\u4e2a\u5171\u540c\u70b9\uff0c\u5c31\u662f\u4f1a\u6d89\u53ca\u5171\u4eab\u5bf9\u8c61\u7684\u64cd\u4f5c\u3002 NOTE: 1\u3001\"\u53ef\u9760\u6027\"\uff0c\u5176\u5b9e\u5c31\u662fHA \"\u591a\u4e2a\u526f\u672c\"\uff0c\u5176\u5b9e\u5c31\u662fmaster-slave \"\u4e00\u81f4\u7684\u5185\u5bb9\"\uff0c\u5176\u5b9e\u5c31\u662fconsistency \"\u5171\u4eab\u5bf9\u8c61\"\u5176\u5b9e\u5c31\u662fmultiple model\u4e2d\u7684shared data 2\u3001\u4e0a\u8ff0\u5176\u5b9e\u53ef\u4ee5\u4f7f\u7528multiple model-shared data\u6765\u8fdb\u884c\u63cf\u8ff0 \u4e00\u65e6\u51fa\u73b0\u5171\u4eab\uff0c\u5c31\u4f1a\u51fa\u73b0\u6b63\u786e\u6027\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u5982\u4f55\u5b9a\u4e49\u5728\u5e76\u53d1\u4e2d\u64cd\u4f5c\u5171\u4eab\u5bf9\u8c61\u7684\u6b63\u786e\u6027\uff0c\u8fd9\u5c31\u662f\u4e00\u81f4\u6027\u534f\u8bae\u7684\u4efb\u52a1\u4e86\u3002 \u672c\u6587\u4e3b\u8981\u9488\u5bf9\u591a\u5904\u7406\u5668\u7cfb\u7edf\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u8fdb\u884c\u4e86\u4e00\u4e9b\u603b\u7ed3\uff0c\u5bf9\u4e8e\u5206\u5e03\u5f0f\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u4f1a\u5728\u540e\u9762\u6587\u7ae0\u4e2d\u603b\u7ed3\u3002 \u591a\u5904\u7406\u5668\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u6e90\u4e8e\u5e76\u53d1\uff0c\u6e90\u4e8e\u5171\u4eab\u3002\u5bf9\u4e8e\u5171\u4eab\u5185\u5b58\u5e76\u53d1\u64cd\u4f5c\u4e0b\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\u662f\u786c\u4ef6\u8bbe\u8ba1\u8005\u9700\u8981\u63d0\u4f9b\u7ed9\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6700\u91cd\u8981\u7684\u4fdd\u8bc1\u3002\u5bf9\u4e8e\u4e0a\u5c42\u5f00\u53d1\u4eba\u5458\u6765\u8bf4\uff0c\u7cfb\u7edf\u5185\u90e8\u7684\u4e00\u81f4\u6027\u662f\u900f\u660e\u7684\uff0c\u4f46\u662f\u53bb\u4e86\u89e3\u7cfb\u7edf\u5185\u90e8\u4e00\u81f4\u6027\u7684\u8bbe\u8ba1\u53ca\u539f\u7406\u6709\u5229\u4e8e\u6211\u4eec\u66f4\u80fd\u591f\u9762\u5411\u673a\u5668\u7f16\u7a0b\uff0c\u5199\u51fa\u6b63\u786e\u7684\u4ee3\u7801\u3002 \u7531\u4e8e\u6c34\u5e73\u6709\u9650\uff0c\u6587\u4e2d\u5982\u5b58\u5728\u95ee\u9898\uff0c\u8bf7\u5e2e\u5fd9\u63d0\u51fa\u3002","title":"0 \u5199\u5728\u524d\u9762"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#1","text":"\u672c\u6587\u4e3b\u8981\u5305\u62ec\u4ee5\u4e0b\u5185\u5bb9\uff1a 1\u3001cache coherence cache design cache coherence protocol store buffer & invalidate message queue memory barrier/ fence 2\u3001memory consistency consistency \u5206\u7c7b\u5b9a\u4e49 SC x86 TSO relaxed memory model 3\u3001C++ memory order memory model and memory order memory order release/acquire memory order sequence memory order relaxed 4\u3001Synchronize insight lock","title":"1 \u672c\u6587\u4e3b\u8981\u5185\u5bb9"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#2#cache#coherence4","text":"\u4e00\u81f4\u6027\u7684\u4efb\u52a1\u5c31\u662f\u4fdd\u8bc1\u5e76\u53d1\u64cd\u4f5c\u5171\u4eab\u5185\u5b58\u7684\u6b63\u786e\u6027\uff0c\u5728\u8ba8\u8bba\u5185\u5b58\u64cd\u4f5c\u6b63\u786e\u6027\u7684\u65f6\u5019\uff0c\u901a\u5e38\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1amemory consistency \u548c cache coherence[3]\u3002 \u672c\u8282\u7740\u91cd\u8ba8\u8bbacache coherence\u7684\u5b9e\u73b0\u3002","title":"2 cache coherence[4]"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#21#cache#vs#buffer","text":"\u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u901a\u5e38\u4f1a\u6709cache \u53ca buffer\u7684\u8bbe\u8ba1\uff0c\u4e24\u8005\u90fd\u6709\u7f13\u5b58\u7684\u610f\u601d\uff0c\u90a3\u4e48\u771f\u6b63\u7684\u533a\u522b\u5728\u54ea\u91cc\u5462\uff0c\u77e5\u4e4e\u4e0a\u6709\u4e2a\u56de\u7b54\u6211\u611f\u89c9\u633a\u597d\u7684\u3002 Cache \u548c Buffer \u90fd\u662f\u7f13\u5b58\uff0c\u4e3b\u8981\u533a\u522b\u662f\u4ec0\u4e48\uff1f Buffer \uff08\u7f13\u51b2\u533a\uff09\u662f\u7cfb\u7edf\u4e24\u7aef\u5904\u7406**\u901f\u5ea6\u5e73\u8861**\uff08\u4ece\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u770b\uff09\u65f6\u4f7f\u7528\u7684\u3002\u5b83\u7684\u5f15\u5165\u662f\u4e3a\u4e86\u51cf\u5c0f\u77ed\u671f\u5185\u7a81\u53d1I/O\u7684\u5f71\u54cd\uff0c\u8d77\u5230**\u6d41\u91cf\u6574\u5f62**\u7684\u4f5c\u7528\u3002\u6bd4\u5982\u751f\u4ea7\u8005\u2014\u2014\u6d88\u8d39\u8005\u95ee\u9898\uff0c\u4ed6\u4eec\u4ea7\u751f\u548c\u6d88\u8017\u8d44\u6e90\u7684\u901f\u5ea6\u5927\u4f53\u63a5\u8fd1\uff0c\u52a0\u4e00\u4e2abuffer\u53ef\u4ee5\u62b5\u6d88\u6389\u8d44\u6e90\u521a\u4ea7\u751f/\u6d88\u8017\u65f6\u7684\u7a81\u7136\u53d8\u5316\u3002 Cache \uff08\u7f13\u5b58\uff09\u5219\u662f\u7cfb\u7edf\u4e24\u7aef\u5904\u7406**\u901f\u5ea6\u4e0d\u5339\u914d**\u65f6\u7684\u4e00\u79cd**\u6298\u8877\u7b56\u7565**\u3002\u56e0\u4e3aCPU\u548cmemory\u4e4b\u95f4\u7684\u901f\u5ea6\u5dee\u5f02\u8d8a\u6765\u8d8a\u5927\uff0c\u6240\u4ee5\u4eba\u4eec\u5145\u5206\u5229\u7528\u6570\u636e\u7684\u5c40\u90e8\u6027\uff08locality\uff09\u7279\u5f81\uff0c\u901a\u8fc7\u4f7f\u7528\u5b58\u50a8\u7cfb\u7edf\u5206\u7ea7\uff08memory hierarchy\uff09\u7684\u7b56\u7565\u6765\u51cf\u5c0f\u8fd9\u79cd\u5dee\u5f02\u5e26\u6765\u7684\u5f71\u54cd\u3002","title":"2.1 cache vs buffer"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#22#cache","text":"\u4ececache vs buffer\u7684\u533a\u522b\u4e2d\u6211\u4eec\u53ef\u770b\u51fa\uff0ccache\u7684\u51fa\u73b0\u5176\u5b9e\u5c31\u662f\u4e3a\u4e86\u7f13\u89e3CPU\u4e0e\u4e3b\u5b58\u4e4b\u95f4\u7684\u901f\u5ea6\u6781\u5ea6\u4e0d\u5e73\u8861\u3002\u56e0\u6b64\u901a\u5e38CPU\u8bbe\u8ba1\u4e2d\u5f15\u5165\u4e86cache\uff0c\u5206\u4e3aL1 cache\uff0cL2 cache\u548cL3 cache\uff0c\u5176\u6027\u80fd\u6307\u6570\u5927\u81f4\u5982\u4e0b[11]\uff1a \u901a\u5e38L1 cache\u53c8\u4f1a\u5206\u4e3aI cache\u548cD cache\uff0c\u5206\u522b\u5b58\u653e\u6307\u4ee4\u4e0e\u6570\u636e\uff0cL1 cache\u901a\u5e38\u662fcore\u72ec\u5360\u7684\uff0cL2\u53ef\u4ee5\u662f\u4e24\u4e2acore\u5171\u4eab\uff0cL3 cache\u5219\u662f\u6240\u6709core\u5171\u4eab\u3002","title":"2.2 cache\u8bbe\u8ba1"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#220#cpu#cache","text":"\u5982\u4e0b\u56fe[4]\uff0c \u76ee\u524d\u7684CPU\u67b6\u6784\u4e2d\u7684cache\u901a\u5e38\u67b6\u6784\u5982\u6b64\uff0c\u8fd9\u91cc\u7684cache\u662fL1 cache\u3002\u6211\u4eec\u8fd9\u91cc\u8ba8\u8bba\u7684cache\u57fa\u672c\u90fd\u662f\u57fa\u4e8eL1 cache\u7684\uff0c\u56e0\u4e3aL1 cache\u901a\u5e38\u662fcore\u72ec\u5360\u7684\uff0c\u6240\u4ee5L1 cache\u7684\u8bbe\u8ba1\u53ca\u6570\u636e\u4e00\u81f4\u6027\u662f\u96be\u70b9\u4e5f\u662f\u91cd\u70b9\u3002 cache**\u4e2d\u7684\u6570\u636e\u5b58\u653e\u5355\u4f4d\u662f**cache line \uff0c\u901a\u5e38\u4e00\u4e2a**cache line**\u957f\u5ea6\u5927\u6982\u572816-256Bytes\u4e4b\u95f4\u3002CPU\u8bbf\u95eememory\u7684\u65f6\u5019\u9996\u5148\u4f1a\u5148\u67e5\u770bcache\u4e2d\u662f\u5426\u5df2\u7ecf\u7f13\u5b58\u8be5address\uff0c\u5982\u679c\u5df2\u7f13\u5b58\u5f53\u524daddress\u5e76\u4e14\u662f\u53ef\u7528\u7684\uff0c\u90a3\u4e48CPU\u5c31\u4f1a\u76f4\u63a5\u4ececache\u8bfb\u53d6\u8be5\u503c\uff0c\u8fd9\u6837\u6781\u5927\u7684\u63d0\u9ad8performance\u3002\u5982\u679c\u6240\u8981\u8bbf\u95ee\u7684address\u4e0d\u5728cache\u4e2d\uff0cCPU\u5219\u8981\u4ecememory\u8bfb\u53d6\uff0cCPU\u4f1a\u8bfb\u53d6cache line\u5927\u5c0f\u7684\u6570\u636e\uff0c\u5e76\u5c06\u5176\u653e\u5165cache\u4e2d\u8fd9\u6837\u4e0b\u6b21\u8bfb\u53d6\u7684\u65f6\u5019\u53ef\u4ee5\u76f4\u63a5\u4ececache\u4e2d\u8bfb\u3002L1 cache\u901a\u5e38\u572832KB\u5927\u5c0f\uff0c\u5f53cache\u88ab\u586b\u6ee1\u4e4b\u540e\u5c31\u9700\u8981\u6309\u7167\u4e00\u5b9a\u7b56\u7565\u5c06\u5143\u7d20\u8e22\u51fa\u53bb\uff0c\u8fd9\u4e2a\u7b56\u7565\u53ef\u4ee5\u662f**LRU**\u6216\u8005**\u968f\u673a**\u7b49\u7b49\u3002 NOTE: \u8fd9\u8ba9\u6211\u60f3\u5230\u4e86virtual memory\u7684page swap \u8fd9\u91cc\u6709\u4e24\u4e2a\u5c0f\u70b9\u9700\u8981\u6ce8\u610f\uff1a 1\u3001L1 cache\u72ec\u5360\u5bfc\u81f4\u7684\u6570\u636e\u4e0d\u4e00\u81f4 \u56e0\u4e3a\u6bcf\u4e2acore\u90fd\u6709\u81ea\u5df1\u72ec\u7acb\u7684L1 cache\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u5171\u4eab\u7684memory location\u4e24\u4e2acache\u53ef\u4ee5\u6709\u81ea\u5df1\u7684copy\u3002\u90a3\u4e48\u8fd9\u5c31\u4f1a\u51fa\u73b0\u4e86\u6570\u636e\u4e0d\u4e00\u81f4\u7684\u72b6\u51b5\uff0c\u4e24\u4e2acore\u53ef\u80fd\u540c\u65f6\u8bbf\u95ee\u8fd9\u4e2amemory location\uff0c\u4e14\u5982\u679c\u4e00\u4e2acore\u662f\u5bf9\u8fd9\u4e2amemory location\u8fdb\u884c\u4fee\u6539\uff0c\u90a3\u4e48\u8fd9\u5c31\u9700\u8981\u4e24\u8fb9\u7684cache\u8fdb\u884c\u540c\u6b65\uff0c\u9632\u6b62\u6570\u636e\u4e0d\u4e00\u81f4\u3002\u8fd9\u4e2a\u5de5\u4f5c\u5c31\u662f**cache coherence protocol**\u8981\u505a\u7684\u4e8b\u60c5\u3002 2\u3001cacheline\u5bfc\u81f4\u7684false sharing struct test { uint32_t a ; uint32_t b ; } test t ; t\u662f\u4e00\u4e2a\u5171\u4eab\u53d8\u91cf\uff0ccore1\u4e0a\u7ebf\u7a0bt1\u8bbf\u95ee t.a \uff0c core2\u7ebf\u7a0bt2\u8bbf\u95ee t.b ,\u5982\u679ccacheline\u5927\u5c0f\u662f16Bytes\uff0c\u90a3\u4e48\u5f53t1\u8bbf\u95eet.a\u65f6cache miss\uff0c\u7136\u540e\u4f1a\u5c06\u8fd9\u4e2at.a\u8bfb\u53d6\u5230cache\u4e2d\uff0c\u4f46\u662f\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0ccore\u6bcf\u6b21\u8bfb\u53d6\u7684\u957f\u5ea6\u662fcacheline\u7684\u957f\u5ea6\uff0ct.a\u957f\u5ea6\u662f4\uff0c t.b\u957f\u5ea6\u662f4\uff0c\u6240\u4ee5\u5728\u4e00\u6b21\u8bfb\u53d6\u4e2d\u4f1a\u5c06t.a\u548ct.b\u4e00\u6b21\u6027\u8bfb\u5230\u4e86\u81ea\u5df1cache\u4e2d\u4e86\uff0c\u90a3\u4e48t2\u4e5f\u540c\u6837\u5b58\u5728\u8fd9\u4e2a\u95ee\u9898\uff0c\u5982\u679ct1\u4fee\u6539\u5f53\u524dcacheline\u4e2d\u7684\u503c\uff0c\u4ed6\u9700\u8981\u4e0ecore2\u5bf9\u5e94\u7684cacheline\u540c\u6b65\uff0c\u90a3\u4e48\u672c\u6765\u4e24\u4e2a\u53ef\u4ee5\u5e76\u53d1\u7684memory location\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u3002\u5bf9\u6027\u80fd\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u5f71\u54cd\u3002 NOTE: \u4e0a\u8ff0\u95ee\u9898\u548cbyte assignment \u975e\u5e38\u7c7b\u4f3c \u90a3\u4e48\u4e3a\u4e86\u907f\u514dfalse sharing\uff0c\u5c31\u9700\u8981\u8ba9\u8fd9\u4e24\u4e2amemory location\u8fdb\u884ccacheline\u5bf9\u9f50\u3002C++ \u63d0\u4f9b\u4e86alignas\u7684\u65b9\u6cd5\u3002","title":"2.2.0 CPU cache\u67b6\u6784"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#23#cache#coherence#protocol","text":"\u524d\u9762\u5c0f\u8282\u63d0\u5230\u4e86\u4e3a\u4ec0\u4e48\u9700\u8981coherence protocol\uff0c\u90a3\u4e48coherence protocol\u662f\u5982\u4f55\u4fdd\u8bc1cache\u4e4b\u95f4\u7684\u6570\u636e\u4e00\u81f4\u6027\u7684\u5462\uff1f \u8fd9\u91cc\u5c31\u9700\u8981\u5f15\u5165MESI\u534f\u8bae\u4e86\uff0cMESI\u662f\u4e00\u79cd\u7ef4\u62a4cacheline\u72b6\u6001\u4e00\u81f4\u6027\u7684\u534f\u8bae\u3002MESI: \u7531Modified,Exclusive,Share,Invalid\u56db\u79cd\u72b6\u6001\u7ec4\u5408\u3002","title":"2.3 cache coherence protocol"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#231#cacheline#state","text":"1\u3001Modified(M): \u5f53\u4e00\u4e2acore \u7684cacheline\u7684\u72b6\u6001\u662fM\u65f6\uff0c\u8bf4\u660e\u5f53\u524dcore\u6700\u8fd1\u4fee\u6539\u4e86\u8fd9\u4e2acache\uff0c\u90a3\u4e48\u5176\u4ed6core\u7684cache\u4e0d\u80fd\u518d\u4fee\u6539\u5f53\u524dcache line\u5bf9\u5e94\u7684memory location\uff0c\u9664\u975e\u8be5cache\u5c06\u8fd9\u4e2a\u4fee\u6539\u540c\u6b65\u5230\u4e86memory\u3002\u8fd9\u4e2acore\u5bf9\u8fd9\u4e2amemory location\u53ef\u4ee5\u7406\u89e3\u4e3aOwned\u3002 NOTE: \u5176\u4ed6core\u7684cache\u5982\u4f55\u77e5\u9053\"\u4e0d\u80fd\u518d\u4fee\u6539\u5f53\u524dcache line\u5bf9\u5e94\u7684memory location\"\uff1f\u53c2\u89c1\u4e0b\u9762\u7684share\u3002 2\u3001Exclusive(E): NOTE: \"exclusive\"\u7684\u610f\u601d\u662f\u6392\u4ed6\u7684\u3001\u72ec\u6709\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u8fd9\u4e2acache\u4e2d\u7684memory location\u662f\u72ec\u6709\u7684\uff0c\u5176\u4ed6\u7684cache\u4e2d\u90fd\u4e0d\u5305\u542b\uff1b\u663e\u7136\u5b83\u548c\"share\"\u662f\u76f8\u53cd\u7684 E\u8fd9\u4e2a\u72b6\u6001\u4e0eM\u5f88\u50cf\uff0c\u533a\u522b\u5728\u4e8e\u5f53\u524dcore\u5e76\u6ca1\u6709\u4fee\u6539\u5f53\u524d\u7684cacheline\uff0c\u8fd9\u610f\u5473\u7740\u5f53\u524dcacheline\u5b58\u50a8\u7684memory location\u7684\u503c\u662f\u6700\u65b0\u7684\u3002\u5f53\u524dcore\u53ef\u4ee5\u5bf9\u8be5cacheline\u8fdb\u884cmodify\u4e14\u4e0d\u9700\u8981\u4e0e\u5176\u4ed6core\u7684cache\u540c\u6b65\u3002\u8fd9\u4e2acore\u5bf9\u8fd9\u4e2amemory location\u53ef\u4ee5\u7406\u89e3\u4e3aOwned\u3002 3\u3001Share(S): S\u8868\u793a\u5f53\u524dcacheline\u5728\u5176\u4ed6core\u7684cache\u4e5f\u5b58\u5728copy\uff0c\u5f53\u524dcore\u5982\u679c\u9700\u8981\u4fee\u6539\u8be5cacheline\u5219\u9700\u8981\u4e0e\u5176\u4ed6core\u7684cache\u8fdb\u884c\u63d0\u524d\u6c9f\u901a\u3002 4\u3001Invalid(I): I\u8868\u793a\u5f53\u524dcacheline\u662f\u7a7a\u7684\u3002 cacheline\u7684\u72b6\u6001\u53d8\u5316\u9700\u8981\u5728\u5404\u4e2acore\u4e4b\u95f4\u540c\u6b65\uff0c\u90a3\u4e48\u5982\u4f55\u8fdb\u884c\u540c\u6b65\u5462\uff0cMESI\u4f7f\u7528protocol message\u3002 NOTE: \u8fd9\u8ba9\u6211\u60f3\u5230\u4e86gossip protocol","title":"2.3.1 cacheline state"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#232#protocol#message","text":"\u4f5c\u4e3acore\u4e4b\u95f4\u7684\u6c9f\u901a\u5de5\u5177\uff0cprotocol message\u5206\u4e3a\u4ee5\u4e0b\u51e0\u79cd\u6d88\u606f\u7c7b\u578b\uff1a 1\u3001Read \u5f53\u4e00\u4e2acache\u9700\u8981\u8bfb\u53d6\u67d0\u4e2acacheline\u6d88\u606f\u7684\u65f6\u5019\u5c31\u4f1a\u53d1\u8d77read\u6d88\u606f\u3002 2\u3001Read Response read response\u662fread\u7684\u56de\u5e94\uff0c\u8fd9response\u53ef\u4ee5\u6765\u81ea\u5176\u4ed6core\u7684cache\u4e5f\u53ef\u4ee5\u6765\u81eamemory\u3002\u5f53\u5176\u4ed6core\u4e2d\u5bf9\u5f53\u524dcacheline\u662fM\u72b6\u6001\u65f6\uff0c\u5219\u4f1a\u53d1\u8d77read response\u3002 3\u3001Invalidate Invalidate\u6d88\u606f\u5305\u542b\u5bf9\u5e94\u7684memory location\uff0c\u63a5\u6536\u5230\u8fd9\u4e2a\u6d88\u606f\u7684cache\u9700\u8981\u5c06\u81ea\u5df1cacheline\u5185\u5bb9\u5254\u9664\uff0c\u5e76\u54cd\u5e94\u3002 4\u3001Invalidate acknowledge \u63a5\u6536\u5230Invalidate\u540e\u5220\u9664cacheline\u4e2d\u7684\u6570\u636e\u5c31\u5411\u53d1\u8d77\u8005\u56de\u590dinvalidate ack\u3002 5\u3001Read Invalidate \u8fd9\u4e2a\u6d88\u606f\u5305\u542b\u4e24\u4e2a\u64cd\u4f5c\uff0cread\u548cinvalidate\uff0c\u90a3\u4e48\u5b83\u4e5f\u9700\u8981\u63a5\u6536read response\u548c\u591a\u4e2ainvalidate ack\u54cd\u5e94\u3002 6\u3001write back writeback\u5305\u542b\u6570\u636e\u548c\u5730\u5740\uff0c\u4f1a\u5c06\u8fd9\u4e2a\u5730\u5740\u5bf9\u5e94\u7684\u6570\u636e\u5237\u5230\u5185\u5b58\u4e2d\u3002 NOTE: commit","title":"2.3.2 protocol message"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#24#store#buffer#and#invalidate#queue","text":"MESI\u4e2d\u7684\u56db\u79cd\u72b6\u6001\u53ef\u4ee5\u4e92\u76f8\u8f6c\u5316\uff0c\u5177\u4f53\u72b6\u6001\u8fc1\u79fb\u6761\u4ef6\u8bf7\u53c2\u8003[4]\uff0c\u8fd9\u91cc\u4e0d\u518d\u8d58\u8ff0\u3002","title":"2.4 store buffer and invalidate queue"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#241#store#buffer","text":"NOTE: \u5176\u5b9e\u5c31\u662f\u5c06write\u7684\u503c\uff0c\u653e\u5230\u7f13\u5b58\u4e2d CPU\u8bbe\u8ba1\u8005\u90fd\u662f\u5728\u6781\u81f4\u538b\u69a8\u5176\u6027\u80fd\u3002cache\u5c31\u662fCPU\u8bbe\u8ba1\u8005\u538b\u69a8CPU\u6027\u80fd\u7684\u4e00\u79cd\u4f53\u73b0\uff0c\u4f46\u662f\u8fd9\u6837\u4ed6\u4eec\u89c9\u5f97\u8fd8\u4e0d\u591f\u3002\u8003\u8651\u4e00\u79cd\u72b6\u51b5\uff0c\u5047\u8bbe\u5f53core1\u6267\u884c\u4e00\u6b21store\u64cd\u4f5c\uff0c\u4e14\u8fd9\u4e2astore\u64cd\u4f5c\u7684memory location\u5bf9\u5e94\u7684cacheline\u5728\u53e6\u5916\u4e00\u4e2acore2\u4e0a\u662fowned\u72b6\u6001\uff0c\u90a3\u4e48core1\u9700\u8981\u5411core2\u53d1\u9001Invalidate message\uff0c\u5e76\u4e14\u9700\u8981\u7b49\u5230core2\u8fd4\u56deinvalidate ack\u4e4b\u540e\u624d\u80fd\u7ee7\u7eed\u5411\u4e0b\u6267\u884c\uff0c\u90a3\u4e48\u5728\u8fd9\u4e2a\u671f\u95f4core1\u5c31\u5904\u4e8e**\u76f2\u7b49**\u9636\u6bb5\uff0c\u90a3\u4e48core1\u5fc5\u987b\u8981\u7b49\u8fd9\u4e48\u4e45\u5417\uff1f\u4e8e\u662fCPU\u8bbe\u8ba1\u8005\u5f15\u5165\u4e86store buffer\uff0c\u8fd9\u4e2abuffer\u5904\u4e8eCPU\u4e0ecache\u4e4b\u95f4\u3002\u5982\u4e0b\u56fe[4]. NOTE: core1\u6267\u884c\u4e86\u4fee\u6539\uff0c\u56e0\u6b64\u5176\u4ed6\u7684cache\u9700\u8981\u66f4\u65b0\u6700\u65b0\u503c\uff1b \u6709\u4e86store buffer\u4e4b\u540ecore1\u5982\u679c\u6267\u884cstore\u64cd\u4f5c\u5c31\u4e0d\u7528\u7acb\u523b\u5411core2\u53d1\u9001invalidate message\u4e86\uff0ccore1\u53ea\u9700\u8981\u5c06store\u503c\u6dfb\u52a0\u5230store buffer\u4e2d\u5373\u53ef\u3002\u4f46\u662f\u5f15\u5165store buffer\u4f1a\u5e26\u6765\u4e24\u4e2a\u95ee\u9898\u3002 1\u3001\u8003\u8651\u4ee5\u4e0b\u573a\u666f\uff1a // a, b init to 0. a=1; b = a + 1; assert(b == 2); \u4e0a\u8ff0\u4ee3\u7801\u4e2d\uff0ccore\u6267\u884ca=1\u540e\uff0ca=1\u8fd9\u4e2a\u503c\u88ab\u653e\u5230core\u7684storebuffer\u91cc\u4e86\uff0c\u7136\u540e\u7ee7\u7eed\u6267\u884cb=a+1,\u8fd9\u65f6\u5019core\u7684cacheline\u4e2d\u4fdd\u5b58\u7684a\u8fd8\u662f\u539f\u6765\u76840.\u8fd9\u4e2a\u65f6\u5019\u5c31\u4f1a\u5bfc\u81f4assert\u5931\u8d25\u3002\u56e0\u4e3a\u6211\u4eec\u5728storebuffer\u91cc\u548ccacheline\u4e2d\u7684a\u662f\u4e24\u4e2a\u72ec\u7acb\u7684\u62f7\u8d1d\uff0c\u6240\u4ee5\u5bfc\u81f4\u8fd9\u79cd\u9519\u8bef\u7684\u7ed3\u679c\uff0c\u56e0\u6b64CPU\u8bbe\u8ba1\u8005\u901a\u8fc7\u4f7f\u7528\"store Forwarding\"\u7684\u65b9\u5f0f\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u5728\u6267\u884cload\u64cd\u4f5c\u65f6\u5148\u53bbstorebuffer\u4e2d\u67e5\u627e\u5bf9\u5e94\u7684memory location\uff0c\u5982\u679c\u67e5\u5230\u5c31\u4f7f\u7528storebuffer\u4e2d\u7684\u6700\u65b0\u503c\u3002 2\u3001\u8003\u8651\u53e6\u5916\u4e00\u4e2a\u573a\u666f void foo ( void ) { a = 1 ; b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; assert ( a == 1 ); } foo\u548cbar\u4e24\u4e2a\u51fd\u6570\u5982\u679c\u5728\u4e24\u4e2a\u4e0d\u540c\u7684core\u4e0a\u6267\u884c\uff0c\u5047\u8bbecore1\u6267\u884cfoo\uff0ccore2\u6267\u884cbar\uff0ca\u4e0d\u5728core1\u7684cache\u4e2d\uff0cb\u5728core1\u7684cache\u4e2d\uff0c\u4e14b\u5bf9\u5e94\u7684cacheline\u72b6\u6001\u662fM\u3002 NOTE: \u4e24\u4e2athread\u5206\u522b\u6267\u884cfoo\u3001bar \u90a3\u4e48core1\u6267\u884ca=1\u7684\u65f6\u5019\u4f1a\u5c06a=1\u653e\u5230storebuffer\u4e2d\uff0c\u7136\u540e\u518d\u6267\u884cb=1\uff0c\u56e0\u4e3ab\u5728core1\u4e0a\u662fM\u72b6\u6001\uff0c\u6240\u4ee5\u4fee\u6539b\u4e0d\u9700\u8981\u4e0e\u5176\u4ed6core\u8fdb\u884c\u540c\u6b65\uff0cb\u7684\u4fee\u6539\u76f4\u63a5\u5c31\u5728cacheline\u4e2d\u8fdb\u884c\u4e86\uff0c\u6240\u4ee5\u4e5f\u4e0d\u4f1a\u8fdbstorebuffer\u3002\u8fd9\u65f6\u5019core2\u6267\u884cwhile(b==0)\u5224\u65ad\u7684\u65f6\u5019\u53d1\u73b0b=1\u4e86\uff0c\u90a3\u4e48\u5c31\u4f1a\u8fdb\u5165\u5230assert\uff0c\u4f46\u8fd9\u4e2a\u65f6\u5019\u5982\u679ca=1\u7684storebuffer\u8fd8\u6ca1\u6709\u66f4\u65b0core1\u4e2d\u7684a\u7684cacheline\u7684\u8bdd\uff0ccore2\u83b7\u5f97\u7684a\u7684\u503c\u4e3a0\uff0c\u90a3\u4e48\u8fd9\u4e2a\u65f6\u5019\u7ed3\u679c\u4e5f\u662f\u4e0d\u7b26\u5408\u9884\u671f\u7684\u3002 NOTE: a\u4e0d\u662fcore1 owner\u7684\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u5165store buffer \u4f46\u662f\u5728CPU\u8bbe\u8ba1\u5c42\u9762\u662f\u65e0\u6cd5\u5224\u65ad\u5f53\u524dcore\u4e2d\u6267\u884c\u7684\u53d8\u91cf\u662f\u5426\u4e0e\u5176\u4ed6\u7684core\u4e2d\u7684\u53d8\u91cf\u5b58\u5728\u5173\u7cfb\uff0c\u56e0\u4e3aCPU\u5728\u6267\u884c\u4ee3\u7801\u7684\u65f6\u5019\u4ed6\u8ba4\u4e3a\u8fd9\u4e2a\u5f53\u524d\u6240\u6267\u884c\u7a0b\u5e8f\u5c31\u662f\u4e00\u4e2a\u5355\u7ebf\u7a0b\u7684\uff0c\u4ed6\u65e0\u6cd5\u611f\u77e5\u591a\u7ebf\u7a0b\u7684\u5b58\u5728\u3002\u56e0\u6b64\u8fd9\u4e2a\u95ee\u9898\u65e0\u6cd5\u5728CPU\u8bbe\u8ba1\u5c42\u9762\u89e3\u51b3\uff0c\u8fd9\u4e2a\u5c31\u9700\u8981\u7f16\u7801\u4eba\u5458\u4ecb\u5165\u4e86\uff0c\u7f16\u7801\u4eba\u5458\u9700\u8981\u544a\u8bc9CPU\u73b0\u5728\u9700\u8981\u5c06storebuffer flush\u5230cache\u91cc\uff0c\u4e8e\u662fCPU\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u53eb**memory barrier**\u7684\u5de5\u5177\u3002 void foo ( void ) { a = 1 ; smp_mb (); // memory barrier b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; assert ( a == 1 ); } smp_mb()\u4f1a\u5728\u6267\u884c\u7684\u65f6\u5019\u5c06storebuffer\u4e2d\u7684\u6570\u636e\u5168\u90e8\u5237\u8fdbcache\u3002\u8fd9\u6837assert\u5c31\u4f1a\u6267\u884c\u6210\u529f\u4e86\u3002","title":"2.4.1 store buffer"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#242#invalidate#message#queue","text":"storebuffer\u5e2e\u52a9core\u5728\u8fdb\u884cstore\u64cd\u4f5c\u7684\u65f6\u5019\u5c3d\u5feb\u8fd4\u56de\uff0c\u8fd9\u91cc\u7684buffer\u548ccache\u90fd\u662f\u786c\u4ef6\u5143\u7d20\uff0c\u6240\u4ee5\u8fd9\u4e9bbuffer\u4e00\u822c\u90fd\u6bd4\u8f83\u5c0f\uff08\u6216\u8bb8\u5c31\u53ea\u6709\u51e0\u5341\u4e2a\u5b57\u8282\u8fd9\u4e48\u5927\uff09\uff0c\u5f53storebuffer\u6ee1\u4e86\u4e4b\u540e\u5c31\u9700\u8981\u5c06buffer\u4e2d\u7684\u5185\u5bb9\u5237\u5230cache\uff0c\u5237\u5230cache\u5c31\u4f1a\u89e6\u53d1cacheline\u7684invalidate message\uff0c\u8fd9\u4e9bmessage\u4f1a\u4e00\u8d77\u53d1\u9001\u7ed9\u5176\u4ed6\u7684core\uff0c\u7136\u540e\u7b49\u5230\u5176\u4ed6\u7684core\u8fd4\u56deinvalidate ack\u4e4b\u540e\u624d\u80fd\u7ee7\u7eed\u5411\u4e0b\u6267\u884c\u3002\u90a3\u4e48\u8fd9\u4e2a\u65f6\u5019\u95ee\u9898\u53c8\u6765\u4e86\uff0c\u8fd9\u4e9bmessage\u53d1\u5230\u53e6\u5916\u7684core\uff0c\u8fd9\u4e9bcore\u9700\u8981\u5148invlidate\uff0c\u7136\u540e\u5728\u8fd4\u56deack\uff0c\u5982\u679c\u8fd9\u4e9bcore\u672c\u6765\u5c31\u5f88\u5fd9\u7684\u8bdd\u5c31\u4f1a\u5bfc\u81f4message\u5904\u7406\u88ab\u5ef6\u540e\uff0c\u8fd9\u5bf9\u4e8eCPU\u8bbe\u8ba1\u8005\u6765\u8bf4\u540c\u6837\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002\u56e0\u6b64CPU\u8bbe\u8ba1\u8005\u53c8\u5f15\u5165\u4e86invalidate queue\u3002\u5982\u4e0b\u56fe[4]. \u540c\u6837\uff0c\u7c7b\u4f3c\u4e8estorebuffer\uff0c\u6709\u4e86invalidate queue\u4e4b\u540e\uff0c\u53d1\u9001\u7684invalidate message\u53ea\u9700\u8981push\u5230\u5bf9\u5e94core\u7684invalidate queue\u5373\u53ef\uff0c\u7136\u540e\u8fd9\u4e2acore\u5c31\u4f1a\u8fd4\u56de\u5bf9\u5e94\u7684invalidate ack\u4e86\uff0c\u4e2d\u95f4\u4e0d\u9700\u8981\u7b49\u5f85\u3002\u8fd9\u6837cache\u4e4b\u95f4\u7684\u6c9f\u901a\u5c31\u4e0d\u4f1a\u6709\u5f88\u5927\u7684\u963b\u585e\u4e86\uff0c\u4f46\u662f\u8fd9\u540c\u6837\u5e26\u6765\u4e86\u95ee\u9898\u3002 1\u3001\u8003\u8651\u4ee5\u4e0b\u573a\u666f \u8fd9\u4e2a\u4f8b\u5b50\u4e0estore buffer\u4e2d\u7684\u4f8b\u5b50\u4e00\u6837\u3002 void foo ( void ) { a = 1 ; smp_mb (); // memory barrier b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; assert ( a == 1 ); } core1\u6267\u884cfoo\uff0ccore2\u6267\u884cbar\uff0c\u540c\u6837\u5f53core1\u6267\u884c\u5230smp_mb()\u4f1a\u5c06storebuffer\u4e2d\u7684\u6570\u636e\u5168\u90e8\u5237\u5230cache\uff0c\u7136\u540ecache\u4f1a\u5411core2\u53d1\u9001invalidate message\uff0c\u8fd9\u4e2amessage\u4f1apush\u5230core2\u7684invalidate queue, \u7136\u540e\u6267\u884cb=1\u4e4b\u540e\uff0ccore2\u7684bar\u5224\u65adb==0\u5931\u8d25\u7136\u540e\u6267\u884cassert\uff0c\u4f46\u662f\u8fd9\u65f6\u5019\u5982\u679ccore2\u4e2da\u7684cacheline\u4e0d\u4e3aI\uff0c\u4e14invalidate queue\u8fd8\u6ca1\u6709\u5237\u5230core2\u7684cache\uff0c\u8fd9\u65f6\u5019assert\u8fd8\u4f1a\u5931\u8d25\u3002\u8fd9\u4e5f\u662f\u4e0d\u7b26\u5408\u7a0b\u5e8f\u8bed\u610f\u7684\u3002\u4f46\u662f\u540c\u6837core2\u5e76\u4e0d\u77e5\u9053\u5f53\u524d\u6267\u884c\u7684a\u4e0e\u5176\u4ed6core\u4e2d\u7684\u53d8\u91cf\u6709\u4ec0\u4e48\u5173\u7cfb\uff0cCPU\u8bbe\u8ba1\u5c42\u9762\u4f9d\u7136\u4e0d\u80fd\u76f4\u63a5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u4e8e\u662f\u8fd8\u662f\u9700\u8981\u52a0\u4e0amemory_barrier. void foo ( void ) { a = 1 ; smp_mb (); // memory barrier b = 1 ; } void bar ( void ) { while ( b == 0 ) continue ; smp_mb (); // memory_barrier assert ( a == 1 ); } \u540c\u6837\uff0c\u5728core2\u4e2d\u6dfb\u52a0smp_mb()\uff0c\u8fd9\u4e2amemory_barrier\u4f1a\u5c06\u6240\u5728core\u7684storebuffer\u548cinvalidate queue\u90fd flush\u3002","title":"2.4.2 invalidate message queue"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#25#memory#barrier","text":"\u4e0a\u9762\u4e24\u5c0f\u8282\u4e2d\u53ef\u5df2\u770b\u51famemory barrier\u7684\u4f5c\u7528\uff0c\u4e0a\u9762\u63d0\u5230\u7684smp_mb()\u662f\u4e00\u79cdfull memory barrier\uff0c\u4ed6\u4f1a\u5c06store buffer\u548cinvalidate queue\u90fdflush\u4e00\u904d\u3002\u4f46\u662f\u5c31\u50cf\u4e0a\u9762\u4f8b\u5b50\u4e2d\u4f53\u73b0\u7684\u90a3\u6837\u6709\u65f6\u5019\u6211\u4eec\u4e0d\u7528\u4e24\u4e2a\u90fdflush\uff0c\u4e8e\u662f\u786c\u4ef6\u8bbe\u8ba1\u8005\u5f15\u5165\u4e86**read memory barrier**\u548c**write memory barrier**\u3002 read memory barrier\u4f1a\u5c06invalidate queue flush\u3002 write memory barrier\u4f1a\u5c06storebuffer flush\u3002 NOTE: \u4e3a\u4ec0\u4e48\u53eb\u505aread\u3001write\uff1f write\u662f\u6307\u5c06storebuffer\u4e2d\u7684\u5185\u5bb9write\u5230 cache\u4e2d read\u662f\u6307cache\u8bfb\u53d6invalid queue \u4e2d\u7684\u5185\u5bb9\uff0c\u5176\u5b9e\u8054\u7cfb\u4e0a\u8ff0\u56fe\u5c31\u53ef\u4ee5\u77e5\u9053","title":"2.5 memory barrier"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#26","text":"\u4ece\u4e0a\u51e0\u8282\u8ba8\u8bba\u53ef\u4ee5\u770b\u51fa\uff0ccache coherence\u4e3b\u8981\u96c6\u4e2d\u5728\u5bf9\u4e00\u4e2amemory location\u7684\u4e00\u81f4\u6027\u4fdd\u8bc1\uff0cMESI\u534f\u8bae\u662f\u5bf9\u540c\u4e00\u4e2amemory location\u5728\u4e0d\u540ccache\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u4fdd\u8bc1\u3002","title":"2.6 \u603b\u7ed3"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#3#memory#consistency3","text":"memory model, memory model consistency, memory consistency\u4e09\u4e2a\u6307\u7684\u662f\u540c\u4e00\u4e2a\u4e1c\u897f\u3002 memory consistency\u662f\u4ec0\u4e48[8]\uff1a \\1. The guarantees provided by the runtime environment to a multithreaded program, regarding the order of memory operations. \\2. Each level of the environment might have a different memory model \u2013 CPU, virtual machine, language. \\3. The correctness of parallel algorithms depends on the memory model. \u8fd9\u8282\u6211\u4eec\u8ba8\u8bba\u7684\u662fCPU memory model\u3002","title":"3 memory consistency[3]"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#31#consistency","text":"\u4e00\u81f4\u6027\u4ece\u5f3a\u5230\u5f31\u6709\u591a\u79cd\u5206\u7c7b\uff0c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5e38\u7528lineraziblity consistency\uff08\u7ebf\u6027\u4e00\u81f4\u6027\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5728\u5bf9\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u4e00\u81f4\u6027\u4fdd\u8bc1\u4e2d\u53c8\u6709**Serializability** consistency\u8fd9\u4e5f\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u65f6\u53c8\u6709sequence consistency\uff08SC\uff09\u4e00\u81f4\u6027\u6a21\u578b\uff0csequence consistency\u4e5f\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u4e0d\u8fc7\u6bd4\u7ebf\u6027\u4e00\u81f4\u6027\u7a0d\u5f31\u3002\u5f53\u7136\u8fd8\u6709\u82e5\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u6bd4\u5982\u5185\u5b58\u4e00\u81f4\u6027\u4e2d\u7684relaxed memory consistency model\u3002 \u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u65f6\uff0c\u4e00\u81f4\u6027\u8bbe\u8ba1\u4e3b\u8981\u662f\u5728SC\u548cXC\u7684\u4e00\u79cd\u8bbe\u8ba1\uff0c\u4e3a\u4ec0\u4e48\u5728\u5e95\u5c42\u7cfb\u7edf\u8bbe\u8ba1\u65f6\u4e0d\u662f\u5bf9lineraziblity consistency\u7684\u5b9e\u73b0\u800c\u662f\u5bf9\u76f8\u6bd4\u8f83\u8f83\u5f31\u7684SC\u6216\u8005XC\u7684\u5b9e\u73b0\uff1f\u8fd9\u4e2a\u5728\u672c\u8282\u672b\u5c3e\u4f1a\u8c08\u4e00\u4e0b\u4e2a\u4eba\u7684\u89c2\u70b9\u3002 \u672c\u6587\u4e3b\u8981\u662f\u5bf9memory consistency\u7684\u4e00\u81f4\u6027\u6a21\u578b\u7684\u603b\u7ed3\uff0c\u6240\u4ee5\u5bf9lineraziblity\u548c**Serializability** consistency\u4e0d\u4f1a\u6d89\u53ca\u5f88\u591a\u3002","title":"3.1 consistency\u5206\u7c7b"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#32#sequence#consistency","text":"","title":"3.2 Sequence Consistency"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#320#reorder","text":"\u5728\u8c08memory consistency\u4e4b\u524d\u6211\u4eec\u5148\u4e86\u89e3\u4e0breorder\uff0c\u6211\u4eec\u7f16\u7801\u5e76\u53d1\u5e03\u8fd0\u884c\u9700\u8981\u7ecf\u8fc7\u7f16\u8bd1\u5668\u7f16\u8bd1\u540e\u7136\u540e\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u901a\u5e38\u6211\u4eec\u8ba4\u4e3a\u6211\u4eec\u6240\u5199\u7684\u4ee3\u7801\u662f\u6309\u7167\u987a\u5e8f\u6267\u884c\u4e0b\u53bb\u7684\uff0c\u5c31\u662f\u8bf4\u4e0a\u4e00\u4e2a\u8bed\u53e5\u4e00\u5b9a\u5728\u4e0b\u4e00\u4e2a\u8bed\u53e5\u6267\u884c\u4e4b\u524d\u6267\u884c\u3002\u8fd9\u662f\u6211\u4eec\u7684\u6f5c\u610f\u8bc6\uff0c\u7136\u800c\u4e8b\u5b9e\u53ef\u80fd\u5e76\u4e0d\u662f\u8fd9\u6837\uff0c\u56e0\u4e3a\u4e2d\u95f4\u7ecf\u8fc7\u4e86\u7f16\u8bd1\u5668\u4e5f\u7ecf\u8fc7\u4e86CPU\u3002\u7f16\u8bd1\u5668\u548cCPU\u4e3a\u4e86\u5145\u5206\u63d0\u9ad8\u7a0b\u5e8f\u8fd0\u884c\u6027\u80fd\u4f1a\u5728\u5185\u90e8\u8fdb\u884c\u4e00\u7cfb\u5217\u4f18\u5316\uff0c\u8fd9\u4e9b\u4f18\u5316\u65b9\u6cd5\u6709\u5f88\u591a\uff0c\u4e5f\u5f88\u590d\u6742\u3002\u6bd4\u8f83\u5178\u578b\u7684\u6709reorder\uff0cSpeculative execution\u7b49\u3002\u7f16\u8bd1\u5668\u4f1a\u5bf9\u6211\u4eec\u5199\u7684\u4ee3\u7801\u987a\u5e8f\u8fdb\u884creorder\uff0cCPU\u6267\u884c\u7684\u65f6\u5019\u4e5f\u4f1a\u8fdb\u884creorder\uff0c\u4e5f\u5c31\u662f\u8bf4\u5728\u6267\u884c\u65f6\uff0c\u6211\u4eec\u5199\u7684\u4ee3\u7801\u5e76\u4e0d\u662f\u4e00\u5b9a\u6309\u7167\u6211\u4eec\u6240\u770b\u5230\u987a\u5e8f\u3002\u4f46\u662f\u4e0d\u7528\u62c5\u5fc3\uff0cCPU\u6216\u8005\u7f16\u8bd1\u5668\u5728reorder\u7684\u65f6\u5019\u5e76\u4e0d\u4f1a\u65e0\u5398\u5934\u7684reorder\uff0c\u4ed6\u4eec\u81f3\u5c11\u8981\u4fdd\u8bc1\u7684\u662f\uff0c\u5728reorder\u4e4b\u540e\uff0c\u7a0b\u5e8f\u6240\u8868\u73b0\u51fa\u6765\u7684\u884c\u4e3a\u6548\u679c\u4e0e\u5355\u7ebf\u7a0b\u6267\u884c\u6548\u679c\u662f\u4e00\u81f4\u7684\u3002\u8fd9\u91cc\u63d0\u5230\u7684\u662f\u5355\u7ebf\u7a0b\uff0c\u4e5f\u5c31\u662f\u8bf4CPU\u548c\u7f16\u8bd1\u5668\u5e76\u4e0d\u80fd\u611f\u77e5\u9053\u4f60\u7684\u4ee3\u7801\u662f\u591a\u7ebf\u7a0b\u8fd8\u662f\u5355\u7ebf\u7a0b\uff0c\u4ed6\u53ea\u80fd\u4fdd\u8bc1\u5355\u7ebf\u7a0b\u72b6\u51b5\u65f6\u6b63\u786e\u7684\uff0c\u591a\u7ebf\u7a0b\u5c31\u4e0d\u5f97\u800c\u77e5\u4e86\u3002 \u6240\u4ee5 *C++ and the Perils of Double-Checked Locking.*\u8fd9\u7bc7\u8bba\u6587\u7684\u4f5c\u8005\u6709\u8fd9\u4e48\u4e00\u53e5\u8bdd\uff1a\"Multithreading is just one damn thing after, before, or simultaneous with another. \"\u3002 \u5176\u5b9e\u86ee\u6709\u9053\u7406\u7684\u3002 \u90a3\u4e48\u65e2\u7136CPU\u548ccompiler\u4e0d\u80fd\u611f\u77e5**\u591a\u7ebf\u7a0b**\uff0c\u90a3\u4f1a\u51fa\u73b0\u4ec0\u4e48\u95ee\u9898\u5462\uff1f\u5982\u4e0b\u3002 NOTE: \u65e0\u6cd5\u8fdb\u884ccontrol","title":"3.2.0 reorder"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#321#memory#consistency#motivation","text":"\u5728memory \u7cfb\u7edf\u8bbe\u8ba1\u65f6\u4e3a\u4ec0\u4e48\u9700\u8981memory consistency\u8fd9\u6837\u7684\u7ea6\u675f\uff1f\u8003\u8651\u4e0b\u9762\u8868\u683c\u9501\u63cf\u8ff0\u7684\u4e00\u79cd\u573a\u666f\u3002 core C1\u4e0e core C2\u662f\u4e24\u4e2a\u5355\u72ec\u7684core\uff0c\u5404\u81ea\u6267\u884c\u81ea\u5df1\u7684\u7a0b\u5e8f\uff0c\u5c31\u50cf\u8868\u683c\u6807\u9898r2\u4f1a\u4e00\u76f4\u88ab\u7f6e\u4e3aNEW\u5417\uff1f\u7b54\u6848\u5f53\u7136\u4e0d\u662f\uff0c\u9996\u5148CPU\u662f\u5b58\u5728out-of-order[6]\u4f18\u5316\u7684\uff0c\u53e6\u5916\u5bf9\u4e8ecore\u6765\u8bf4\uff0c\u4ed6\u5e76\u4e0d\u77e5\u9053\u81ea\u5df1\u5f53\u524d\u6267\u884c\u7684\u662f\u4e00\u4e2a\u591a\u7ebf\u7a0b\u7a0b\u5e8f\u3002\u4e00\u4e2aCPU core\u5728\u8bbe\u8ba1\u65f6\u81f3\u5c11\u9700\u8981\u4fdd\u8bc1\u5f53\u524dcore\u4e0a\u6267\u884c\u7684\u7a0b\u5e8f\u662f\u9075\u5faa\u5176program order\u7684(program order\u5c31\u662f\u4ee3\u7801\u987a\u5e8f)\u3002\u4f46\u662f\u4e3a\u4e86\u63d0\u9ad8\u6027\u80fd,CPU\u4f18\u5316\u5f15\u5165\u4e86out-of-order-excution\u673a\u5236\u548cSpeculative execution\u673a\u5236\uff0c\u90a3\u4e48\u5bf9\u4e8eC1\u800c\u8a00\uff0cCPU\u53ef\u4ee5\u6267\u884cS2->S1,\u4e5f\u53ef\u4ee5\u6267\u884cS1->S2, \u5bf9\u4e8e\u8fd9\u4e24\u79cd\u6267\u884c\u65b9\u5f0f\u5728C1\u770b\u6765\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u56e0\u4e3a\u5355\u7ebf\u7a0b\u800c\u8a00\u8fd9\u4e24\u79cd\u6267\u884c\u65b9\u5f0f\u6700\u540e\u8fbe\u5230\u7684\u6548\u679c\u662f\u4e00\u6837\u7684\u3002\uff08\u56e0\u4e3aS2\u548cS1\u662f\u5bf9\u4e0d\u540c\u7684memory location\u7684\u64cd\u4f5c\uff0c\u6240\u4ee5\u4f1areorder\uff0c\u5982\u679c\u662f\u5bf9\u540c\u4e00\u4e2amemory location\u64cd\u4f5c\u662f\u4e0d\u5141\u8bb8\u51fa\u73b0\u8fd9\u79cdreorder\u7684\uff0c\u5f53\u7136TSO\u5141\u8bb8\u8fd9\u79cdreorder\uff0c\u4f46\u662f\u5bf9\u540c\u4e00location\u800c\u8a00reorder\u524d\u540e\u6548\u679c\u4e00\u81f4\uff0c\u5177\u4f53\u4f1a\u5728\u540e\u9762\u7ae0\u8282\u8be6\u7ec6\u63cf\u8ff0\u3002\uff09 NOTE: \u5173\u4e8eTSO\uff0c\u53c2\u89c1 wikipedia Memory ordering : TSO Total store order (default) \u90a3\u4e48\u5982\u679cC1\u7684\u6267\u884c\u987a\u5e8f\u662fS2->L1->L2->S1\uff0c\u90a3\u4e48\u5f97\u5230\u7684\u7ed3\u679cr2 = 0\uff0c\u800c\u4e0d\u662f\u5411\u6211\u4eec\u9884\u671f\u7684r2 = NEW\u3002\u5bf9\u6211\u4eec\u800c\u8a00\u8fd9\u79cd\u7ed3\u679c\u662f\u8d85\u51fa\u9884\u671f\u7684\uff0c\u662f\u9519\u7684\u3002 \u90a3\u4e48\u4e3a\u4e86\u4fdd\u8bc1\u4e0d\u4f1a\u51fa\u73b0\u8fd9\u79cd\u8d85\u51fa\u9884\u671f\u7684\u884c\u4e3a\uff0c\u6211\u4eec\u5c31\u9700\u8981\u4e00\u79cd\u89c4\u5219\u6765\u7ea6\u675f\u8fd9\u79cd\u884c\u4e3a\u4e0d\u80fd\u51fa\u73b0\u3002\u8fd9\u4e2a\u4efb\u52a1\u5c31\u662fmemory consistency\u9700\u8981\u4fdd\u8bc1\u7684\uff08\u8fd9\u91cc\u6307\u7684\u662f\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff1aSC(sequence consistency)/TSO\uff0c XC\u7684memory consistency\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u8fd9\u70b9\uff09\u3002","title":"3.2.1 memory consistency motivation"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#322#program#order#vs#memory#order","text":"\u5728\u5f15\u5165SC\u5b9a\u4e49\u4e4b\u524d\uff0c\u6211\u4eec\u5148\u660e\u786e\u4ec0\u4e48\u662fprogram order\u4ec0\u4e48\u662fmemory order\u3002\u8fd9\u91cc\u6211\u4eec\u8c08\u7684\u90fd\u662f\u9488\u5bf9\u5171\u4eab\u5185\u5b58\u7684\u64cd\u4f5c\uff0c\u5bf9\u90a3\u4e9b\u5c40\u90e8\u53d8\u91cf\uff0c\u5bf9core\u6765\u8bf4\u662f\u79c1\u6709\u7684\uff0c\u4e0d\u5b58\u5728\u5171\u4eab\uff0c\u6240\u4ee5\u4e0d\u4f1a\u5b58\u5728consistency\u7684\u95ee\u9898\u3002 NOTE: shared data program order: \u5c31\u662f\u6211\u4eec\u5199\u7684\u4ee3\u7801\u7684\u987a\u5e8f\uff0c\u8fd9\u4e2a\u662f\u9759\u6001\u7684\u4e5f\u662f\u6bcf\u4e2aCPU core\u5404\u81ea\u62e5\u6709\u7684\u3002 memory order: \u5c31\u662f\u4ee3\u7801\u6267\u884c\u7684\u987a\u5e8f\uff0c\u8fd9\u4e2a\u662f\u5168\u5c40\u7684\uff0c\u6bcf\u4e2aCPU core\u5bf9\u5171\u4eab\u5185\u5b58\u7684\u6267\u884c\u90fd\u4f1a\u51fa\u73b0\u5728memory order\u4e2d\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6bcf\u4e2acore\u7684\u4ee3\u7801\u90fd\u4f1a\u5bf9\u5e94\u5230memory order\u8fd9\u6761\u6267\u884c\u7ebf\u4e0a\u3002","title":"3.2.2 program order vs memory order"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#323#sc#definition","text":"Lamport\u5f15\u5165\u4e86\u5bf9SC\u7684\u5b9a\u4e49\uff0c\u4ed6\u8fd9\u6837\u5b9a\u4e49multiprocessor\u4e0b\u7684SC: \"the result of any execution is the same as if the operations of all processors (cores) were executed in some sequential order, and the operations of each individual processor (core) appear in this sequence in the order specified by its program.\"[3] \u5982\u4e0a\u56fe\u4e2d\uff0cS1 \u4e0e S2\u7684program order\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a S1 <p S2, S1\u4e0eL2\u53ef\u4ee5\u8868\u793a\u4e3a S1 <m L2. \u7528<p \u8868\u793aprogram order\u7684\u5148\u4e8e\u987a\u5e8f\uff0c<m\u8868\u793amemory order\u7684\u5148\u4e8e\u987a\u5e8f\u3002 \u90a3\u4e48SC\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u5982\u4e0b[3]\uff1a (1) All cores insert their loads and stores into the order <m respecting their program order, regardless of whether they are to the same or different addresses (i.e., a=b or a=\u0338b). There are four cases: If L(a) <p L(b) \u21d2 L(a) <m L(b) /* Load\u2192Load */ If L(a) <p S(b) \u21d2 L(a) <m S(b) /* Load\u2192Store */ ***If S(a) <p S(b) \u21d2 S(a) <m S(b) /* Store\u2192Store */** ***If S(a) <p L(b) \u21d2 S(a) <m L(b) /* Store\u2192Load */** (\u6240\u6709\u5bf9\u5171\u4eab\u5185\u5b58\u7684\u64cd\u4f5c\u90fd\u53ef\u4ee5\u62bd\u8c61\u6210load(\u8bfb\u53d6)\u548cstore(\u5199\u5165)\uff0c\u6bcf\u4e00core\u6267\u884cload\u548cstore\u662f\u6309\u7167\u5176program order\uff0c\u90a3\u4e48\u5c31\u6709S1 <p S2\u80af\u5b9a\u4f1a\u63a8\u51fa S1 <m S2\uff0cSC\u7684\u5b9a\u4e49\u4e5f\u7531\u6b64\u5f15\u5165\u4e86load\u548cstore\u7684\u56db\u79cd\u5173\u7cfb\u3002\u5728SC\u7684\u5b9a\u4e49\u4e2d\u8fd9\u56db\u79cd\u5173\u7cfb\u662f\u4e0d\u5141\u8bb8\u88abreorder\u7684\uff0c\u5373\u4f7f\u662f\u5bf9\u4e0d\u540cmemory location\u7684\u64cd\u4f5c\u3002) (2) Every load gets its value from the last store before it (in global memory order) to the same address: Value of L(a) = Value of MAX <m {S(a) | S(a) <m L(a)}, where MAX <m denotes \u201clatest in memory order.\u201d \uff08SC\u4e2d\u5b9a\u4e49\u6bcf\u4e2a\u5171\u4eab\u5185\u5b58\u7684\u8bfb\u53d6\u80af\u5b9a\u662f\u5176\u5728memory order\u4e2d\u6700\u8fd1\u7684\u4e00\u6b21\u5199\u5165\u7684\u503c\uff09\u3002 \u53ea\u8981\u7b26\u5408\u4e0a\u8ff0\u4e24\u4e2a\u6761\u4ef6\uff0c\u90a3\u4e48\u6211\u4eec\u5c31\u53ef\u4ee5\u8bf4\u8fd9\u4e2amemory\u64cd\u4f5c\u662f\u7b26\u5408\u987a\u5e8f\u4e00\u81f4\u6027\u7684\u3002 \u4e0b\u8868\u7ed9\u51fa\u4e86SC\u7684\u5b9a\u4e49\u4e2d\u7ea6\u675f\u7684\u884c\u4e3a\u3002operation1 <p operation2\u3002","title":"3.2.3 SC definition"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#324#sc#example","text":"\u8fd8\u662f\u75283.2.1\u8282\u4e2d\u7684\u4f8b\u5b50\uff0c\u6765\u9a8c\u8bc1\u4e00\u4e0bSC\u5bf9memory\u64cd\u4f5c\u7684\u7ea6\u675f\u3002 \u7a0b\u5e8fmemory operation\u5982\u4e0b \u7a0b\u5e8f\u53ef\u80fd\u51fa\u73b0\u7684SC\u6267\u884c\u7ed3\u679c \u4e0b\u56fe\u4e2d\u524d\u4e09\u79cd\u7ed3\u679c\u662f\u7b26\u5408SC\u5b9a\u4e49\u7684\uff0c\u6700\u540e\u4e00\u79cd\u662f\u4e0d\u7b26\u5408SC\u5b9a\u4e49\u7684\uff0c\u53e6\u5916\u4e00\u65b9\u9762\u53ef\u4ee5\u770b\u51faSC\u4e2d\u662f\u4e0d\u5141\u8bb8\u6267\u884c\u7ebf\uff08\u865a\u7ebf\uff09\u4ea4\u53c9\u7684\u3002\u6bcf\u4e2a\u6267\u884c\u8bed\u53e5\u843d\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u7684\u4f4d\u7f6e\u8868\u793a\u5f53\u524d\u7684\u6267\u884c\u5df2\u7ecf\u53d1\u751f\u4e86\u3002\uff08\u5728lineraziblity consistency\u5b9a\u4e49\u4e2d\u6bcf\u4e2a\u64cd\u4f5c\u4f1a\u5212\u5206\u4e3ainvocation\u548cresponse\u4e24\u4e2a\u9636\u6bb5\uff0c\u4e14\u64cd\u4f5c\u4f1a\u5728\u8fd9\u4e24\u4e2a\u9636\u6bb5\u4e2d\u95f4\u4efb\u4e00\u4e2a\u77ac\u95f4\u53d1\u751f\uff0c\u8fd9\u91cc\u8bed\u53e5\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u7684\u843d\u70b9\u5c31\u662f\u8be5\u64cd\u4f5c\u6267\u884c\u7684\u77ac\u95f4\u3002\uff09 \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\u5728\u4fdd\u8bc1program order\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0cmemory order\u7684\u987a\u5e8f\u53ef\u4ee5\u968f\u610f\u6392\u5217\u3002\u8fd9\u4e2a\u7279\u522b\u50cf\u6211\u4eec\u5e73\u5e38\u6d17\u724c\u7684\u8fc7\u7a0b\uff0c\u4e24\u526f\u6251\u514b\u724c\u4e92\u76f8\u4ea4\u53e0\uff0c\u4f46\u662f\u6bcf\u526f\u724c\u7684\u987a\u5e8f\uff08program order\uff09\u662f\u4e0d\u53d8\u7684\uff0c\u4f46\u662f\u5728\u6d17\u5b8c\u4e4b\u540e\u4e24\u526f\u724c\u5408\u6210\u4e00\u526f\u724c\u7684\u65f6\u5019\u8fd9\u4e2a\u987a\u5e8f\u662f\u968f\u673a\u7684\uff0c\u4e0d\u786e\u5b9a\u3002 \u5728\u7b26\u5408SC\u5b9a\u4e49\u4e0b\u7684\u6267\u884c\u4e2d\u4f1a\u51fa\u73b0\u591a\u79cd\u6b63\u786e\u7684\u7ed3\u679c\uff0c\u8fd9\u4e9b\u7ed3\u679c\u7b26\u5408SC\u5b9a\u4e49\uff0c\u5728\u7f16\u7801\u4eba\u5458\u5f00\u6765\uff0c\u8fd9\u6837\u4e00\u4e2a\u591a\u7ebf\u7a0b\u7a0b\u5e8f\u6267\u884c\u7684\u7ed3\u679c\u4e5f\u662f\u6b63\u786e\u7684\u3002\u5e76\u53d1\u5728\u4e0d\u5f3a\u52a0\u5e72\u6d89\u7684\u60c5\u51b5\u4e0b\u662f\u4e0d\u80fd\u9884\u6d4b\u6267\u884c\u987a\u5e8f\u7684\u3002","title":"3.2.4 SC example"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#33#tso","text":"","title":"3.3 TSO"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#330#sc","text":"SC\u4e25\u683c\u5b9a\u4e49\u4e86\u5bf9\u4e8e\u5171\u4eab\u5185\u5b58\u7684load\u548cstore\u64cd\u4f5c\uff0cloadload\uff0cstorestore\uff0cloadstore\uff0cstoreload\u56db\u79cd\u6267\u884c\u987a\u5e8f\u662f\u4e0d\u5141\u8bb8reorder\u7684\u3002\u5f53\u4e0bCPU\u7684\u6267\u884c\u901f\u5ea6\u5df2\u7ecf\u7529DRAM\uff08memory\uff09\u597d\u51e0\u4e2a\u91cf\u7ea7\uff0c\u5982\u679c\u6bcf\u6b21store\uff0cload\u64cd\u4f5c\u90fd\u4eceDRAM\u8bfb\u53d6\u4f1a\u62d6\u6162CPU\u7684\u6267\u884c\u901f\u5ea6\uff0c\u5728\u8fd9\u4e2a\u6781\u5ea6\u538b\u69a8\u786c\u4ef6\u6027\u80fd\u7684\u65f6\u4ee3\uff0c\u662f\u4e0d\u80fd\u63a5\u53d7\u8fd9\u79cd\u884c\u4e3a\u7684\u3002\u56e0\u6b64\u5728x86\u7684\u67b6\u6784\u5b9e\u73b0\u4e2d\u5f15\u5165\u4e86TSO\u3002 However, current commercial compilers and most current commercial hardware do not preserve sequential consistency. [9]","title":"3.3.0 SC\u5e26\u6765\u7684\u95ee\u9898"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#331#tsosc","text":"TSO\u5168\u79f0Total Store Order\uff0c\u6211\u4eec\u5728\u8ba8\u8bbaTSO\u7684\u65f6\u5019\u5148\u5ffd\u7565cache\u8fd9\u4e00\u5c42\u3002 TSO\u5728CPU\u4e0ememory\u4e4b\u95f4\u5f15\u5165\u4e86write buffer\u3002CPU\u5199\u5165\u7684\u65f6\u5019\u5148\u5199\u5165write buffer\u7136\u540e\u5c31\u8fd4\u56de\u4e86\uff0c\u8fd9\u6837\u5c31\u5c06cpu\u4e0ememory\u4e4b\u95f4\u7684\u5dee\u8ddd\u9690\u85cf\u4e86\uff0c\u4f46\u662f\u8fd9\u6837\u540c\u6837\u5e26\u6765\u4e86\u4e00\u4e2a\u95ee\u9898\u3002 \u8fd8\u662f\u4e0a\u9762\u8fd9\u4e2a\u4f8b\u5b50\uff0cS1\u5c06x=NEW\u653e\u5230\u4e86core C1\u7684write buffer\u4e2d\uff0cS2\u5c06y=NEW\u653e\u5230\u4e86C2\u7684write buffer\u4e2d\uff0c\u90a3\u4e48\u5728\u6267\u884cL1,L2\u7684\u65f6\u5019\uff0cr1\u4e0er2\u8fd9\u65f6\u5019\u4ecememory\u8bfb\u5230\u662f0\u3002\u8fd9\u4e2a\u662f\u8fdd\u80cc\u4e86SC\u7684\uff0c\u4f46\u662f\u8fd9\u6837\u7684\u8bbe\u8ba1\u786e\u5b9e\u5e26\u6765\u4e86\u6027\u80fd\u7684\u63d0\u5347\u3002 \u90a3\u4e48\u5728TSO\u6a21\u578b\u4e0b\u7684\u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff1a \u524d\u4e09\u79cd\u4e0eSC\u4e00\u81f4\uff0c\u7b2c\u56db\u4e2a\u6267\u884c\u7ed3\u679c\u5219\u662fTSO\u72ec\u6709\u7684\uff0c\u53ef\u4ee5\u770b\u51fa\uff0cTSO\u4e2d\u5141\u8bb8\u6267\u884c\u7ebf\u4ea4\u53c9\u3002","title":"3.3.1 TSO\u5982\u4f55\u89e3\u51b3SC\u7684\u95ee\u9898"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#332#tso#definition","text":"TSO\u5728\u5b9e\u73b0SC\u7684\u8fc7\u7a0b\u4e2d\u505a\u4e86\u4e00\u4e9b\u6539\u52a8\uff0c\u5e26\u6765\u4e86\u6027\u80fd\u63d0\u5347\u3002SC\u662fTSO\u7684\u4e00\u4e9b\u7279\u4f8b\uff0cx86\u901a\u8fc7\u5728\u6bcf\u4e2acore\u4e0a\u5f15\u5165FIFO \u7684write buffer\u5b9e\u73b0\u4e86TSO[3]\u3002 TSO\u4e0eSC\u7684\u5b9a\u4e49\u5f88\u50cf\uff0c\u5176\u5b9a\u4e49\u5982\u4e0b\uff1a \uff081\uff09 All cores insert their loads and stores into the memory order <m respecting their program order, regardless of whether they are to the same or different addresses (i.e., a==b or a!=b). There are four cases: If L(a) <p L(b) \u21d2 L(a) <m L(b) /* Load\u2192Load */ If L(a) <p S(b) \u21d2 L(a) <m S(b) /* Load\u2192Store */ If S(a) <p S(b) \u21d2 S(a) <m S(b) /* Store\u2192Store */ // If S(a) S(a) Load*/ *//\u8fd9\u4e2a\u662fSC\u7684\uff0cTSO\u6ca1\u6709* \uff082\uff09Every load gets its value from the last store before it to the same address: // Value of L(a) = Value of MAX <m {S(a) | S(a) <m L(a)} *// \u8fd9\u4e2a\u662fSC\u7684\uff0c TSO\u6ca1\u6709* Value of L(a) = Value of MAX <m {S(a) | S(a) <m L(a) or S(a) <p L(a)} TSO\u7684\u5b9a\u4e49\u4e0eSC\u7684\u5b9a\u4e49\u6709\u4e24\u4e2a\u53d8\u5316\uff1a \u53d8\u5316\u4e00\uff1a \u4e0d\u4fdd\u8bc1storeload\u987a\u5e8f \u4e3e\u4e2a\u4f8b\u5b50\uff1aCore C1\u4e2dS1\u548cL1\uff0c S1\u5148\u53bbL1\u6267\u884c\uff0c\u4f46\u662fS1\u53ea\u662f\u5c06\u503c\u9001\u5165\u4e86write buffer\u5c31\u8fd4\u56de\u4e86\uff0c\u7d27\u63a5\u7740\u6267\u884cL1\uff0cL1\u5728memory order\u4e2d\u7684\u70b9\u6267\u884c\u5b8c\u4e4b\u540e\uff0cS1\u7684write buffer\u8fd9\u65f6\u5019flush\u5230\u5185\u5b58\uff0c\u90a3\u4e48S1\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u771f\u6b63\u6267\u884c\u7684\u70b9\u5728L1\u4e4b\u540e\u4e86\uff0c\u90a3\u4e48\u8fd9\u65f6\u5019S1\u4e0eL1\u5c31\u51fa\u73b0\u4e86reorder\u4e86\u3002 \u53d8\u5316\u4e8c\uff1a load\u7684\u6700\u65b0\u503c\u4e0d\u4e00\u5b9a\u662fmemory order\u4e2d\u6700\u8fd1\u7684\uff0c\u6709\u53ef\u80fd\u662fprogram order\u6700\u8fd1\u7684store \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u65e0\u8bba\u662fTSO\u8fd8\u662fSC\u90fd\u9700\u8981\u81f3\u5c11\u4fdd\u8bc1\u4e00\u70b9\uff0c\u5373\u4f7f\u5141\u8bb8reorder\u4e5f\u8981\u4fdd\u8bc1program\u6267\u884c\u7684\u7ed3\u679c\u4e0e\u5355\u7ebf\u7a0b\u6267\u884c\u7684\u7ed3\u679c\u662f\u4e00\u81f4\u7684\u3002\u6bd4\u5982\u4e00\u5bf9\u64cd\u4f5c\uff1a S1: x = new L1: y = x. \u65e0\u8bba\u662fTSO\u8fd8\u662fSC\u90fd\u9700\u8981\u4fdd\u8bc1y\u8bfb\u5230\u7684\u662fx=new\u7684\u503c\uff08\u6392\u9664\u5176\u4ed6\u7ebf\u7a0b\u5728\u8fd9\u4e24\u4e2a\u8bed\u53e5\u4e4b\u524d\u5bf9x\u8fdb\u884cstore\u64cd\u4f5c\u3002\uff09 \u56e0\u4e3aTSO\u5f15\u5165\u4e86write buffer\uff0c\u90a3\u4e48\u4e0a\u8ff0x=new\u4f1a\u5199\u5165buffer\uff0c\u5982\u4f55\u786e\u4fddL1\u4f1a\u8bfb\u5230\u6700\u65b0\u7684\u503c\u5462\uff0cTSO\u5f15\u5165\u4e86\u4e00\u79cd\u53eb\u201cbypass\u201d\u7684\u6982\u5ff5\uff0c\u5c31\u662f\u5bf9\u4e8e**\u540c\u4e00memory location**\u7684\u8bfb\u5199\u4f1a\u4fdd\u969cload\u4f1a\u8bfb\u5230store\u7684\u6700\u65b0\u503c\u65e0\u8bba\u8fd9\u4e2astore\u4f1a\u4e0d\u4f1a\u8fdb\u5165write buffer\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff1a L1\u8bfb\u53d6\u7684\u662fS1\u7684\u503c\uff0c\u5373\u4f7fL1 <m S1 \u4e14 S1 <p L1.","title":"3.3.2 TSO definition"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#333#fence","text":"3.3.1\u5c0f\u8282\u63d0\u5230\u7684\u4f8b\u5b50\u4e2d\uff0c\u51fa\u73b0\u4e86\u4e00\u79cd\u8d85\u51fa\u9884\u671f\u7684\u6267\u884c\u72b6\u51b5\uff0c\u5982\u679c\u6211\u4eec\u60f3\u907f\u514d\u8fd9\u79cd\u95ee\u9898\uff0c\u90a3\u4e48\u9700\u8981\u5728\u4e0a\u5c42\u4ee3\u7801\u4e2d\u6dfb\u52a0FENCE\uff0c\u8fd9\u4e2afence\u53ef\u4ee5\u7406\u89e3\u4e3amemory barrier\uff0c\u4ed6\u7684\u4f5c\u7528\u662f\u5c06write buffer\u4e2d\u7684\u8bb0\u5f55flush\u5230\u5185\u5b58\u3002 FENCE\u4f1a\u5f3a\u5236\u4fdd\u8bc1program order\u3002 If S(a) <p FENCE \u21d2 S(a) <m FENCE /* Store \u2192 FENCE */ If FENCE <p L(a) \u21d2 FENCE <m L(a) /* FENCE \u2192 Load */ \u5982\u679c\u518dS1\u4e0eL1\u4e4b\u95f4\u52a0\u4e0aFENCE\uff0c\u5c31\u4fdd\u8bc1\u4e86S1 <p L1 \u548c S1 <m L1. x86\u7cfb\u7edf\u4e2d\u5e76\u6ca1\u6709\u4e3a\u6211\u4eec\u52a0\u4e0aFENCE\uff0c\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u6dfb\u52a0FENCE\u53ea\u6709\u5f00\u53d1\u4eba\u5458\u77e5\u9053\uff0c\u6240\u4ee5\u4e3a\u4e86\u907f\u514d\u7a0b\u5e8f\u51fa\u73b0\u83ab\u540d\u5176\u5999\u7684\u9519\u8bef\uff0c\u8bb0\u5f97\u5728store\u548cload\u5171\u4eab\u5185\u5b58\u7684\u65f6\u5019\u52a0\u4e0aFENCE\u3002 \u56e0\u6b64TSO\u4e0b\u7684operation order\u5982\u4e0b: (operation1 <p operation2)","title":"3.3.3 FENCE"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#34#relaxed#memory#consistency","text":"SC\u548cTSO\u4e25\u683c\u610f\u4e49\u4e0a\u6765\u8bf4\u90fd\u662f\u4e00\u79cd\u5f3a\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u56e0\u4e3a\u4ed6\u4eec\u90fd\u5bf9\u7a0b\u5e8f\u7684\u6267\u884c\u987a\u5e8f\u505a\u4e86\u4e00\u5b9a\u7684\u7ea6\u675f\uff0c\u65e2\u7136\u5b58\u5728\u7ea6\u675f\u90a3\u4e48\u5c31\u4f1a\u5e26\u6765\u4e00\u5b9a\u7684\u6027\u80fd\u635f\u8017\u3002 \u90a3\u4e48\u6709\u6ca1\u6709\u4e00\u79cd\u6ca1\u8fd9\u4e48\u591a\u7684\u7ea6\u675f\u7684\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u80fd\u591f\u4f7f\u673a\u5668\u8fdb\u884c\u6df1\u5ea6\u7684\u4f18\u5316\u5e76\u53d1\u6325\u6781\u81f4\u6027\u80fd\u3002\u90a3\u4e48\u6267\u884c\u987a\u5e8f\u7684\u6b63\u786e\u6027\u5c31\u53ea\u80fd\u6709\u7f16\u7801\u4eba\u5458\u6765\u4fdd\u8bc1\u4e86\u3002 relaxed memory consistency\u5b9e\u73b0\u5bf9\u4e8eload\u4e0estore\u987a\u5e8f\u5b8c\u5168\u653e\u5f00\uff0c\u9664\u4e86\u5bf9\u540c\u4e00memory location\u7684\u64cd\u4f5c\u4fdd\u8bc1load\u770b\u5230\u662f\u6700\u65b0\u7684store\u4ee5\u5916\u5176\u4ed6\u90fd\u4e0d\u8fdb\u884c\u7ea6\u675f\uff0c\u7f16\u7801\u4eba\u5458\u5982\u679c\u60f3\u5f3a\u52a0order\u53ef\u4ee5\u901a\u8fc7\u4e0a\u8ff0\u7684FENCE\u3002","title":"3.4 relaxed memory consistency"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#35#linearizability#consistency","text":"**Linearizability**\u662f\u6bd4SC\u66f4\u5f3a\u7684\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5728SC\u5b9a\u4e49\u4e2d\u4e0d\u540ccore\u7684\u6267\u884c\u8bed\u53e5\u5728memory order\u7684\u65f6\u95f4\u7ebf\u4e2d\u53ef\u4ee5\u968f\u610f\u63d2\u5165\uff0c\u800c\u5bf9\u4e8eLinearizability consistency\u4e0d\u4ec5\u9650\u5236\u4e86\u5355\u7ebf\u7a0b\u4e2d\u7684\u6267\u884c\u987a\u5e8f\uff0c\u540c\u65f6\u5bf9\u4e8e\u591a\u7ebf\u7a0b\u4e2d\u6267\u884c\u987a\u5e8f\u4e5f\u505a\u4e86\u4e00\u4e9b\u9650\u5236\u3002\u5177\u4f53 Linearizability consistency\u7684\u76f8\u5173\u77e5\u8bc6\u5728\u540e\u9762\u6587\u7ae0\u518d\u8be6\u7ec6\u603b\u7ed3\u4e00\u4e0b\u3002","title":"3.5 Linearizability consistency"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#36#memory#consistency","text":"1\u3001\u4e3a\u4ec0\u4e48memory consistency\u53ea\u8c08sequence consistency(SC)\u800c\u4e0d\u8c08linearizability(LIN) \u4e00\u65b9\u9762\uff0c\u56e0\u4e3aSC\u5728\u5e95\u5c42\u5df2\u7ecf\u5b9a\u4e49\u4e00\u4e2a\u591a\u7ebf\u7a0b\u80fd\u591f\u5bf9\u5171\u4eab\u5185\u5b58\u5e76\u53d1\u64cd\u4f5c\u7684\u6b63\u786e\u6027\uff0cLIN\u6bd4SC\u66f4\u4e25\u683c\uff0cLIN\u662fSC\u7684\u4e00\u79cd\u5f62\u5f0f\uff0cLIN\u66f4\u8d34\u8fd1\u4e0e\u4e0a\u5c42\uff0c\u5728SC\u63d0\u4f9b\u7684\u591a\u79cd\u6b63\u786e\u6027\u7684\u6267\u884c\u5e8f\u5217\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u7b26\u5408\u6211\u4eec\u4e1a\u52a1\u903b\u8f91\u6b63\u786e\u6027\u7684\u4fdd\u969c\u624b\u6bb5\uff0c\u800cLIN\u5c31\u662f\u8fd9\u79cd\u624b\u6bb5\u3002 \u53e6\u4e00\u65b9\u9762\u662f\uff0c\u5728\u5e95\u5c42\u7cfb\u7edf\u4e2dCPU\u548c\u7f16\u8bd1\u5668\u662f\u4e0d\u80fd\u610f\u8bc6\u5230\u591a\u7ebf\u7a0b\u7684\u5b58\u5728\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4CPU\u53ea\u662f \u77e5\u9053\u5f53\u524d\u7684\u6307\u4ee4\uff0c\u5e76\u4e0d\u77e5\u9053\u5f53\u524d\u6307\u4ee4\u4e0e\u5176\u4ed6CPU\u6307\u4ee4\u4e4b\u95f4\u7684\u5173\u7cfb\uff0cLIN\u4e2d\u7ea6\u675f\u4e86\u4e24\u4e2aprocess\uff08\u4e5f\u5c31\u662f\u4e24\u4e2a\u6267\u884c\u4f53\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u7ebf\u7a0b\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u8fdb\u7a0b\u7b49\uff09\u4e4b\u95f4\u7684partial order\uff08\u504f\u5e8f\u5173\u7cfb\uff09\uff0c\u800c\u8fd9\u79cd\u5173\u7cfb\u5728\u5e95\u5c42\u662f\u4e0d\u5f97\u800c\u77e5\u7684\u3002 \u5728\u4e00\u4e2a\u539f\u56e0\uff0cLIN\u7684\u6210\u672c\u66f4\u9ad8\uff0c\u5982\u679c\u518dCPU\u5c42\u9762\u5b9e\u73b0LIN\uff0c\u90a3\u4e48\u539f\u672c\u7528\u6765\u63d0\u9ad8\u6027\u80fd\u7684cache\u548cstore buffer\u7684\u4f5c\u7528\u5c31\u4f1a\u88ab\u5927\u5927\u524a\u5f31\uff0c\u6240\u4ee5\u5728\u80fd\u591f\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u57fa\u7840\u4e0a\u53c8\u80fd\u5145\u5206\u5229\u7528CPU\u8d44\u6e90\u7684\u524d\u63d0\u4e0b\uff0cLIN\u5e76\u6ca1\u6709\u5728CPU\u5c42\u9762\u5b9e\u73b0\u3002 **SC**\u5728\u5b9a\u4e49\u4e2d\u6ca1\u6709\u7ea6\u5b9a\u6267\u884c\u65f6\u95f4\u8fd9\u4e2a\u6982\u5ff5\uff0c\u53ea\u662f\u5f3a\u8c03\u4e86program order\u4e0eexecution order\uff0c\u4e5f\u5c31\u662fmemory order\u3002 **Linearizability**\u5728\u5b9a\u4e49\u4e2d\u7ea6\u5b9a\u4e86\u4e24\u4e2a\u6267\u884c\u4f53\u4e4b\u95f4\u7684\u6267\u884c\u987a\u5e8f\uff0c\u4e5f\u5c31\u662f\u6267\u884c\u65f6\u95f4\u5148\u540e\u88ab\u7ea6\u5b9a\u4e86\u3002 2\u3001memory consistency\u4e0ecache coherence\u7684\u5173\u7cfb \u4ece\u4ee5\u4e0a\u8ba8\u8bba\u53ef\u4ee5\u770b\u51fa\uff0cmemory consistency\u6ce8\u91cd\u7684\u662f\u5168\u5c40\u7684memory order\uff0c\u800ccache coherence\u5219\u662f\u5173\u6ce8\u4e8e\u4e00\u4e2amemory location\u3002 memory consistency\u662f\u4fdd\u8bc1\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u6b63\u786e\u6027\uff0c\u6211\u4eec\u5728\u8ba8\u8bba\u8fd9\u4e2a\u7684\u65f6\u5019\u53ef\u4ee5\u628acache\u5f53\u505a\u4e00\u4e2a\u9ed1\u76d2\u5b50\u6765\u5904\u7406\uff0c\u4e5f\u5c31\u662f\u8bf4\u5373\u4f7f\u6ca1\u6709cache\uff0c\u6211\u4eec\u4e5f\u540c\u6837\u9700\u8981memory consistency\u6765\u4fdd\u8bc1\u6b63\u786e\u6027\u3002 3\u3001memory barrier\u4e0ememory order\u7684\u5173\u7cfb \u7b2c2\u8282\u4e2d\u6700\u540e\u63d0\u5230\u4e86memory barrier\uff0c\u8fd9\u4e2amemory barrier\u4f1aflush storebuffer\u548cinvalidate queue\u3002\u90a3\u4e48memory barrier\u4e0ememory order\u6709\u5565\u5173\u7cfb\u5462\u3002 memory order\u662f\u5168\u5c40\u7684\u591a\u5904\u7406\u5668\u5bf9\u5171\u4eab\u53d8\u91cf\u7684\u64cd\u4f5c\u7684\u4e00\u4e2a\u6392\u5e8f\u3002\u5bf9\u5171\u4eab\u53d8\u91cf\u64cd\u4f5c\u843d\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u7684\u70b9\u5c31\u662f\u5bf9\u5916\u8868\u73b0\u51fa\u7684\u6267\u884c\u987a\u5e8f\u3002 memory barrier\u4f1a\u5c06storebuffer\u4e2d\u7684\u5185\u5bb9flush\u5230cache\u7136\u540ecache\uff0c\u90a3\u4e48\u53ea\u6709\u6267\u884c\u8fd9\u4e2amemory barrier\u4e4b\u540e\uff0c\u8fd9\u4e2a\u64cd\u4f5c\u624d\u7b97\u771f\u6b63\u7684\u6267\u884c\uff0c\u624d\u80fd\u771f\u6b63\u843d\u5728memory order\u8fd9\u6761\u7ebf\u4e0a\u3002\u6240\u4ee5memory barrier\u8d77\u5230\u4e00\u4e2aorder\u7684\u4f5c\u7528\uff0c\u8fd9\u4e2aorder\u662f\u4e00\u4e2a\u52a8\u8bcd\uff0c\u5c31\u662f\u4ec0\u4e48\u65f6\u5019\u771f\u6b63\u6267\u884c\u8fd9\u4e2a\u52a8\u4f5c\u3002","title":"3.6 memory consistency\u603b\u7ed3"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#4","text":"","title":"4 \u7bc7\u5e45\u9650\u5236\uff0c\u540e\u7eed\u90e8\u5206\u653e\u5728\u540e\u4e00\u7bc7"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/1/#7#references","text":"M. Mizuno, M. Raynal, J.Z. Zhou. Sequential Consistency in Distributed Systems. Scott Meyers and Andrei Alexandrescu. C++ and the Perils of Double-Checked Locking. Daniel J. Sorin, Mark D. Hill, and David A. Wood. A Primer on Memory Consistency and Cache Coherence. Paul E. McKenney . Memory Barriers: a Hardware View for Software Hackers. C++ memory order out of order execution The New C++: Lay down your guns, knives, and clubs c++ memory model H. Boehm, S. V. Adve. Foundations of the C++ Concurrency Memory Model C++ Standard - 2012-01-16 - Working Draft (N3337).pdf CPU Cache and Memory Ordering think cell talk memory model Acquire and Release Semantics C++ Memory model","title":"7 References"},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/2/","text":"zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0b)","title":2},{"location":"CPU-memory-access/CPU-cache-memory/TODO-zhihu-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/2/#zhuanlanzhihu#--","text":"","title":"zhuanlan.zhihu \u9ad8\u5e76\u53d1\u7f16\u7a0b--\u591a\u5904\u7406\u5668\u7f16\u7a0b\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898(\u4e0b)"},{"location":"CPU-memory-access/CPU-memory-model/","text":"\u5173\u4e8e\u672c\u7ae0 \u5185\u5bb9\u6982\u8ff0 Compiler\uff0cCPU\u6267\u884cmemory reordering\u7684\u76ee\u7684\u662f: optimization\uff0c\u663e\u7136\u5b83\u662f\u9075\u5faaoptimization principle\u7684\uff1b \u5728\u8fdb\u884clockless programming\u7684\u65f6\u5019\uff0c\u7531\u4e8ememory reordering\u7684\u5b58\u5728\uff0c\u5bfc\u81f4\u4e86unordering\u3001\u4e0d\u786e\u5b9a\u6027\uff0c\u8fdb\u800c\u5bfc\u81f4uncomputational\uff0c\u663e\u7136\u4e3a\u4e86make it computational\uff0c\u6211\u4eec\u9700\u8981\u6dfb\u52a0control: ordering\uff0c\u901a\u8fc7memory barrier/fence\u6765\u5b9e\u73b0ordering\uff1b \u5728preshing\u7684lockfree programming\u7cfb\u5217\u6587\u7ae0\u4e2d\u5df2\u7ecf\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3\uff0c\u56e0\u6b64\u672c\u7ae0\u53ea\u662f\u8fdb\u884c\u68b3\u7406\uff0c\u57fa\u672c\u4e0a\u6ca1\u6709\u6df1\u5165\u7684\u5185\u5bb9\uff1b \u7ae0\u8282\u8bf4\u660e \u672c\u7ae0\u9996\u5148\u4ecb\u7ecd\u4e00\u4e2aprogrammer\u5e73\u65f6\u4e0d\u4f1a\u53d1\u73b0\u3001\u4f46\u662f\u666e\u904d\u5b58\u5728\u7684\u7684memory reordering\uff0c\u4ee5\u53ca\u5b83\u6240\u5e26\u6765\u7684\u5f71\u54cd\uff1b \u7136\u540e\u6b63\u5f0f\u4ecb\u7ecdmemory ordering\uff1b \u7136\u540e\u4ecb\u7ecdcontrol\u624b\u6bb5: memory barrier\uff1b","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/#_1","text":"","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/CPU-memory-model/#_2","text":"Compiler\uff0cCPU\u6267\u884cmemory reordering\u7684\u76ee\u7684\u662f: optimization\uff0c\u663e\u7136\u5b83\u662f\u9075\u5faaoptimization principle\u7684\uff1b \u5728\u8fdb\u884clockless programming\u7684\u65f6\u5019\uff0c\u7531\u4e8ememory reordering\u7684\u5b58\u5728\uff0c\u5bfc\u81f4\u4e86unordering\u3001\u4e0d\u786e\u5b9a\u6027\uff0c\u8fdb\u800c\u5bfc\u81f4uncomputational\uff0c\u663e\u7136\u4e3a\u4e86make it computational\uff0c\u6211\u4eec\u9700\u8981\u6dfb\u52a0control: ordering\uff0c\u901a\u8fc7memory barrier/fence\u6765\u5b9e\u73b0ordering\uff1b \u5728preshing\u7684lockfree programming\u7cfb\u5217\u6587\u7ae0\u4e2d\u5df2\u7ecf\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u603b\u7ed3\uff0c\u56e0\u6b64\u672c\u7ae0\u53ea\u662f\u8fdb\u884c\u68b3\u7406\uff0c\u57fa\u672c\u4e0a\u6ca1\u6709\u6df1\u5165\u7684\u5185\u5bb9\uff1b","title":"\u5185\u5bb9\u6982\u8ff0"},{"location":"CPU-memory-access/CPU-memory-model/#_3","text":"\u672c\u7ae0\u9996\u5148\u4ecb\u7ecd\u4e00\u4e2aprogrammer\u5e73\u65f6\u4e0d\u4f1a\u53d1\u73b0\u3001\u4f46\u662f\u666e\u904d\u5b58\u5728\u7684\u7684memory reordering\uff0c\u4ee5\u53ca\u5b83\u6240\u5e26\u6765\u7684\u5f71\u54cd\uff1b \u7136\u540e\u6b63\u5f0f\u4ecb\u7ecdmemory ordering\uff1b \u7136\u540e\u4ecb\u7ecdcontrol\u624b\u6bb5: memory barrier\uff1b","title":"\u7ae0\u8282\u8bf4\u660e"},{"location":"CPU-memory-access/CPU-memory-model/CPU-Memory-Model/","text":"CPU memory model Weak and strong preshing Weak vs. Strong Memory Models A memory model tells you, for a given processor or toolchain, exactly what types of memory reordering to expect at runtime relative to a given source code listing. Keep in mind that the effects of memory reordering can only be observed when lock-free programming techniques are used. NOTE: program order and memory order Four type memory ordering and hardware memory model \u7d20\u6750 1\u3001preshing Weak vs. Strong Memory Models \u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3 2\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/CPU-Memory-Model/#cpu#memory#model","text":"","title":"CPU memory model"},{"location":"CPU-memory-access/CPU-memory-model/CPU-Memory-Model/#weak#and#strong","text":"","title":"Weak and strong"},{"location":"CPU-memory-access/CPU-memory-model/CPU-Memory-Model/#preshing#weak#vs#strong#memory#models","text":"A memory model tells you, for a given processor or toolchain, exactly what types of memory reordering to expect at runtime relative to a given source code listing. Keep in mind that the effects of memory reordering can only be observed when lock-free programming techniques are used. NOTE: program order and memory order","title":"preshing Weak vs. Strong Memory Models"},{"location":"CPU-memory-access/CPU-memory-model/CPU-Memory-Model/#four#type#memory#ordering#and#hardware#memory#model","text":"\u7d20\u6750 1\u3001preshing Weak vs. Strong Memory Models \u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u603b\u7ed3 2\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A","title":"Four type memory ordering and hardware memory model"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/","text":"Memory barrier 1\u3001\u5173\u4e8eMemory barrier\u4ecb\u7ecd\u5730\u6700\u597d\u7684\u6587\u7ae0\u662f preshing Memory Barriers Are Like Source Control Operations \uff0c\u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cbarrier\u548cfence\u610f\u601d\u662f\u76f8\u540c\u7684\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1: a\u3001stackoverflow Is memory fence and memory barrier same? b\u3001wikipedia Memory barrier wikipedia Memory barrier A memory barrier , also known as a membar , memory fence or fence instruction , is a type of barrier instruction that causes a central processing unit (CPU) or compiler to enforce an ordering constraint on memory operations issued before and after the barrier instruction. This typically means that operations issued prior to the barrier are guaranteed to be performed before operations issued after the barrier. NOTE: 1\u3001memory barrier \u548c memory fence\u7684\u610f\u601d\u662f\u76f8\u540c\u7684 Memory barriers are necessary because most modern CPUs employ performance optimizations that can result in out-of-order execution . This reordering of memory operations ( loads and stores ) normally goes unnoticed(\u672a\u6ce8\u610f) within a single thread of execution , but can cause unpredictable behaviour in concurrent programs and device drivers unless carefully controlled. The exact nature of an ordering constraint is hardware dependent and defined by the architecture's memory ordering model . Some architectures provide multiple barriers for enforcing different ordering constraints. NOTE: memory barrier\u5e94\u8be5\u8fd9\u6837\u6765\u8fdb\u884c\u7406\u89e3: \u7531\u4e8e out-of-order execution \uff0c\u5bfc\u81f4\u4e86\u5f88\u591a\u95ee\u9898\uff0c\u56e0\u6b64\u5f15\u5165memory barrier\u6765\u4f9bprogrammer\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b83\u7684\u601d\u8def\u5982\u4e0b: 1) ordering and computational\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Make-it-computational \u7ae0\u8282\u3002 2) control theory: \u7531programmer\u6765\u6dfb\u52a0\u663e\u5f0f\u7684\u63a7\u5236\u4ece\u800c\u4f7f\u4e4b\u6709\u5e8f\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e tf.control_dependency 3) instruction level barrier \u5173\u4e8e out-of-order execution \uff0c\u53c2\u89c1\u5de5\u7a0bhardware\u7684 CPU\\Execution-of-instruction\\Out-of-order-execution \u7ae0\u8282\uff1b Memory ordering model \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u91cd\u8981\u662f\u63cf\u8ff0\u7684programming language\u7684memory model\u3002 Memory barriers are typically used when implementing low-level machine code that operates on memory shared by multiple devices. Such code includes synchronization primitives and lock-free data structures on multiprocessor systems, and device drivers that communicate with computer hardware . NOTE: concurrent programming\u4e2d\u7684\u5f88\u591a\u5185\u5bb9\u90fd\u662f\u4f9d\u8d56\u4e8ememory barrier\u7684: 1 synchronization primitives 2 lock-free data structures on multiprocessor systems 3 device drivers that communicate with computer hardware stackoverflow What is a memory fence? A For performance gains modern CPUs often execute instructions out of order to make maximum use of the available silicon (including memory read/writes). Because the hardware enforces instructions integrity you never notice this in a single thread of execution . However for multiple threads or environments with volatile memory (memory mapped I/O for example) this can lead to unpredictable behavior. A memory fence/barrier is a class of instructions that mean memory read/writes occur in the order you expect. For example a 'full fence' means all read/writes before the fence are comitted before those after the fence. Note memory fences are a hardware concept. In higher level languages we are used to dealing with mutexes and semaphores - these may well be implemented using memory fences at the low level and explicit use of memory barriers are not necessary. Use of memory barriers requires a careful study of the hardware architecture and more commonly found in device drivers than application code. The CPU reordering is different from compiler optimisations - although the artefacts can be similar. You need to take separate measures to stop the compiler reordering your instructions if that may cause undesirable behaviour (e.g. use of the volatile keyword in C).","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/#memory#barrier","text":"1\u3001\u5173\u4e8eMemory barrier\u4ecb\u7ecd\u5730\u6700\u597d\u7684\u6587\u7ae0\u662f preshing Memory Barriers Are Like Source Control Operations \uff0c\u5176\u4e2d\u6709\u7740\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cbarrier\u548cfence\u610f\u601d\u662f\u76f8\u540c\u7684\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1: a\u3001stackoverflow Is memory fence and memory barrier same? b\u3001wikipedia Memory barrier","title":"Memory barrier"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/#wikipedia#memory#barrier","text":"A memory barrier , also known as a membar , memory fence or fence instruction , is a type of barrier instruction that causes a central processing unit (CPU) or compiler to enforce an ordering constraint on memory operations issued before and after the barrier instruction. This typically means that operations issued prior to the barrier are guaranteed to be performed before operations issued after the barrier. NOTE: 1\u3001memory barrier \u548c memory fence\u7684\u610f\u601d\u662f\u76f8\u540c\u7684 Memory barriers are necessary because most modern CPUs employ performance optimizations that can result in out-of-order execution . This reordering of memory operations ( loads and stores ) normally goes unnoticed(\u672a\u6ce8\u610f) within a single thread of execution , but can cause unpredictable behaviour in concurrent programs and device drivers unless carefully controlled. The exact nature of an ordering constraint is hardware dependent and defined by the architecture's memory ordering model . Some architectures provide multiple barriers for enforcing different ordering constraints. NOTE: memory barrier\u5e94\u8be5\u8fd9\u6837\u6765\u8fdb\u884c\u7406\u89e3: \u7531\u4e8e out-of-order execution \uff0c\u5bfc\u81f4\u4e86\u5f88\u591a\u95ee\u9898\uff0c\u56e0\u6b64\u5f15\u5165memory barrier\u6765\u4f9bprogrammer\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b83\u7684\u601d\u8def\u5982\u4e0b: 1) ordering and computational\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Make-it-computational \u7ae0\u8282\u3002 2) control theory: \u7531programmer\u6765\u6dfb\u52a0\u663e\u5f0f\u7684\u63a7\u5236\u4ece\u800c\u4f7f\u4e4b\u6709\u5e8f\uff0c\u975e\u5e38\u7c7b\u4f3c\u4e8e tf.control_dependency 3) instruction level barrier \u5173\u4e8e out-of-order execution \uff0c\u53c2\u89c1\u5de5\u7a0bhardware\u7684 CPU\\Execution-of-instruction\\Out-of-order-execution \u7ae0\u8282\uff1b Memory ordering model \u6240\u94fe\u63a5\u7684\u6587\u7ae0\u91cd\u8981\u662f\u63cf\u8ff0\u7684programming language\u7684memory model\u3002 Memory barriers are typically used when implementing low-level machine code that operates on memory shared by multiple devices. Such code includes synchronization primitives and lock-free data structures on multiprocessor systems, and device drivers that communicate with computer hardware . NOTE: concurrent programming\u4e2d\u7684\u5f88\u591a\u5185\u5bb9\u90fd\u662f\u4f9d\u8d56\u4e8ememory barrier\u7684: 1 synchronization primitives 2 lock-free data structures on multiprocessor systems 3 device drivers that communicate with computer hardware","title":"wikipedia Memory barrier"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/#stackoverflow#what#is#a#memory#fence","text":"","title":"stackoverflow What is a memory fence?"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/#a","text":"For performance gains modern CPUs often execute instructions out of order to make maximum use of the available silicon (including memory read/writes). Because the hardware enforces instructions integrity you never notice this in a single thread of execution . However for multiple threads or environments with volatile memory (memory mapped I/O for example) this can lead to unpredictable behavior. A memory fence/barrier is a class of instructions that mean memory read/writes occur in the order you expect. For example a 'full fence' means all read/writes before the fence are comitted before those after the fence. Note memory fences are a hardware concept. In higher level languages we are used to dealing with mutexes and semaphores - these may well be implemented using memory fences at the low level and explicit use of memory barriers are not necessary. Use of memory barriers requires a careful study of the hardware architecture and more commonly found in device drivers than application code. The CPU reordering is different from compiler optimisations - although the artefacts can be similar. You need to take separate measures to stop the compiler reordering your instructions if that may cause undesirable behaviour (e.g. use of the volatile keyword in C).","title":"A"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Compiler-memory-barrier/","text":"Compiler only memory barrier stackoverflow When is a compiler-only memory barrier (such as std::atomic_signal_fence) useful?","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Compiler-memory-barrier/#compiler#only#memory#barrier","text":"","title":"Compiler only memory barrier"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Compiler-memory-barrier/#stackoverflow#when#is#a#compiler-only#memory#barrier#such#as#stdatomic_signal_fence#useful","text":"","title":"stackoverflow When is a compiler-only memory barrier (such as std::atomic_signal_fence) useful?"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/","text":"kernel LINUX KERNEL MEMORY BARRIERS NOTE: \u8fd9\u662f\u975e\u5e38\u597d\u7684\u5185\u5bb9\uff0c\u5728 preshing Memory Barriers Are Like Source Control Operations \u7684\u6700\u540e\u4e5f\u63a8\u8350\u4e86\u8fd9\u7bc7\u6587\u7ae0\u3002 DISCLAIMER CONTENTS (*) Abstract memory access model. - Device operations. - Guarantees. (*) What are memory barriers? - Varieties of memory barrier. - What may not be assumed about memory barriers? - Data dependency barriers (historical). - Control dependencies. - SMP barrier pairing. - Examples of memory barrier sequences. - Read memory barriers vs load speculation. - Multicopy atomicity. (*) Explicit kernel barriers. - Compiler barrier. - CPU memory barriers. (*) Implicit kernel memory barriers. - Lock acquisition functions. - Interrupt disabling functions. - Sleep and wake-up functions. - Miscellaneous functions. (*) Inter-CPU acquiring barrier effects. - Acquires vs memory accesses. (*) Where are memory barriers needed? - Interprocessor interaction. - Atomic operations. - Accessing devices. - Interrupts. (*) Kernel I/O barrier effects. (*) Assumed minimum execution ordering model. (*) The effects of the cpu cache. - Cache coherency. - Cache coherency vs DMA. - Cache coherency vs MMIO. (*) The things CPUs get up to. - And then there's the Alpha. - Virtual Machine Guests. (*) Example uses. - Circular buffers. (*) References.","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#kernel#linux#kernel#memory#barriers","text":"NOTE: \u8fd9\u662f\u975e\u5e38\u597d\u7684\u5185\u5bb9\uff0c\u5728 preshing Memory Barriers Are Like Source Control Operations \u7684\u6700\u540e\u4e5f\u63a8\u8350\u4e86\u8fd9\u7bc7\u6587\u7ae0\u3002","title":"kernel LINUX KERNEL MEMORY BARRIERS"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#disclaimer","text":"","title":"DISCLAIMER"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#contents","text":"","title":"CONTENTS"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#abstract#memory#access#model","text":"- Device operations. - Guarantees.","title":"(*) Abstract memory access model."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#what#are#memory#barriers","text":"- Varieties of memory barrier. - What may not be assumed about memory barriers? - Data dependency barriers (historical). - Control dependencies. - SMP barrier pairing. - Examples of memory barrier sequences. - Read memory barriers vs load speculation. - Multicopy atomicity.","title":"(*) What are memory barriers?"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#explicit#kernel#barriers","text":"- Compiler barrier. - CPU memory barriers.","title":"(*) Explicit kernel barriers."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#implicit#kernel#memory#barriers","text":"- Lock acquisition functions. - Interrupt disabling functions. - Sleep and wake-up functions. - Miscellaneous functions.","title":"(*) Implicit kernel memory barriers."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#inter-cpu#acquiring#barrier#effects","text":"- Acquires vs memory accesses.","title":"(*) Inter-CPU acquiring barrier effects."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#where#are#memory#barriers#needed","text":"- Interprocessor interaction. - Atomic operations. - Accessing devices. - Interrupts.","title":"(*) Where are memory barriers needed?"},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#kernel#io#barrier#effects","text":"","title":"(*) Kernel I/O barrier effects."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#assumed#minimum#execution#ordering#model","text":"","title":"(*) Assumed minimum execution ordering model."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#the#effects#of#the#cpu#cache","text":"- Cache coherency. - Cache coherency vs DMA. - Cache coherency vs MMIO.","title":"(*) The effects of the cpu cache."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#the#things#cpus#get#up#to","text":"- And then there's the Alpha. - Virtual Machine Guests.","title":"(*) The things CPUs get up to."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#example#uses","text":"- Circular buffers.","title":"(*) Example uses."},{"location":"CPU-memory-access/CPU-memory-model/Memory-barrier/Linux-kernel-memory-barrier/#references","text":"","title":"(*) References."},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/","text":"Memory ordering wikipedia Memory ordering Memory ordering describes the order of accesses to computer memory by a CPU. The term can refer either to the memory ordering generated by the compiler during compile time , or to the memory ordering generated by a CPU during runtime . NOTE: compiler time\u548cruntime In modern microprocessors , memory ordering characterizes the CPUs ability to reorder memory operations - it is a type of out-of-order execution . Memory reordering can be used to fully utilize the bus-bandwidth of different types of memory such as caches and memory banks . NOTE: CPU\u7684\u4e00\u79cd\u4f18\u5316\u65b9\u5f0f On most modern uniprocessors memory operations are not executed in the order specified by the program code. NOTE: \u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u4e8b\u5b9e\uff0c\u53ef\u80fd\u4e0e\u6211\u4eec\u7684\u76f4\u89c9\u76f8\u8fdd\u80cc In single threaded programs all operations appear to have been executed in the order specified, with all out-of-order execution hidden to the programmer \u2013 however in multi-threaded environments (or when interfacing with other hardware via memory buses) this can lead to problems. NOTE: ordering\u5bf9programmer\u662f\u900f\u660e\u7684\u3002multi-threaded environments\u4e2d\u7684\u4e00\u4e2a\u5178\u578b\u95ee\u9898\u5c31\u662f\u4f7f\u7528 Double-checked locking \u6765\u5b9e\u73b0 Singleton pattern . To avoid problems memory barriers can be used in these cases. Compile-time memory ordering The compiler has some freedom to resort the order of operations during compile time . However this can lead to problems if the order of memory accesses is of importance. NOTE: \u5173\u4e8e Memory barrier \u7684\u8ba8\u8bba\u653e\u5230\u4e86 Memory-barrier \u7ae0\u8282\u3002 Runtime memory ordering In symmetric multiprocessing (SMP) microprocessor systems There are several memory-consistency models for SMP systems: 1\u3001Sequential consistency (all reads and all writes are in-order) 2\u3001Relaxed consistency (some types of reordering are allowed) Loads can be reordered after loads (for better working of cache coherency, better scaling) Loads can be reordered after stores Stores can be reordered after stores Stores can be reordered after loads NOTE: four type memory ordering 3\u3001Weak consistency (reads and writes are arbitrarily reordered, limited only by explicit memory barriers ) NOTE: \u5173\u4e8e Memory barrier \u7684\u8ba8\u8bba\u653e\u5230\u4e86 Memory-barrier \u7ae0\u8282\u3002 Four type memory ordering and semantic \u53c2\u8003: 1\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A 2\u3001preshing Memory Barriers Are Like Source Control Operations 3\u3001wikipedia Memory ordering CPU\u7684memory instruction\u6709\u4e24\u79cd: load(read)\u3001store(store)\uff0c\u56e0\u6b64\u53ef\u80fd\u7684reordering\u53ea\u6709\u5982\u4e0b\u56db\u79cd\u7ec4\u5408: 1\u3001\u4e0a\u8ff0table\u662f\u6e90\u81ea preshing Memory Barriers Are Like Source Control Operations \u3002 2\u3001\u4e0a\u8ff0\u56db\u79cdmemory reordering\u975e\u5e38\u91cd\u8981\uff0c\u540e\u7eed\u5f88\u591a\u5173\u4e8ememory\u7684\u5206\u6790\u90fd\u662f\u57fa\u4e8e\u5b83\u7684\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u79cdreordering\uff0cCPU\u5f80\u5f80\u63d0\u4f9b\u4e86\u5bf9\u5176\u8fdb\u884c\u63a7\u5236\u7684instruction\uff0c\u8fd9\u5c31\u662fmemory barrier\u3002\u63d0\u4f9bmemory barrier\uff0cprogrammer\u5bf9memory ordering\u8fdb\u884c\u63a7\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u76f8\u5e94\u7684memory semantic\uff0c\u4e0b\u9762\u662f\u5bf9\u6b64\u7684\u603b\u7ed3: \u4ecedata dependency\u7684\u89d2\u5ea6\u6765\u5206\u6790\u8fd9\u56db\u79cdordering 1\u3001load-load\u3001store-store\u3001load-store\u90fd\u6ca1\u6709data dependency\uff0c\u56e0\u6b64\u6267\u884c\u8fd9\u79cdreordering\uff0c\u4e00\u822c\u4e0d\u4f1a\u4f53\u73b0\u5bf9program order\u7684\u8fdd\u80cc\u3002 2\u3001store-load \u5b58\u5728data dependency(\u5199\u540e\uff0c\u80fd\u591f\u8bfb\u5230\u521a\u521a\u5199\u5165\u7684\u503c)\uff0c\u8fd9\u4f53\u73b0\u5728program order\u4e2d\uff0c\u5982\u679c\u5141\u8bb8\u8fd9\u79cdreordering\uff0c\u90a3\u4e48\u663e\u7136\u5c31\u8fdd\u53cd\u4e86 program order\uff0c\u663e\u7136\u5c31\u8fdd\u53cd\u4e86sequential consistency\u3002 NOTE: 1\u3001\u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662f\u7406\u89e3sequential consistency\u7684\u57fa\u7840 2\u3001\u5173\u4e8e\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\uff0c\u53c2\u89c1 : a\u3001\"preshing Memory Barriers Are Like Source Control Operations # #StoreLoad\"\u6bb5 b\u3001 std::memory_order # Sequentially-consistent ordering Reordering\u3001memory barrier\u3001memory semantic zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A : aquire\u8bed\u4e49 \uff1aload \u4e4b\u540e\u7684**\u8bfb\u5199\u64cd\u4f5c**\u65e0\u6cd5\u88ab\u91cd\u6392\u81f3 load \u4e4b\u524d\u3002\u5373 load-load, load-store \u4e0d\u80fd\u88ab\u91cd\u6392\u3002 release\u8bed\u4e49 \uff1astore \u4e4b\u524d\u7684**\u8bfb\u5199\u64cd\u4f5c**\u65e0\u6cd5\u88ab\u91cd\u6392\u81f3 store \u4e4b\u540e\u3002\u5373 load-store, store-store \u4e0d\u80fd\u88ab\u91cd\u6392\u3002 NOTE: 1\u3001\u53ef\u4ee5\u770b\u5230\uff0c\u6bd4\u8f83\u7279\u6b8a\u7684\u662f**load-store** sequential consistency \u4e0a\u8ff0\u56db\u79cdreordering\u90fd\u662f\u4e0d\u5141\u8bb8\u7684\u3002 \u603b\u7ed3 reordering \u542b\u4e49 memory barrier/fence load-load(read-read) acquire semantic store-store(write-write) release semantic load-store release semantic\u3001acquire semantic store-load sequential consistency 1\u3001\u542b\u4e49\u8fd9\u4e00\u5217\u7701\u7565\u4e86\uff0c\u53c2\u89c1 preshing Memory Barriers Are Like Source Control Operations \uff0c\u5176\u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u6700\u540e\u4e00\u5217\u7684\u542b\u4e49\u662f: \u5bf9\u4e8e\u6bcf\u4e00\u79cdreordering\uff0c\u90fd\u6709\u5bf9\u5e94\u7684memory barrier\u6765\u963b\u6b62\u5b83\uff0c\u6dfb\u52a0\u4e86\u5bf9\u5e94\u7684memory barrier\uff0c\u5c31\u80fd\u591f\u4fdd\u8bc1\u5bf9\u5e94\u7684semantic\u3002 Examples \u5728\u4e0b\u9762\u6587\u7ae0\u3001\u7ae0\u8282\u4e2d\uff0c\u8ba8\u8bba\u4e86Out of order execution \u548c memory reordering\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e00\u4e9b\u975e\u5e38\u597d\u7684\u4f8b\u5b50: 1\u3001\"aristeia-C++and-the-Perils-of-Double-Checked-Locking\" \u7ae0\u8282 \u5176\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d 2\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A 3\u3001 Concurrent-computing\\Expert-Jeff-Preshing\\Lock-Free-Programming \u4ed6\u7684\u8fd9\u4e2a\u7cfb\u5217\u7684\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u53ef\u4ee5\u4f5c\u4e3aguideline\u3002 4\u3001\u5de5\u7a0bhardware\u7684\u5982\u4e0b\u7ae0\u8282: a\u3001 CPU\\Execution-of-instruction b\u3001 CPU-memory-access\\Memory-ordering 5\u3001 acmqueue-Shared-Variables-or-Memory-Models 6\u3001stackoverflow How to understand acquire and release semantics? # A \u6536\u5f55\u5728\u5de5\u7a0bprogramming language\u7684 Release-acquire \u7ae0\u8282\u4e2d\uff0c\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\u3002 7\u3001cppreference std::memory_order # Explanation # Relaxed ordering \u5176\u4e2d\u7ed9\u51fa\u4e86example\uff0c\u975e\u5e38\u597d\u3002","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#memory#ordering","text":"","title":"Memory ordering"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#wikipedia#memory#ordering","text":"Memory ordering describes the order of accesses to computer memory by a CPU. The term can refer either to the memory ordering generated by the compiler during compile time , or to the memory ordering generated by a CPU during runtime . NOTE: compiler time\u548cruntime In modern microprocessors , memory ordering characterizes the CPUs ability to reorder memory operations - it is a type of out-of-order execution . Memory reordering can be used to fully utilize the bus-bandwidth of different types of memory such as caches and memory banks . NOTE: CPU\u7684\u4e00\u79cd\u4f18\u5316\u65b9\u5f0f On most modern uniprocessors memory operations are not executed in the order specified by the program code. NOTE: \u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u4e8b\u5b9e\uff0c\u53ef\u80fd\u4e0e\u6211\u4eec\u7684\u76f4\u89c9\u76f8\u8fdd\u80cc In single threaded programs all operations appear to have been executed in the order specified, with all out-of-order execution hidden to the programmer \u2013 however in multi-threaded environments (or when interfacing with other hardware via memory buses) this can lead to problems. NOTE: ordering\u5bf9programmer\u662f\u900f\u660e\u7684\u3002multi-threaded environments\u4e2d\u7684\u4e00\u4e2a\u5178\u578b\u95ee\u9898\u5c31\u662f\u4f7f\u7528 Double-checked locking \u6765\u5b9e\u73b0 Singleton pattern . To avoid problems memory barriers can be used in these cases.","title":"wikipedia Memory ordering"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#compile-time#memory#ordering","text":"The compiler has some freedom to resort the order of operations during compile time . However this can lead to problems if the order of memory accesses is of importance. NOTE: \u5173\u4e8e Memory barrier \u7684\u8ba8\u8bba\u653e\u5230\u4e86 Memory-barrier \u7ae0\u8282\u3002","title":"Compile-time memory ordering"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#runtime#memory#ordering","text":"","title":"Runtime memory ordering"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#in#symmetric#multiprocessing#smp#microprocessor#systems","text":"There are several memory-consistency models for SMP systems: 1\u3001Sequential consistency (all reads and all writes are in-order) 2\u3001Relaxed consistency (some types of reordering are allowed) Loads can be reordered after loads (for better working of cache coherency, better scaling) Loads can be reordered after stores Stores can be reordered after stores Stores can be reordered after loads NOTE: four type memory ordering 3\u3001Weak consistency (reads and writes are arbitrarily reordered, limited only by explicit memory barriers ) NOTE: \u5173\u4e8e Memory barrier \u7684\u8ba8\u8bba\u653e\u5230\u4e86 Memory-barrier \u7ae0\u8282\u3002","title":"In symmetric multiprocessing (SMP) microprocessor systems"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#four#type#memory#ordering#and#semantic","text":"\u53c2\u8003: 1\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A 2\u3001preshing Memory Barriers Are Like Source Control Operations 3\u3001wikipedia Memory ordering CPU\u7684memory instruction\u6709\u4e24\u79cd: load(read)\u3001store(store)\uff0c\u56e0\u6b64\u53ef\u80fd\u7684reordering\u53ea\u6709\u5982\u4e0b\u56db\u79cd\u7ec4\u5408: 1\u3001\u4e0a\u8ff0table\u662f\u6e90\u81ea preshing Memory Barriers Are Like Source Control Operations \u3002 2\u3001\u4e0a\u8ff0\u56db\u79cdmemory reordering\u975e\u5e38\u91cd\u8981\uff0c\u540e\u7eed\u5f88\u591a\u5173\u4e8ememory\u7684\u5206\u6790\u90fd\u662f\u57fa\u4e8e\u5b83\u7684\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u79cdreordering\uff0cCPU\u5f80\u5f80\u63d0\u4f9b\u4e86\u5bf9\u5176\u8fdb\u884c\u63a7\u5236\u7684instruction\uff0c\u8fd9\u5c31\u662fmemory barrier\u3002\u63d0\u4f9bmemory barrier\uff0cprogrammer\u5bf9memory ordering\u8fdb\u884c\u63a7\u5236\uff0c\u4ece\u800c\u5b9e\u73b0\u76f8\u5e94\u7684memory semantic\uff0c\u4e0b\u9762\u662f\u5bf9\u6b64\u7684\u603b\u7ed3:","title":"Four type memory ordering and semantic"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#data#dependencyordering","text":"1\u3001load-load\u3001store-store\u3001load-store\u90fd\u6ca1\u6709data dependency\uff0c\u56e0\u6b64\u6267\u884c\u8fd9\u79cdreordering\uff0c\u4e00\u822c\u4e0d\u4f1a\u4f53\u73b0\u5bf9program order\u7684\u8fdd\u80cc\u3002 2\u3001store-load \u5b58\u5728data dependency(\u5199\u540e\uff0c\u80fd\u591f\u8bfb\u5230\u521a\u521a\u5199\u5165\u7684\u503c)\uff0c\u8fd9\u4f53\u73b0\u5728program order\u4e2d\uff0c\u5982\u679c\u5141\u8bb8\u8fd9\u79cdreordering\uff0c\u90a3\u4e48\u663e\u7136\u5c31\u8fdd\u53cd\u4e86 program order\uff0c\u663e\u7136\u5c31\u8fdd\u53cd\u4e86sequential consistency\u3002 NOTE: 1\u3001\u8fd9\u6bb5\u603b\u7ed3\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u662f\u7406\u89e3sequential consistency\u7684\u57fa\u7840 2\u3001\u5173\u4e8e\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\uff0c\u53c2\u89c1 : a\u3001\"preshing Memory Barriers Are Like Source Control Operations # #StoreLoad\"\u6bb5 b\u3001 std::memory_order # Sequentially-consistent ordering","title":"\u4ecedata dependency\u7684\u89d2\u5ea6\u6765\u5206\u6790\u8fd9\u56db\u79cdordering"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#reorderingmemory#barriermemory#semantic","text":"zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A : aquire\u8bed\u4e49 \uff1aload \u4e4b\u540e\u7684**\u8bfb\u5199\u64cd\u4f5c**\u65e0\u6cd5\u88ab\u91cd\u6392\u81f3 load \u4e4b\u524d\u3002\u5373 load-load, load-store \u4e0d\u80fd\u88ab\u91cd\u6392\u3002 release\u8bed\u4e49 \uff1astore \u4e4b\u524d\u7684**\u8bfb\u5199\u64cd\u4f5c**\u65e0\u6cd5\u88ab\u91cd\u6392\u81f3 store \u4e4b\u540e\u3002\u5373 load-store, store-store \u4e0d\u80fd\u88ab\u91cd\u6392\u3002 NOTE: 1\u3001\u53ef\u4ee5\u770b\u5230\uff0c\u6bd4\u8f83\u7279\u6b8a\u7684\u662f**load-store** sequential consistency \u4e0a\u8ff0\u56db\u79cdreordering\u90fd\u662f\u4e0d\u5141\u8bb8\u7684\u3002","title":"Reordering\u3001memory barrier\u3001memory semantic"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#_1","text":"reordering \u542b\u4e49 memory barrier/fence load-load(read-read) acquire semantic store-store(write-write) release semantic load-store release semantic\u3001acquire semantic store-load sequential consistency 1\u3001\u542b\u4e49\u8fd9\u4e00\u5217\u7701\u7565\u4e86\uff0c\u53c2\u89c1 preshing Memory Barriers Are Like Source Control Operations \uff0c\u5176\u4e2d\u6709\u975e\u5e38\u597d\u7684\u63cf\u8ff0\u3002 2\u3001\u6700\u540e\u4e00\u5217\u7684\u542b\u4e49\u662f: \u5bf9\u4e8e\u6bcf\u4e00\u79cdreordering\uff0c\u90fd\u6709\u5bf9\u5e94\u7684memory barrier\u6765\u963b\u6b62\u5b83\uff0c\u6dfb\u52a0\u4e86\u5bf9\u5e94\u7684memory barrier\uff0c\u5c31\u80fd\u591f\u4fdd\u8bc1\u5bf9\u5e94\u7684semantic\u3002","title":"\u603b\u7ed3"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/#examples","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u3001\u7ae0\u8282\u4e2d\uff0c\u8ba8\u8bba\u4e86Out of order execution \u548c memory reordering\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e00\u4e9b\u975e\u5e38\u597d\u7684\u4f8b\u5b50: 1\u3001\"aristeia-C++and-the-Perils-of-Double-Checked-Locking\" \u7ae0\u8282 \u5176\u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u4ecb\u7ecd\uff0c\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d 2\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A 3\u3001 Concurrent-computing\\Expert-Jeff-Preshing\\Lock-Free-Programming \u4ed6\u7684\u8fd9\u4e2a\u7cfb\u5217\u7684\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u53ef\u4ee5\u4f5c\u4e3aguideline\u3002 4\u3001\u5de5\u7a0bhardware\u7684\u5982\u4e0b\u7ae0\u8282: a\u3001 CPU\\Execution-of-instruction b\u3001 CPU-memory-access\\Memory-ordering 5\u3001 acmqueue-Shared-Variables-or-Memory-Models 6\u3001stackoverflow How to understand acquire and release semantics? # A \u6536\u5f55\u5728\u5de5\u7a0bprogramming language\u7684 Release-acquire \u7ae0\u8282\u4e2d\uff0c\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\u3002 7\u3001cppreference std::memory_order # Explanation # Relaxed ordering \u5176\u4e2d\u7ed9\u51fa\u4e86example\uff0c\u975e\u5e38\u597d\u3002","title":"Examples"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/How-does-memory-reordering-help-processors-and-compilers/","text":"How does memory reordering help processors and compilers? \u603b\u7684\u6765\u8bf4\uff0c\u5c31\u662foptimization principle\u3002 stackoverflow How does memory reordering help processors and compilers?","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/How-does-memory-reordering-help-processors-and-compilers/#how#does#memory#reordering#help#processors#and#compilers","text":"\u603b\u7684\u6765\u8bf4\uff0c\u5c31\u662foptimization principle\u3002","title":"How does memory reordering help processors and compilers?"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/How-does-memory-reordering-help-processors-and-compilers/#stackoverflow#how#does#memory#reordering#help#processors#and#compilers","text":"","title":"stackoverflow How does memory reordering help processors and compilers?"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/Memory-Ordering-at-Compile-Time/","text":"Memory Ordering at Compile Time \u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u6df1\u5165\u8ba8\u8bba\u8fd9\u4e2a\u95ee\u9898: 1\u3001preshing Memory Ordering at Compile Time \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d 2\u3001stackoverflow How to understand acquire and release semantics? # A \u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u597d Thoughts Compiler memory optimization compiler\u80fd\u591f\u5168\u5c40\u5730\u3001\u9ad8\u5c4b\u5efa\u74f4\u5730\u5206\u6790\u6211\u4eec\u7684\u7a0b\u5e8f\uff0c\u80fd\u591f\u901a\u8fc7\u5bf9memory\u64cd\u4f5c\u987a\u5e8f\u6765\u8fdb\u884coptimization\u3002\u9700\u8981\u6574\u7406\u8fd9\u65b9\u9762\u6587\u7ae0\u3002","title":"Introduction"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/Memory-Ordering-at-Compile-Time/#memory#ordering#at#compile#time","text":"\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u6df1\u5165\u8ba8\u8bba\u8fd9\u4e2a\u95ee\u9898: 1\u3001preshing Memory Ordering at Compile Time \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d 2\u3001stackoverflow How to understand acquire and release semantics? # A \u8fd9\u7bc7\u6587\u7ae0\u6bd4\u8f83\u597d","title":"Memory Ordering at Compile Time"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/Memory-Ordering-at-Compile-Time/#thoughts","text":"","title":"Thoughts"},{"location":"CPU-memory-access/CPU-memory-model/Memory-ordering/Memory-Ordering-at-Compile-Time/#compiler#memory#optimization","text":"compiler\u80fd\u591f\u5168\u5c40\u5730\u3001\u9ad8\u5c4b\u5efa\u74f4\u5730\u5206\u6790\u6211\u4eec\u7684\u7a0b\u5e8f\uff0c\u80fd\u591f\u901a\u8fc7\u5bf9memory\u64cd\u4f5c\u987a\u5e8f\u6765\u8fdb\u884coptimization\u3002\u9700\u8981\u6574\u7406\u8fd9\u65b9\u9762\u6587\u7ae0\u3002","title":"Compiler memory optimization"},{"location":"CPU-memory-access/Endianess/","text":"Endianness Endianness in software engineering \u672c\u8282\u603b\u7ed3\u5728software engineering\u4e2d\u7684\u4e00\u4e9b\u5173\u4e8eendian\u7684\u5185\u5bb9\uff0c\u5b83\u80fd\u591f\u542f\u793a\u6211\u4eec\uff0c\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u8003\u8651endian\u3002 C-family language object representation \u4eceC++\u548cC\u6807\u51c6\u6765\u770b\uff0c\u201cendianness\u201d\u51b3\u5b9a\u4e86object\u7684value\u7684memory representation\uff0c\u5173\u4e8ememory representation\uff0c\u53c2\u89c1\uff1a \u5de5\u7a0bcomputer-arithmetic\u7684 Bitwise-operation\\Binary-representation \u7ae0\u8282\u7684\u5185\u5bb9\u3002 \u5173\u4e8eobject\u3001value\u3001\u548cmemory representation\uff0c\u5728cppreference Object#Object representation and value representation \u4e2d\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e\uff0c\u5176\u4e2d\u7684object representation\u5c31\u662f\u524d\u9762\u6240\u8bf4\u7684memory representation\u3002 Sqlite file format \u5728 About SQLite \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0 The database file format is cross-platform - you can freely copy a database between 32-bit and 64-bit systems or between big-endian and little-endian architectures. These features make SQLite a popular choice as an Application File Format . TODO C network library \u5728C network library\u4e2d\u63d0\u4f9b\u4e86endian\u8f6c\u6362\u51fd\u6570\u3002 wikipedia Endianness In computing , endianness refers to the order of bytes (or sometimes bits ) within a binary representation of a number. It can also be used more generally to refer to the internal ordering of any representation, such as the digits in a numeral system or the sections of a date . NOTE: \u6211\u4eec\u4e3b\u8981\u5173\u6ce8\u7684\u662fbyte ordering In its most common usage, endianness indicates the ordering of bytes within a multi-byte number. A little-endian ordering places the least significant byte first and the most significant byte last, while a big-endian ordering does the opposite. For example, consider the unsigned hexadecimal number 0x1234 , which requires at least two bytes to represent. In a little-endian ordering, the bytes would be arranged [ 0x34, 0x12 ] , while in a big-endian ordering they would be [ 0x12, 0x34 ] . NOTE : 0x1234 \u8868\u7684\u662f\u4e00\u4e2a\u6570\uff0c\u5373\u6211\u4eec\u524d\u9762\u6240\u8ff0\u7684value\uff0c\u6211\u4eec\u7684\u8ba4\u8bfb\u987a\u5e8f\u662f\u4ece\u53f3\u5f80\u5de6\uff0c\u5373 0X34 \u662f\u6700\u4f4e\u6709\u6548\u4f4d\uff0c\u800c 0X12 \u662f\u6700\u9ad8\u6709\u6548\u4f4d\uff1b [ 0x34, 0x12 ] \u8868\u793a\u7684\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u987a\u5e8f\u662f\u4ece\u5de6\u5f80\u53f3\uff0c\u5373\u5b83\u7684\u7b2c0\u4e2a\u5143\u7d20\u662f 0X34 \uff0c\u7b2c1\u4e2a\u5143\u7d20\u662f 0x12 \uff1b\u663e\u7136\uff0c numeric literals \u662fbig-endian\uff0c\u6211\u4eec\u4f1a\u628a\u6700\u9ad8\u6709\u6548\u4f4d\u653e\u5230\u4f4e\u5730\u5740 Historically, various methods of endianness have been used in computing, including exotic forms such as middle-endianness. Today, however, little-endianness is the dominant ordering for processor architectures ( x86 , most ARM implementations) and their associated memory . Conversely, big-endianness is the dominant ordering in networking protocols ( IP , TCP , UDP ). File formats can use either ordering; in fact, some formats use a mixture of both. NOTE: \u4e24\u8005\u5404\u6709\u4f18\u52bf\uff0c\u5728\u540e\u9762\u4f1a\u5bf9\u5b83\u4eec\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u6790\u8bf4\u660e\uff1b\u8fd9\u5c31\u9020\u6210\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u9886\u57df\u7684application\u3002 In left-to-right scripts , numbers are written with their digits in big-endian order. Similarly, programming languages use big-endian digit ordering for numeric literals as well as big-endian language (\u201cleft\u201d and \u201cright\u201d) for bit-shift operations, regardless of the endianness of the target architecture. This can lead to confusion when interacting with little-endian numbers. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u89e3\u7b54\u4e86\u6211\u5173\u4e8ebitwise-operation\u548cendian\u4e4b\u95f4\u5173\u7cfb\u7684\u7591\u60d1\uff0c\u5b83\u603b\u7ed3\u5f97\u975e\u5e38\u597d\u3002\u5173\u4e8ebitwise-operation\uff0c\u53c2\u89c1\u5de5\u7a0bcomputer-arithmetic\u7684 Bitwise-operation\\Bitwise-operation.md \u3002 \u5bf9\u4e8eprogrammer\u800c\u8a00\u7684\u8bdd\uff0c\u5e94\u8be5\u8fd9\u6837\u601d\u8003\uff1a\u6211\u4eec\u5728\u8fdb\u884cprogram\u7684\u65f6\u5019\uff0c\u6211\u4eec\u662f\u4f7f\u7528\u7684value\uff0c\u6211\u4eec\u662f\u4e0d\u9700\u8981\u5173\u6ce8\u8fd9\u4e2avalue\u7684memory representation\u7684\uff0c\u4e5f\u5c31\u662f\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u662f\u4e0d\u9700\u8981\u5173\u6ce8endian\uff0c\u6b63\u5982\u524d\u9762\u6240\u8a00\u201c\u201cendianness\u201d\u51b3\u5b9a\u4e86value\u7684memory representation\u3002\u201d\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5bf9\u4e8eprogrammer\u800c\u8a00\u662f\u900f\u660e\u7684\uff0c\u5b83\u5f80\u5f80\u662f\u7531compiler\u6765\u5b9e\u73b0\u7684\u3002 Illustration The following two descriptive illustrations assume a normal reading and writing convention of left to right , where the left-most digit or character therefore corresponds to data being sent or received first, or being in the lowest address in memory, and the right-most digit or character corresponds to the data being sent or received last, or being in the highest address in memory. Big-endianness may be demonstrated by writing a decimal number, say one hundred twenty-three, on paper in the usual positional notation understood by a numerate reader: 123 . The digits are written starting from the left and to the right, with the most significant digit, 1 , written first. This is analogous to the lowest address of memory being used first. This is an example of a big-endian convention taken from daily life. The little-endian way of writing the same number, one hundred twenty-three, would place the hundreds-digit 1 in the right-most position: 321 . A person following conventional big-endian place-value order, who is not aware of this special ordering, would read a different number: three hundred and twenty one. Endianness in computing is similar, but it usually applies to the ordering of bytes, rather than of digits. The illustrations to the right, where a is a memory address, show big-endian and little-endian storage in memory. Hardware Computer memory consists of a sequence of storage cells. Each cell is identified in hardware and software by its memory address . If the total number of storage cells in memory is n , then addresses are enumerated from 0 to n-1 . Computer programs often use data structures of fields that may consist of more data than is stored in one memory cell. For the purpose of this article where its use as an operand of an instruction is relevant, a field consists of a consecutive sequence of bytes and represents a simple data value. In addition to that, it has to be of numeric type in some positional number system (mostly base-10 or base-2 \u2013 or base-256 in case of 8-bit bytes).[ 5] In such a number system the \"value\" of a digit is determined not only by its value as a single digit, but also by the position it holds in the complete number, its \"significance\". These positions can be mapped to memory mainly in two ways:[ 6] increasing numeric significance with increasing memory addresses (or increasing time), known as little-endian , and decreasing numeric significance with increasing memory addresses (or increasing time), known as big-endian [ 7] Current architectures The Intel x86 and also AMD64 / x86-64 series of processors use the little-endian format. Recently designed instruction set architectures typically follow this convention, either allowing only little-endian mode (e.g. RISC-V , Nios II , Andes Technology NDS32, or Qualcomm Hexagon ), or running mostly little-endian software on a bi-endian architecture (e.g. ARM Aarch64 , C-Sky ). Some big-endian architectures that remain popular include mostly older examples like the IBM z/Architecture , Freescale ColdFire (which is Motorola 68000 series -based) and Atmel AVR32 , but also the more recent OpenRISC . The IBM AIX and Oracle Solaris operating systems on bi-endian Power ISA and SPARC run in big-endian mode, while Linux on Power has moved to little-endian mode for new distributions. Floating point Although the ubiquitous x86 processors of today use little-endian storage for all types of data (integer, floating point, BCD ), there are a number of hardware architectures where floating-point numbers are represented in big-endian form while integers are represented in little-endian form.[ 15] There are ARM processors that have half little-endian, half big-endian floating-point representation for double-precision numbers: both 32-bit words are stored in little-endian like integer registers, but the most significant one first. Because there have been many floating-point formats with no \" network \" standard representation for them, the XDR standard uses big-endian IEEE 754 as its representation. It may therefore appear strange that the widespread IEEE 754 floating-point standard does not specify endianness.[ 16] Theoretically, this means that even standard IEEE floating-point data written by one machine might not be readable by another. However, on modern standard computers (i.e., implementing IEEE 754), one may in practice safely assume that the endianness is the same for floating-point numbers as for integers, making the conversion straightforward regardless of data type. (Small embedded systems using special floating-point formats may be another matter however.) Optimization The little-endian system has the property that the same value can be read from memory at different lengths without using different addresses (even when alignment restrictions are imposed). For example, a 32-bit memory location with content 4A 00 00 00 can be read at the same address as either 8-bit (value = 4A ), 16-bit ( 004A ), 24-bit ( 00004A ), or 32-bit ( 0000004A ), all of which retain the same numeric value. Although this little-endian property is rarely used directly by high-level programmers, it is often employed by code optimizers as well as by assembly language programmers. In more concrete terms, such optimizations are the equivalent of the following C code returning true on most little-endian systems: NOTE: \u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8868\u8fbe\u7684\u601d\u60f3\uff0c\u9700\u8981\u641e\u6e05\u695a\uff1avalue\u3001memory representation\u3001endian\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5728\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C++\\Language-reference\\Basic-concept\\Data-model\\Object\\Object.md \u4e2d\uff0c\u6709\u8fc7\u8fd9\u65b9\u9762\u7684\u8ba8\u8bba\uff1aType determines the interpretion of memory representation, and further determine value\u3002\u4e0b\u9762\u7ed3\u5408endian\u8fdb\u884c\u66f4\u52a0\u6df1\u5165\u7684\u8bba\u8ff0\uff1a \u4e0a\u8ff0\u201ca 32-bit memory location with content 4A 00 00 00 \u201d\u4e2d\u7684 4A 00 00 00 \u662fmemory representation\uff1a \u7b2c\u4e00\u5b57\u8282\uff1a 4A \u7b2c\u4e8c\u5b57\u8282\uff1a 00 \u7b2c\u4e09\u5b57\u8282\uff1a 00 \u7b2c\u56db\u5b57\u8282\uff1a 00 \u4e0d\u540c\u7684endian\u4e0b\uff0c\u5b83\u8868\u793a\u7684value\u662f\u4e0d\u540c\u7684\uff1a \u6700\u9ad8/\u6700\u4f4e\u6709\u6548\u4f4d memory representation in bit value big endian \u6700\u9ad8: 4A \u6700\u4f4e: 00 0100 1010 0000 0000 0000 0000 0000 0000 2^{30} + 2^{27} + 2^{25} 2^{30} + 2^{27} + 2^{25} little endian \u6700\u9ad8: 00 \u6700\u4f4e: 4A 0100 1010 0000 0000 0000 0000 0000 0000 2^6 + 2^3 + 2^1 2^6 + 2^3 + 2^1 \u901a\u8fc7\u4e0a\u8ff0\u89e3\u91ca\uff0c\u5c31\u80fd\u591f\u7406\u89e3\u539f\u6587\u4e2d\"The little-endian system has the property that the same value can be read from memory at different lengths without using different addresses (even when alignment restrictions are imposed). \"\u7684\u542b\u4e49\u4e86\uff0c\u4e0b\u9762\u662f\u6211\u7684\u76f4\u89c2\u7684\u7406\u89e3\uff1a \u5bf9\u4e8elittle endian\uff0c\u7ed9\u5b9amemory representation\uff0cCPU\u5bf9memory representation\u7684interpretion\u662f\u53ef\u4ee5\u6e10\u8fdb\u5f0f\u5730\u8ba1\u7b97\u7684\uff1a\u5373\u4ece\u4f4e\u5730\u5740\u5f00\u59cb\u9010\u6b65\u8ba1\u7b97\uff0c\u56e0\u4e3a**\u6700\u4f4e\u6709\u6548\u4f4d**\u4f4d\u4e8e**\u4f4e\u5730\u5740**\uff0c\u5bf9\u4e8e\u4f4e\u5730\u5740\u7684\u503c\u7684\u8ba1\u7b97\u662f\u4e0d\u53d7\u540e\u9762\u7684\u9ad8\u5730\u5740\u7684\u503c\u7684\u5f71\u54cd\u3002 \u5bf9\u4e8ebig endain\uff0c\u4e0elittle endian\u662f\u6b63\u597d\u76f8\u53cd\u7684\u3002 \u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u6781\u597d\u7684\u5c55\u793a\u4e86little endian\u7684\u8fd9\u79cdattribute\uff1a #include \"stdint.h\" #include \"stdio.h\" int main ( void ) { union u_t { uint8_t u8 ; uint16_t u16 ; uint32_t u32 ; uint64_t u64 ; } u = { . u64 = 0x4A }; puts ( u . u8 == u . u16 && u . u8 == u . u32 && u . u8 == u . u64 ? \"true\" : \"false\" ); } // gcc test.c NOTE: 0x4A \u4f4d\u4e8e\u4f4e\u5730\u5740\uff0c\u5b83\u53ea\u9700\u8981\u4e00\u4e2a\u5b57\u8282\u5373\u53ef\u3002 While not allowed by C++, such Type punning code is allowed as \"implementation-defined\" by the C11 standard[ 17] and commonly used[ 18] in code interacting with hardware.[ 19] On the other hand, in some situations it may be useful to obtain an approximation of a multi-byte or multi-word value by reading only its most significant portion instead of the complete representation; a big-endian processor may read such an approximation using the same base-address that would be used for the full value. NOTE: \u56e0\u4e3a\u5bf9\u4e8ebig-endian\u800c\u8a00\uff0c\u5b83\u7684most significant bit\u4f4d\u4e8e\u4f4e\u5730\u5740 Optimizations of this kind are not portable across systems of different endianness. Calculation order Little-endian representation simplifies hardware in processors that add multi-byte integral values a byte at a time, such as small-scale byte-addressable processors and microcontrollers . As carry propagation\uff08\u8fdb\u4f4d\uff09 must start at the least significant bit (and thus byte), multi-byte addition can then be carried out with a monotonically-incrementing address sequence, a simple operation already present in hardware. On a big-endian processor, its addressing unit has to be told how big the addition is going to be so that it can hop forward to the least significant byte, then count back down towards the most significant byte ( MSB ). On the other hand, arithmetic division is done starting from the MSB , so it is more natural for big-endian processors. However, high-performance processors usually fetch typical multi-byte operands from memory in the same amount of time they would have fetched a single byte, so the complexity of the hardware is not affected by the byte ordering . Mapping multi-byte binary values to memory We can assume that as we write text left to right , we are increasing the 'address' on paper, as a processor would write bytes with increasing memory addresses \u2014 as in the adjacent table. On paper, the hex value 0a0b0c0d (written 168496141 in usual decimal notation) is big-endian style since we write the most significant digit first and the rest follow in ***de*creasing** significance. Mapping this number as a binary value to a sequence of 4 bytes in memory in big-endian style also writes the bytes from left to right in ***de*creasing** significance: 0Ah at +0, 0Bh at +1, 0Ch at +2, 0Dh at +3. On a little-endian system, the bytes are written from left to right in ***in**creasing* significance, starting with the one's byte: 0Dh at +0, 0Ch at +1, 0Bh at +2, 0Ah at +3. Writing a 32-bit binary value to a memory location on a little-endian system and outputting the memory location (with growing addresses from left to right) shows that the order is reversed (byte-swapped) compared to usual big-endian notation. This is the way a hexdump is displayed: because the dumping program is unable to know what kind of data it is dumping, the only orientation it can observe is monotonically increasing addresses. The human reader, however, who knows that they are reading a hexdump of a little-endian system and who knows what kind of data they are reading, reads the byte sequence 0Dh , 0Ch , 0Bh , 0Ah as the 32-bit binary value 168496141, or 0x0a0b0c0d in hexadecimal notation. (Of course, this is not the same as the Examples A C programming example Consider the following C language program: #include <stdio.h> int main ( void ) { union { unsigned int word ; // a 32-bit integer unsigned char bytes [ 4 ]; } u ; u . word = 0x0A0B0C0D ; for ( int i = 0 ; i < 4 ; i ++ ) { printf ( \"byte[%d] = 0x%x \\n \" , i , ( int ) u . bytes [ i ]); } } On a little-endian computer, this program would output: byte [ 0 ] = 0xd byte [ 1 ] = 0xc byte [ 2 ] = 0xb byte [ 3 ] = 0xa On a big-endian computer, this program would output: byte [ 0 ] = 0xa byte [ 1 ] = 0xb byte [ 2 ] = 0xc byte [ 3 ] = 0xd Files and byte swap Endianness is a problem when a binary file created on a computer is read on another computer with different endianness. Some CPU instruction sets provide native support for endian byte swapping, such as bswap [ 21] ( x86 - 486 and later), and rev [ 22] ( ARMv6 and later). Some compilers have built-in facilities to deal with data written in other formats. For example, the Intel Fortran compiler supports the non-standard CONVERT specifier, so a file can be opened as OPEN(unit,CONVERT='BIG_ENDIAN',...) or OPEN(unit,CONVERT='LITTLE_ENDIAN',...) Some compilers have options to generate code that globally enables the conversion for all file IO operations. This allows programmers to reuse code on a system with the opposite endianness without having to modify the code itself. If the compiler does not support such conversion, the programmer needs to swap the bytes via ad hoc \uff08\u7279\u6b8a\u7684\uff0c\u4e13\u95e8\u7684\uff09 code. Unicode text can optionally start with a byte order mark (BOM) to signal the endianness of the file or stream. Its code point is U+FEFF . In UTF-32 for example, a big-endian file should start with 00 00 FE FF ; a little-endian should start with FF FE 00 00 . Application binary data formats, such as for example MATLAB .mat files, or the .BIL data format, used in topography, are usually endianness-independent. This is achieved by: storing the data always in one fixed endianness, or carrying with the data a switch to indicate which endianness the data was written with. When reading the file, the application converts the endianness, invisibly from the user. An example of the first case is the binary XLS file format that is portable between Windows and Mac systems and always little-endian, leaving the Mac application to swap the bytes on load and save when running on a big-endian Motorola 68K or PowerPC processor.[ 23] TIFF image files are an example of the second strategy, whose header instructs the application about endianness of their internal binary integers. If a file starts with the signature \" MM \" it means that integers are represented as big-endian, while \" II \" means little-endian. Those signatures need a single 16-bit word each, and they are palindromes (that is, they read the same forwards and backwards), so they are endianness independent. \" I \" stands for Intel and \" M \" stands for Motorola , the respective CPU providers of the IBM PC compatibles (Intel) and Apple Macintosh platforms (Motorola) in the 1980s. Intel CPUs are little-endian, while Motorola 680x0 CPUs are big-endian. This explicit signature allows a TIFF reader program to swap bytes if necessary when a given file was generated by a TIFF writer program running on a computer with a different endianness. Since the required byte swap depends on the size of the numbers stored in the file (two 2-byte integers require a different swap than one 4-byte integer), the file format must be known to perform endianness conversion. /* C function to change endianness for byte swap in an unsigned 32-bit integer */ uint32_t ChangeEndianness ( uint32_t value ) { uint32_t result = 0 ; result |= ( value & 0x000000FF ) << 24 ; result |= ( value & 0x0000FF00 ) << 8 ; result |= ( value & 0x00FF0000 ) >> 8 ; result |= ( value & 0xFF000000 ) >> 24 ; return result ; } NOTE :\u5173\u4e8e\u4e0a\u9762\u8fd9\u6bb5\u4ee3\u7801\u7684\u7ec6\u81f4\u5206\u6790\uff0c\u53c2\u89c1 Endianness conversion in C File systems Networking Many IETF RFCs use the term network order , meaning the order of transmission for bits and bytes over the wire in network protocols . Among others, the historic RFC 1700 (also known as Internet standard STD 2) has defined the network order for protocols in the Internet protocol suite to be big-endian , hence the use of the term \"network byte order\" for big-endian byte order.[ 25] However, not all protocols use big-endian byte order as the network order. The Server Message Block (SMB) protocol uses little-endian byte order. In CANopen , multi-byte parameters are always sent least significant byte first (little-endian). The same is true for Ethernet Powerlink .[ 26] The Berkeley sockets API defines a set of functions to convert 16-bit and 32-bit integers to and from network byte order: the htons (host-to-network-short) and htonl (host-to-network-long) functions convert 16-bit and 32-bit values respectively from machine ( host ) to network order; the ntohs and ntohl functions convert from network to host order. These functions may be a no-op on a big-endian system. While the high-level network protocols usually consider the byte (mostly meant as octet ) as their atomic unit, the lowest network protocols may deal with ordering of bits within a byte. Bit endianness Bit numbering is a concept similar to endianness, but on a level of bits, not bytes. Bit endianness or bit-level endianness refers to the transmission order of bits over a serial medium. The bit-level analogue of little-endian (least significant bit goes first) is used in RS-232 , HDLC , Ethernet , and USB . Some protocols use the opposite ordering (e.g. Teletext , I\u00b2C , SMBus , PMBus , and SONET and SDH [ 27] ). Usually, there exists a consistent view to the bits irrespective of their order in the byte, such that the latter becomes relevant only on a very low level. One exception is caused by the feature of some cyclic redundancy checks to detect all burst errors up to a known length, which would be spoiled if the bit order is different from the byte order on serial transmission. Apart from serialization, the terms bit endianness and bit-level endianness are seldom used, as computer architectures where each individual bit has a unique address are rare. Individual bits or bit fields are accessed via their numerical value or, in high-level programming languages, assigned names, the effects of which, however, may be machine dependent or lack software portability . The natural numbering is that the arithmetic left shift 1 << *n* yields a mask for the bit of position n , a rule which exhibits the machine's (byte) endianness at least if *n* >= 8 , e.g. if used for indexing a sufficiently large bit array. Other numberings do occur in various documentations.","title":"Introduction"},{"location":"CPU-memory-access/Endianess/#endianness","text":"","title":"Endianness"},{"location":"CPU-memory-access/Endianess/#endianness#in#software#engineering","text":"\u672c\u8282\u603b\u7ed3\u5728software engineering\u4e2d\u7684\u4e00\u4e9b\u5173\u4e8eendian\u7684\u5185\u5bb9\uff0c\u5b83\u80fd\u591f\u542f\u793a\u6211\u4eec\uff0c\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u8003\u8651endian\u3002","title":"Endianness in software engineering"},{"location":"CPU-memory-access/Endianess/#c-family#language#object#representation","text":"\u4eceC++\u548cC\u6807\u51c6\u6765\u770b\uff0c\u201cendianness\u201d\u51b3\u5b9a\u4e86object\u7684value\u7684memory representation\uff0c\u5173\u4e8ememory representation\uff0c\u53c2\u89c1\uff1a \u5de5\u7a0bcomputer-arithmetic\u7684 Bitwise-operation\\Binary-representation \u7ae0\u8282\u7684\u5185\u5bb9\u3002 \u5173\u4e8eobject\u3001value\u3001\u548cmemory representation\uff0c\u5728cppreference Object#Object representation and value representation \u4e2d\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bf4\u660e\uff0c\u5176\u4e2d\u7684object representation\u5c31\u662f\u524d\u9762\u6240\u8bf4\u7684memory representation\u3002","title":"C-family language object representation"},{"location":"CPU-memory-access/Endianess/#sqlite#file#format","text":"\u5728 About SQLite \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0 The database file format is cross-platform - you can freely copy a database between 32-bit and 64-bit systems or between big-endian and little-endian architectures. These features make SQLite a popular choice as an Application File Format .","title":"Sqlite file format"},{"location":"CPU-memory-access/Endianess/#todo#c#network#library","text":"\u5728C network library\u4e2d\u63d0\u4f9b\u4e86endian\u8f6c\u6362\u51fd\u6570\u3002","title":"TODO C network library"},{"location":"CPU-memory-access/Endianess/#wikipedia#endianness","text":"In computing , endianness refers to the order of bytes (or sometimes bits ) within a binary representation of a number. It can also be used more generally to refer to the internal ordering of any representation, such as the digits in a numeral system or the sections of a date . NOTE: \u6211\u4eec\u4e3b\u8981\u5173\u6ce8\u7684\u662fbyte ordering In its most common usage, endianness indicates the ordering of bytes within a multi-byte number. A little-endian ordering places the least significant byte first and the most significant byte last, while a big-endian ordering does the opposite. For example, consider the unsigned hexadecimal number 0x1234 , which requires at least two bytes to represent. In a little-endian ordering, the bytes would be arranged [ 0x34, 0x12 ] , while in a big-endian ordering they would be [ 0x12, 0x34 ] . NOTE : 0x1234 \u8868\u7684\u662f\u4e00\u4e2a\u6570\uff0c\u5373\u6211\u4eec\u524d\u9762\u6240\u8ff0\u7684value\uff0c\u6211\u4eec\u7684\u8ba4\u8bfb\u987a\u5e8f\u662f\u4ece\u53f3\u5f80\u5de6\uff0c\u5373 0X34 \u662f\u6700\u4f4e\u6709\u6548\u4f4d\uff0c\u800c 0X12 \u662f\u6700\u9ad8\u6709\u6548\u4f4d\uff1b [ 0x34, 0x12 ] \u8868\u793a\u7684\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u987a\u5e8f\u662f\u4ece\u5de6\u5f80\u53f3\uff0c\u5373\u5b83\u7684\u7b2c0\u4e2a\u5143\u7d20\u662f 0X34 \uff0c\u7b2c1\u4e2a\u5143\u7d20\u662f 0x12 \uff1b\u663e\u7136\uff0c numeric literals \u662fbig-endian\uff0c\u6211\u4eec\u4f1a\u628a\u6700\u9ad8\u6709\u6548\u4f4d\u653e\u5230\u4f4e\u5730\u5740 Historically, various methods of endianness have been used in computing, including exotic forms such as middle-endianness. Today, however, little-endianness is the dominant ordering for processor architectures ( x86 , most ARM implementations) and their associated memory . Conversely, big-endianness is the dominant ordering in networking protocols ( IP , TCP , UDP ). File formats can use either ordering; in fact, some formats use a mixture of both. NOTE: \u4e24\u8005\u5404\u6709\u4f18\u52bf\uff0c\u5728\u540e\u9762\u4f1a\u5bf9\u5b83\u4eec\u8fdb\u884c\u8be6\u7ec6\u7684\u5206\u6790\u8bf4\u660e\uff1b\u8fd9\u5c31\u9020\u6210\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u9886\u57df\u7684application\u3002 In left-to-right scripts , numbers are written with their digits in big-endian order. Similarly, programming languages use big-endian digit ordering for numeric literals as well as big-endian language (\u201cleft\u201d and \u201cright\u201d) for bit-shift operations, regardless of the endianness of the target architecture. This can lead to confusion when interacting with little-endian numbers. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981\uff0c\u5b83\u89e3\u7b54\u4e86\u6211\u5173\u4e8ebitwise-operation\u548cendian\u4e4b\u95f4\u5173\u7cfb\u7684\u7591\u60d1\uff0c\u5b83\u603b\u7ed3\u5f97\u975e\u5e38\u597d\u3002\u5173\u4e8ebitwise-operation\uff0c\u53c2\u89c1\u5de5\u7a0bcomputer-arithmetic\u7684 Bitwise-operation\\Bitwise-operation.md \u3002 \u5bf9\u4e8eprogrammer\u800c\u8a00\u7684\u8bdd\uff0c\u5e94\u8be5\u8fd9\u6837\u601d\u8003\uff1a\u6211\u4eec\u5728\u8fdb\u884cprogram\u7684\u65f6\u5019\uff0c\u6211\u4eec\u662f\u4f7f\u7528\u7684value\uff0c\u6211\u4eec\u662f\u4e0d\u9700\u8981\u5173\u6ce8\u8fd9\u4e2avalue\u7684memory representation\u7684\uff0c\u4e5f\u5c31\u662f\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u662f\u4e0d\u9700\u8981\u5173\u6ce8endian\uff0c\u6b63\u5982\u524d\u9762\u6240\u8a00\u201c\u201cendianness\u201d\u51b3\u5b9a\u4e86value\u7684memory representation\u3002\u201d\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5bf9\u4e8eprogrammer\u800c\u8a00\u662f\u900f\u660e\u7684\uff0c\u5b83\u5f80\u5f80\u662f\u7531compiler\u6765\u5b9e\u73b0\u7684\u3002","title":"wikipedia Endianness"},{"location":"CPU-memory-access/Endianess/#illustration","text":"The following two descriptive illustrations assume a normal reading and writing convention of left to right , where the left-most digit or character therefore corresponds to data being sent or received first, or being in the lowest address in memory, and the right-most digit or character corresponds to the data being sent or received last, or being in the highest address in memory. Big-endianness may be demonstrated by writing a decimal number, say one hundred twenty-three, on paper in the usual positional notation understood by a numerate reader: 123 . The digits are written starting from the left and to the right, with the most significant digit, 1 , written first. This is analogous to the lowest address of memory being used first. This is an example of a big-endian convention taken from daily life. The little-endian way of writing the same number, one hundred twenty-three, would place the hundreds-digit 1 in the right-most position: 321 . A person following conventional big-endian place-value order, who is not aware of this special ordering, would read a different number: three hundred and twenty one. Endianness in computing is similar, but it usually applies to the ordering of bytes, rather than of digits. The illustrations to the right, where a is a memory address, show big-endian and little-endian storage in memory.","title":"Illustration"},{"location":"CPU-memory-access/Endianess/#hardware","text":"Computer memory consists of a sequence of storage cells. Each cell is identified in hardware and software by its memory address . If the total number of storage cells in memory is n , then addresses are enumerated from 0 to n-1 . Computer programs often use data structures of fields that may consist of more data than is stored in one memory cell. For the purpose of this article where its use as an operand of an instruction is relevant, a field consists of a consecutive sequence of bytes and represents a simple data value. In addition to that, it has to be of numeric type in some positional number system (mostly base-10 or base-2 \u2013 or base-256 in case of 8-bit bytes).[ 5] In such a number system the \"value\" of a digit is determined not only by its value as a single digit, but also by the position it holds in the complete number, its \"significance\". These positions can be mapped to memory mainly in two ways:[ 6] increasing numeric significance with increasing memory addresses (or increasing time), known as little-endian , and decreasing numeric significance with increasing memory addresses (or increasing time), known as big-endian [ 7]","title":"Hardware"},{"location":"CPU-memory-access/Endianess/#current#architectures","text":"The Intel x86 and also AMD64 / x86-64 series of processors use the little-endian format. Recently designed instruction set architectures typically follow this convention, either allowing only little-endian mode (e.g. RISC-V , Nios II , Andes Technology NDS32, or Qualcomm Hexagon ), or running mostly little-endian software on a bi-endian architecture (e.g. ARM Aarch64 , C-Sky ). Some big-endian architectures that remain popular include mostly older examples like the IBM z/Architecture , Freescale ColdFire (which is Motorola 68000 series -based) and Atmel AVR32 , but also the more recent OpenRISC . The IBM AIX and Oracle Solaris operating systems on bi-endian Power ISA and SPARC run in big-endian mode, while Linux on Power has moved to little-endian mode for new distributions.","title":"Current architectures"},{"location":"CPU-memory-access/Endianess/#floating#point","text":"Although the ubiquitous x86 processors of today use little-endian storage for all types of data (integer, floating point, BCD ), there are a number of hardware architectures where floating-point numbers are represented in big-endian form while integers are represented in little-endian form.[ 15] There are ARM processors that have half little-endian, half big-endian floating-point representation for double-precision numbers: both 32-bit words are stored in little-endian like integer registers, but the most significant one first. Because there have been many floating-point formats with no \" network \" standard representation for them, the XDR standard uses big-endian IEEE 754 as its representation. It may therefore appear strange that the widespread IEEE 754 floating-point standard does not specify endianness.[ 16] Theoretically, this means that even standard IEEE floating-point data written by one machine might not be readable by another. However, on modern standard computers (i.e., implementing IEEE 754), one may in practice safely assume that the endianness is the same for floating-point numbers as for integers, making the conversion straightforward regardless of data type. (Small embedded systems using special floating-point formats may be another matter however.)","title":"Floating point"},{"location":"CPU-memory-access/Endianess/#optimization","text":"The little-endian system has the property that the same value can be read from memory at different lengths without using different addresses (even when alignment restrictions are imposed). For example, a 32-bit memory location with content 4A 00 00 00 can be read at the same address as either 8-bit (value = 4A ), 16-bit ( 004A ), 24-bit ( 00004A ), or 32-bit ( 0000004A ), all of which retain the same numeric value. Although this little-endian property is rarely used directly by high-level programmers, it is often employed by code optimizers as well as by assembly language programmers. In more concrete terms, such optimizations are the equivalent of the following C code returning true on most little-endian systems: NOTE: \u8981\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8868\u8fbe\u7684\u601d\u60f3\uff0c\u9700\u8981\u641e\u6e05\u695a\uff1avalue\u3001memory representation\u3001endian\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5728\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C++\\Language-reference\\Basic-concept\\Data-model\\Object\\Object.md \u4e2d\uff0c\u6709\u8fc7\u8fd9\u65b9\u9762\u7684\u8ba8\u8bba\uff1aType determines the interpretion of memory representation, and further determine value\u3002\u4e0b\u9762\u7ed3\u5408endian\u8fdb\u884c\u66f4\u52a0\u6df1\u5165\u7684\u8bba\u8ff0\uff1a \u4e0a\u8ff0\u201ca 32-bit memory location with content 4A 00 00 00 \u201d\u4e2d\u7684 4A 00 00 00 \u662fmemory representation\uff1a \u7b2c\u4e00\u5b57\u8282\uff1a 4A \u7b2c\u4e8c\u5b57\u8282\uff1a 00 \u7b2c\u4e09\u5b57\u8282\uff1a 00 \u7b2c\u56db\u5b57\u8282\uff1a 00 \u4e0d\u540c\u7684endian\u4e0b\uff0c\u5b83\u8868\u793a\u7684value\u662f\u4e0d\u540c\u7684\uff1a \u6700\u9ad8/\u6700\u4f4e\u6709\u6548\u4f4d memory representation in bit value big endian \u6700\u9ad8: 4A \u6700\u4f4e: 00 0100 1010 0000 0000 0000 0000 0000 0000 2^{30} + 2^{27} + 2^{25} 2^{30} + 2^{27} + 2^{25} little endian \u6700\u9ad8: 00 \u6700\u4f4e: 4A 0100 1010 0000 0000 0000 0000 0000 0000 2^6 + 2^3 + 2^1 2^6 + 2^3 + 2^1 \u901a\u8fc7\u4e0a\u8ff0\u89e3\u91ca\uff0c\u5c31\u80fd\u591f\u7406\u89e3\u539f\u6587\u4e2d\"The little-endian system has the property that the same value can be read from memory at different lengths without using different addresses (even when alignment restrictions are imposed). \"\u7684\u542b\u4e49\u4e86\uff0c\u4e0b\u9762\u662f\u6211\u7684\u76f4\u89c2\u7684\u7406\u89e3\uff1a \u5bf9\u4e8elittle endian\uff0c\u7ed9\u5b9amemory representation\uff0cCPU\u5bf9memory representation\u7684interpretion\u662f\u53ef\u4ee5\u6e10\u8fdb\u5f0f\u5730\u8ba1\u7b97\u7684\uff1a\u5373\u4ece\u4f4e\u5730\u5740\u5f00\u59cb\u9010\u6b65\u8ba1\u7b97\uff0c\u56e0\u4e3a**\u6700\u4f4e\u6709\u6548\u4f4d**\u4f4d\u4e8e**\u4f4e\u5730\u5740**\uff0c\u5bf9\u4e8e\u4f4e\u5730\u5740\u7684\u503c\u7684\u8ba1\u7b97\u662f\u4e0d\u53d7\u540e\u9762\u7684\u9ad8\u5730\u5740\u7684\u503c\u7684\u5f71\u54cd\u3002 \u5bf9\u4e8ebig endain\uff0c\u4e0elittle endian\u662f\u6b63\u597d\u76f8\u53cd\u7684\u3002 \u4e0b\u9762\u7684\u4ee3\u7801\uff0c\u6781\u597d\u7684\u5c55\u793a\u4e86little endian\u7684\u8fd9\u79cdattribute\uff1a #include \"stdint.h\" #include \"stdio.h\" int main ( void ) { union u_t { uint8_t u8 ; uint16_t u16 ; uint32_t u32 ; uint64_t u64 ; } u = { . u64 = 0x4A }; puts ( u . u8 == u . u16 && u . u8 == u . u32 && u . u8 == u . u64 ? \"true\" : \"false\" ); } // gcc test.c NOTE: 0x4A \u4f4d\u4e8e\u4f4e\u5730\u5740\uff0c\u5b83\u53ea\u9700\u8981\u4e00\u4e2a\u5b57\u8282\u5373\u53ef\u3002 While not allowed by C++, such Type punning code is allowed as \"implementation-defined\" by the C11 standard[ 17] and commonly used[ 18] in code interacting with hardware.[ 19] On the other hand, in some situations it may be useful to obtain an approximation of a multi-byte or multi-word value by reading only its most significant portion instead of the complete representation; a big-endian processor may read such an approximation using the same base-address that would be used for the full value. NOTE: \u56e0\u4e3a\u5bf9\u4e8ebig-endian\u800c\u8a00\uff0c\u5b83\u7684most significant bit\u4f4d\u4e8e\u4f4e\u5730\u5740 Optimizations of this kind are not portable across systems of different endianness.","title":"Optimization"},{"location":"CPU-memory-access/Endianess/#calculation#order","text":"Little-endian representation simplifies hardware in processors that add multi-byte integral values a byte at a time, such as small-scale byte-addressable processors and microcontrollers . As carry propagation\uff08\u8fdb\u4f4d\uff09 must start at the least significant bit (and thus byte), multi-byte addition can then be carried out with a monotonically-incrementing address sequence, a simple operation already present in hardware. On a big-endian processor, its addressing unit has to be told how big the addition is going to be so that it can hop forward to the least significant byte, then count back down towards the most significant byte ( MSB ). On the other hand, arithmetic division is done starting from the MSB , so it is more natural for big-endian processors. However, high-performance processors usually fetch typical multi-byte operands from memory in the same amount of time they would have fetched a single byte, so the complexity of the hardware is not affected by the byte ordering .","title":"Calculation order"},{"location":"CPU-memory-access/Endianess/#mapping#multi-byte#binary#values#to#memory","text":"We can assume that as we write text left to right , we are increasing the 'address' on paper, as a processor would write bytes with increasing memory addresses \u2014 as in the adjacent table. On paper, the hex value 0a0b0c0d (written 168496141 in usual decimal notation) is big-endian style since we write the most significant digit first and the rest follow in ***de*creasing** significance. Mapping this number as a binary value to a sequence of 4 bytes in memory in big-endian style also writes the bytes from left to right in ***de*creasing** significance: 0Ah at +0, 0Bh at +1, 0Ch at +2, 0Dh at +3. On a little-endian system, the bytes are written from left to right in ***in**creasing* significance, starting with the one's byte: 0Dh at +0, 0Ch at +1, 0Bh at +2, 0Ah at +3. Writing a 32-bit binary value to a memory location on a little-endian system and outputting the memory location (with growing addresses from left to right) shows that the order is reversed (byte-swapped) compared to usual big-endian notation. This is the way a hexdump is displayed: because the dumping program is unable to know what kind of data it is dumping, the only orientation it can observe is monotonically increasing addresses. The human reader, however, who knows that they are reading a hexdump of a little-endian system and who knows what kind of data they are reading, reads the byte sequence 0Dh , 0Ch , 0Bh , 0Ah as the 32-bit binary value 168496141, or 0x0a0b0c0d in hexadecimal notation. (Of course, this is not the same as the","title":"Mapping multi-byte binary values to memory"},{"location":"CPU-memory-access/Endianess/#examples","text":"","title":"Examples"},{"location":"CPU-memory-access/Endianess/#a#c#programming#example","text":"Consider the following C language program: #include <stdio.h> int main ( void ) { union { unsigned int word ; // a 32-bit integer unsigned char bytes [ 4 ]; } u ; u . word = 0x0A0B0C0D ; for ( int i = 0 ; i < 4 ; i ++ ) { printf ( \"byte[%d] = 0x%x \\n \" , i , ( int ) u . bytes [ i ]); } } On a little-endian computer, this program would output: byte [ 0 ] = 0xd byte [ 1 ] = 0xc byte [ 2 ] = 0xb byte [ 3 ] = 0xa On a big-endian computer, this program would output: byte [ 0 ] = 0xa byte [ 1 ] = 0xb byte [ 2 ] = 0xc byte [ 3 ] = 0xd","title":"A C programming example"},{"location":"CPU-memory-access/Endianess/#files#and#byte#swap","text":"Endianness is a problem when a binary file created on a computer is read on another computer with different endianness. Some CPU instruction sets provide native support for endian byte swapping, such as bswap [ 21] ( x86 - 486 and later), and rev [ 22] ( ARMv6 and later). Some compilers have built-in facilities to deal with data written in other formats. For example, the Intel Fortran compiler supports the non-standard CONVERT specifier, so a file can be opened as OPEN(unit,CONVERT='BIG_ENDIAN',...) or OPEN(unit,CONVERT='LITTLE_ENDIAN',...) Some compilers have options to generate code that globally enables the conversion for all file IO operations. This allows programmers to reuse code on a system with the opposite endianness without having to modify the code itself. If the compiler does not support such conversion, the programmer needs to swap the bytes via ad hoc \uff08\u7279\u6b8a\u7684\uff0c\u4e13\u95e8\u7684\uff09 code. Unicode text can optionally start with a byte order mark (BOM) to signal the endianness of the file or stream. Its code point is U+FEFF . In UTF-32 for example, a big-endian file should start with 00 00 FE FF ; a little-endian should start with FF FE 00 00 . Application binary data formats, such as for example MATLAB .mat files, or the .BIL data format, used in topography, are usually endianness-independent. This is achieved by: storing the data always in one fixed endianness, or carrying with the data a switch to indicate which endianness the data was written with. When reading the file, the application converts the endianness, invisibly from the user. An example of the first case is the binary XLS file format that is portable between Windows and Mac systems and always little-endian, leaving the Mac application to swap the bytes on load and save when running on a big-endian Motorola 68K or PowerPC processor.[ 23] TIFF image files are an example of the second strategy, whose header instructs the application about endianness of their internal binary integers. If a file starts with the signature \" MM \" it means that integers are represented as big-endian, while \" II \" means little-endian. Those signatures need a single 16-bit word each, and they are palindromes (that is, they read the same forwards and backwards), so they are endianness independent. \" I \" stands for Intel and \" M \" stands for Motorola , the respective CPU providers of the IBM PC compatibles (Intel) and Apple Macintosh platforms (Motorola) in the 1980s. Intel CPUs are little-endian, while Motorola 680x0 CPUs are big-endian. This explicit signature allows a TIFF reader program to swap bytes if necessary when a given file was generated by a TIFF writer program running on a computer with a different endianness. Since the required byte swap depends on the size of the numbers stored in the file (two 2-byte integers require a different swap than one 4-byte integer), the file format must be known to perform endianness conversion. /* C function to change endianness for byte swap in an unsigned 32-bit integer */ uint32_t ChangeEndianness ( uint32_t value ) { uint32_t result = 0 ; result |= ( value & 0x000000FF ) << 24 ; result |= ( value & 0x0000FF00 ) << 8 ; result |= ( value & 0x00FF0000 ) >> 8 ; result |= ( value & 0xFF000000 ) >> 24 ; return result ; } NOTE :\u5173\u4e8e\u4e0a\u9762\u8fd9\u6bb5\u4ee3\u7801\u7684\u7ec6\u81f4\u5206\u6790\uff0c\u53c2\u89c1 Endianness conversion in C","title":"Files and byte swap"},{"location":"CPU-memory-access/Endianess/#file#systems","text":"","title":"File systems"},{"location":"CPU-memory-access/Endianess/#networking","text":"Many IETF RFCs use the term network order , meaning the order of transmission for bits and bytes over the wire in network protocols . Among others, the historic RFC 1700 (also known as Internet standard STD 2) has defined the network order for protocols in the Internet protocol suite to be big-endian , hence the use of the term \"network byte order\" for big-endian byte order.[ 25] However, not all protocols use big-endian byte order as the network order. The Server Message Block (SMB) protocol uses little-endian byte order. In CANopen , multi-byte parameters are always sent least significant byte first (little-endian). The same is true for Ethernet Powerlink .[ 26] The Berkeley sockets API defines a set of functions to convert 16-bit and 32-bit integers to and from network byte order: the htons (host-to-network-short) and htonl (host-to-network-long) functions convert 16-bit and 32-bit values respectively from machine ( host ) to network order; the ntohs and ntohl functions convert from network to host order. These functions may be a no-op on a big-endian system. While the high-level network protocols usually consider the byte (mostly meant as octet ) as their atomic unit, the lowest network protocols may deal with ordering of bits within a byte.","title":"Networking"},{"location":"CPU-memory-access/Endianess/#bit#endianness","text":"Bit numbering is a concept similar to endianness, but on a level of bits, not bytes. Bit endianness or bit-level endianness refers to the transmission order of bits over a serial medium. The bit-level analogue of little-endian (least significant bit goes first) is used in RS-232 , HDLC , Ethernet , and USB . Some protocols use the opposite ordering (e.g. Teletext , I\u00b2C , SMBus , PMBus , and SONET and SDH [ 27] ). Usually, there exists a consistent view to the bits irrespective of their order in the byte, such that the latter becomes relevant only on a very low level. One exception is caused by the feature of some cyclic redundancy checks to detect all burst errors up to a known length, which would be spoiled if the bit order is different from the byte order on serial transmission. Apart from serialization, the terms bit endianness and bit-level endianness are seldom used, as computer architectures where each individual bit has a unique address are rare. Individual bits or bit fields are accessed via their numerical value or, in high-level programming languages, assigned names, the effects of which, however, may be machine dependent or lack software portability . The natural numbering is that the arithmetic left shift 1 << *n* yields a mask for the bit of position n , a rule which exhibits the machine's (byte) endianness at least if *n* >= 8 , e.g. if used for indexing a sufficiently large bit array. Other numberings do occur in various documentations.","title":"Bit endianness"},{"location":"CPU-memory-access/Endianess/Endianness-operation/","text":"Endianness operation \u9700\u8981\u8003\u8651\u5728\u54ea\u4e9b\u60c5\u51b5\uff0cprogrammer\u662f\u9700\u8981\u8003\u8651endian\u7684\uff0c\u54ea\u4e9b\u60c5\u51b5\u662f\u4e0d\u9700\u8981\u8003\u8651\u7684\u3002 \u4e0b\u9762\u7f57\u5217\u4e86\u9700\u8981\u8003\u8651\u7684\u5178\u578b\u573a\u666f\uff1a \u8ba1\u7b97hash\u503c\uff08\u53c2\u89c1 quarkslab Unaligned accesses in C/C++: what, why and solutions to do it properly \uff09 \u7f51\u7edc\u4f20\u8f93\uff1a\u7f51\u7edc\u6570\u636e\u91c7\u7528\u7684\u662fbig endian\uff0c\u53d1\u9001\u65f6\uff0c\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3abig endian\uff0c\u63a5\u6536\u65f6\uff0c\u9700\u8981\u5c06\u5176\u4ecebig endian\u8f6c\u6362\u4e3a\u4e3b\u673a\u7684endian\u3002 \u4e0b\u9762\u7f57\u5217\u4e86\u4e0d\u9700\u8981\u8003\u8651\u7684\u5178\u578b\u573a\u666f\uff1a literal\u9ed8\u8ba4\u662fbig endian\uff0c\u6240\u4ee5literal\u3001bit-shift operation\u662f\u4e0d\u9700\u8981\u8003\u8651endian\u7684\uff08\u53c2\u89c1wikipedia Endianness \uff09 Conversion stackexchange Endianness conversion in C I have written a simple C header for converting the endianness of short integers and long integers. It uses the GCC macro __BYTE_ORDER__ to check the system's byte order and define the macros based on that. The header creates the macros LITTLE_ENDIAN_SHORT(n) , LITTLE_ENDIAN_LONG(n) , BIG_ENDIAN_SHORT(n) , BIG_ENDIAN_LONG(n) which convert the value n from host endianness to the endianness specified. Here is the source for endian.h : #ifndef ENDIAN_H #define ENDIAN_H #define REVERSE_SHORT(n) ((unsigned short) (((n & 0xFF) << 8) | \\ ((n & 0xFF00) >> 8))) #define REVERSE_LONG(n) ((unsigned long) (((n & 0xFF) << 24) | \\ ((n & 0xFF00) << 8) | \\ ((n & 0xFF0000) >> 8) | \\ ((n & 0xFF000000) >> 24))) #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ # define LITTLE_ENDIAN_SHORT(n) (n) # define LITTLE_ENDIAN_LONG(n) (n) # define BIG_ENDIAN_SHORT(n) REVERSE_SHORT(n) # define BIG_ENDIAN_LONG(n) REVERSE_LONG(n) #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ # define LITTLE_ENDIAN_SHORT(n) REVERSE_SHORT(n) # define LITTLE_ENDIAN_LONG(n) REVERSE_LONG(n) # define BIG_ENDIAN_SHORT(n) (n) # define BIG_ENDIAN_LONG(n) (n) #else # error unsupported endianness #endif #endif Is this a good way to implement the macros or is there a better way? COMMENTS 1 You may want to take a look at codereview.stackexchange.com/a/149751/4203 \u2013 forsvarir Dec 28 '16 at 14:29 3 A better way to implement the macros would be to replace them with inline functions. :-) \u2013 Cody Gray Dec 28 '16 at 18:24 It should be 0xFF000000U (and for good measure, you can add U to the other bitmasks as well). While most compilers do the right thing, you are technically overflowing a signed integer here. \u2013 Simon Richter Dec 29 '16 at 3:16 1 Is there some reason why you want to roll your own endian-conversion code rather than using the standard htonl() / ntohl() and htons() / ntohs() functions? The latter are much harder to mess up and easier for other programmers to read/understand/trust... \u2013 Jeremy Friesner Dec 29 '16 at 5:29 A First off, Jean-Fran\u00e7ois is absolutely right : you cannot assume any particular bit widths for the built-in types, short , int , long , etc. Use the types defined in stdint.h that have explicit bit widths to ensure that the code is correct and portable. Otherwise, your code looks pretty good, and this is a reasonable implementation. But\u2026 Is this a good way to implement the macros or is there a better way? There is indeed a better way: to use inline functions rather than macros. :-) After an optimizing compiler finishes, the resulting code will be equivalent, but the advantages of functions are numerous: type safety, the ability to use expressions as arguments, no insidious parenthesization bugs, etc. You will still need some preprocessor magic to avoid generating code when it is not necessary, but this is still much cleaner than implementing the whole header as a mess of macros. Define a couple of inline helper functions that do the conversion, like so: inline uint16_t Reverse16 ( uint16_t value ) { return ((( value & 000 xFF ) << 8 ) | (( value & 0xFF00 ) >> 8 )); } inline uint32_t Reverse32 ( uint32_t value ) { return ((( value & 0x000000FF ) << 24 ) | (( value & 0x0000FF00 ) << 8 ) | (( value & 0x00FF0000 ) >> 8 ) | (( value & 0xFF000000 ) >> 24 )); } Although it is not strictly necessary, I have kept your explicit masking because I think it increases the clarity of the code, both for human readers and for the compiler. Others may feel that simpler is better. For example, Reverse16 could simply be implemented as return ((value << 8) | (value >> 8)); . However, I've chosen to format your code slightly differently for nice vertical alignment . I think this makes it easier to read, and easier to audit for correctness at a glance. On most compilers, there are byte-swapping intrinsics that you could have used to implement the body of these functions. For example, GNU compilers (including GCC and Clang) have __bswap_32 and __bswap_16 macros in the <byteswap.h> header, and Microsoft's compiler offers the _byteswap_ushort and _byteswap_ulong intrinsics in the <intrin.h> header. While intrinsics can often result in better code than writing out the C code long-form, all compilers I tested here are exceptionally smart: GCC, Clang, and ICC all recognize the bit-twiddling code used above and compile it to identical object code as if we had used the intrinsic\u2014a single BSWAP instruction on the x86 architecture! Microsoft's compiler makes this optimization for the 32-bit version, but not for the 16-bit version. However, its output either way is still perfectly reasonable, and if you're seeking portable code, there is no compelling interest in using the intrinsics. For once, you don't need them to get optimal code! Now that we have those helper functions, we need to define some more inline functions that work like your macros, except that the conditional logic will be encapsulated within the body of the functions, rather than surrounding the macro definitions. Naturally, since these are functions, not macros, we'll use a different (title-case) naming convention. We don't need SCREAMING_CASE because they're not scary anymore: inline uint16_t LittleEndian16 ( uint16_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return value ; #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return Reverse16 ( value ); #else # error unsupported endianness #endif } inline uint16_t BigEndian16 ( uint16_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return Reverse16 ( value ); #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return value ; #else # error unsupported endianness #endif } inline uint32_t LittleEndian32 ( uint32_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return value ; #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return Reverse32 ( value ); #else # error unsupported endianness #endif } inline uint32_t BigEndian32 ( uint32_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return Reverse32 ( value ); #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return value ; #else # error unsupported endianness #endif } The complete code is slightly longer than the macro-based version, but the safety and other benefits of functions more than justify this expanded length. The compiler doesn't care, and it only takes a few minutes longer to write. You will more than make up for it later in the time you don't have to spend debugging macros used in expressions. You'll use them in basically the same way as the macros. Instead of LITTLE_ENDIAN_SHORT(value) , you'd call LittleEndian16(value) . Note that I've also used the explicit bit-width in the function's names, instead of the ambiguous short and long type names. stackexchange Implementation of C Standard Library Function ntohl() stackoverflow Convert Little Endian to Big Endian compile-time endianness swap bit arithmetic: compile-time endianness swap based on http://stackoverflow.com/a/36937049 https://en.cppreference.com/w/cpp/language/fold Query Compile time quarkslab Unaligned accesses in C/C++: what, why and solutions to do it properly \u4e2d\u7684\u4f8b\u5b50\uff1a #include <stdint.h> #include <stdlib.h> static uint64_t load64_le ( uint8_t const * V ) { #if !defined(__LITTLE_ENDIAN__) #error This code only works with little endian systems #endif uint64_t Ret = * (( uint64_t const * ) V ); return Ret ; } Dynamic \u8fd9\u4e2a\u4f8b\u5b50\u662f\u6e90\u81eacreference Objects and alignment#Strict aliasing #include <cstdio> int main () { int i = 7 ; //\u6700\u4f4e\u6709\u6548\u4f4d\u662f0x7 char * pc = ( char * ) ( & i ); if ( pc [ 0 ] == '\\x7' ) // aliasing through char is ok puts ( \"This system is little-endian\" ); else puts ( \"This system is big-endian\" ); } i \u7684\u6700\u4f4e\u6709\u6548\u4f4d\u662f 0x7 \uff0c\u6240\u4ee5\u5982\u679c\u4f4e\u5730\u5740 pc[0] \u7684\u503c\u7b49\u4e8e '\\x7' \uff0c\u5219\u662flittle-endian\u3002","title":"Endianness-operation"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#endianness#operation","text":"\u9700\u8981\u8003\u8651\u5728\u54ea\u4e9b\u60c5\u51b5\uff0cprogrammer\u662f\u9700\u8981\u8003\u8651endian\u7684\uff0c\u54ea\u4e9b\u60c5\u51b5\u662f\u4e0d\u9700\u8981\u8003\u8651\u7684\u3002 \u4e0b\u9762\u7f57\u5217\u4e86\u9700\u8981\u8003\u8651\u7684\u5178\u578b\u573a\u666f\uff1a \u8ba1\u7b97hash\u503c\uff08\u53c2\u89c1 quarkslab Unaligned accesses in C/C++: what, why and solutions to do it properly \uff09 \u7f51\u7edc\u4f20\u8f93\uff1a\u7f51\u7edc\u6570\u636e\u91c7\u7528\u7684\u662fbig endian\uff0c\u53d1\u9001\u65f6\uff0c\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3abig endian\uff0c\u63a5\u6536\u65f6\uff0c\u9700\u8981\u5c06\u5176\u4ecebig endian\u8f6c\u6362\u4e3a\u4e3b\u673a\u7684endian\u3002 \u4e0b\u9762\u7f57\u5217\u4e86\u4e0d\u9700\u8981\u8003\u8651\u7684\u5178\u578b\u573a\u666f\uff1a literal\u9ed8\u8ba4\u662fbig endian\uff0c\u6240\u4ee5literal\u3001bit-shift operation\u662f\u4e0d\u9700\u8981\u8003\u8651endian\u7684\uff08\u53c2\u89c1wikipedia Endianness \uff09","title":"Endianness operation"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#conversion","text":"","title":"Conversion"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#stackexchange#endianness#conversion#in#c","text":"I have written a simple C header for converting the endianness of short integers and long integers. It uses the GCC macro __BYTE_ORDER__ to check the system's byte order and define the macros based on that. The header creates the macros LITTLE_ENDIAN_SHORT(n) , LITTLE_ENDIAN_LONG(n) , BIG_ENDIAN_SHORT(n) , BIG_ENDIAN_LONG(n) which convert the value n from host endianness to the endianness specified. Here is the source for endian.h : #ifndef ENDIAN_H #define ENDIAN_H #define REVERSE_SHORT(n) ((unsigned short) (((n & 0xFF) << 8) | \\ ((n & 0xFF00) >> 8))) #define REVERSE_LONG(n) ((unsigned long) (((n & 0xFF) << 24) | \\ ((n & 0xFF00) << 8) | \\ ((n & 0xFF0000) >> 8) | \\ ((n & 0xFF000000) >> 24))) #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ # define LITTLE_ENDIAN_SHORT(n) (n) # define LITTLE_ENDIAN_LONG(n) (n) # define BIG_ENDIAN_SHORT(n) REVERSE_SHORT(n) # define BIG_ENDIAN_LONG(n) REVERSE_LONG(n) #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ # define LITTLE_ENDIAN_SHORT(n) REVERSE_SHORT(n) # define LITTLE_ENDIAN_LONG(n) REVERSE_LONG(n) # define BIG_ENDIAN_SHORT(n) (n) # define BIG_ENDIAN_LONG(n) (n) #else # error unsupported endianness #endif #endif Is this a good way to implement the macros or is there a better way? COMMENTS 1 You may want to take a look at codereview.stackexchange.com/a/149751/4203 \u2013 forsvarir Dec 28 '16 at 14:29 3 A better way to implement the macros would be to replace them with inline functions. :-) \u2013 Cody Gray Dec 28 '16 at 18:24 It should be 0xFF000000U (and for good measure, you can add U to the other bitmasks as well). While most compilers do the right thing, you are technically overflowing a signed integer here. \u2013 Simon Richter Dec 29 '16 at 3:16 1 Is there some reason why you want to roll your own endian-conversion code rather than using the standard htonl() / ntohl() and htons() / ntohs() functions? The latter are much harder to mess up and easier for other programmers to read/understand/trust... \u2013 Jeremy Friesner Dec 29 '16 at 5:29","title":"stackexchange Endianness conversion in C"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#a","text":"First off, Jean-Fran\u00e7ois is absolutely right : you cannot assume any particular bit widths for the built-in types, short , int , long , etc. Use the types defined in stdint.h that have explicit bit widths to ensure that the code is correct and portable. Otherwise, your code looks pretty good, and this is a reasonable implementation. But\u2026 Is this a good way to implement the macros or is there a better way? There is indeed a better way: to use inline functions rather than macros. :-) After an optimizing compiler finishes, the resulting code will be equivalent, but the advantages of functions are numerous: type safety, the ability to use expressions as arguments, no insidious parenthesization bugs, etc. You will still need some preprocessor magic to avoid generating code when it is not necessary, but this is still much cleaner than implementing the whole header as a mess of macros. Define a couple of inline helper functions that do the conversion, like so: inline uint16_t Reverse16 ( uint16_t value ) { return ((( value & 000 xFF ) << 8 ) | (( value & 0xFF00 ) >> 8 )); } inline uint32_t Reverse32 ( uint32_t value ) { return ((( value & 0x000000FF ) << 24 ) | (( value & 0x0000FF00 ) << 8 ) | (( value & 0x00FF0000 ) >> 8 ) | (( value & 0xFF000000 ) >> 24 )); } Although it is not strictly necessary, I have kept your explicit masking because I think it increases the clarity of the code, both for human readers and for the compiler. Others may feel that simpler is better. For example, Reverse16 could simply be implemented as return ((value << 8) | (value >> 8)); . However, I've chosen to format your code slightly differently for nice vertical alignment . I think this makes it easier to read, and easier to audit for correctness at a glance. On most compilers, there are byte-swapping intrinsics that you could have used to implement the body of these functions. For example, GNU compilers (including GCC and Clang) have __bswap_32 and __bswap_16 macros in the <byteswap.h> header, and Microsoft's compiler offers the _byteswap_ushort and _byteswap_ulong intrinsics in the <intrin.h> header. While intrinsics can often result in better code than writing out the C code long-form, all compilers I tested here are exceptionally smart: GCC, Clang, and ICC all recognize the bit-twiddling code used above and compile it to identical object code as if we had used the intrinsic\u2014a single BSWAP instruction on the x86 architecture! Microsoft's compiler makes this optimization for the 32-bit version, but not for the 16-bit version. However, its output either way is still perfectly reasonable, and if you're seeking portable code, there is no compelling interest in using the intrinsics. For once, you don't need them to get optimal code! Now that we have those helper functions, we need to define some more inline functions that work like your macros, except that the conditional logic will be encapsulated within the body of the functions, rather than surrounding the macro definitions. Naturally, since these are functions, not macros, we'll use a different (title-case) naming convention. We don't need SCREAMING_CASE because they're not scary anymore: inline uint16_t LittleEndian16 ( uint16_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return value ; #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return Reverse16 ( value ); #else # error unsupported endianness #endif } inline uint16_t BigEndian16 ( uint16_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return Reverse16 ( value ); #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return value ; #else # error unsupported endianness #endif } inline uint32_t LittleEndian32 ( uint32_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return value ; #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return Reverse32 ( value ); #else # error unsupported endianness #endif } inline uint32_t BigEndian32 ( uint32_t value ) { #if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return Reverse32 ( value ); #elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return value ; #else # error unsupported endianness #endif } The complete code is slightly longer than the macro-based version, but the safety and other benefits of functions more than justify this expanded length. The compiler doesn't care, and it only takes a few minutes longer to write. You will more than make up for it later in the time you don't have to spend debugging macros used in expressions. You'll use them in basically the same way as the macros. Instead of LITTLE_ENDIAN_SHORT(value) , you'd call LittleEndian16(value) . Note that I've also used the explicit bit-width in the function's names, instead of the ambiguous short and long type names.","title":"A"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#stackexchange#implementation#of#c#standard#library#function#ntohl","text":"","title":"stackexchange Implementation of C Standard Library Function ntohl()"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#stackoverflow#convert#little#endian#to#big#endian","text":"","title":"stackoverflow Convert Little Endian to Big Endian"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#compile-time#endianness#swap","text":"bit arithmetic: compile-time endianness swap based on http://stackoverflow.com/a/36937049 https://en.cppreference.com/w/cpp/language/fold","title":"compile-time endianness swap"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#query","text":"","title":"Query"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#compile#time","text":"quarkslab Unaligned accesses in C/C++: what, why and solutions to do it properly \u4e2d\u7684\u4f8b\u5b50\uff1a #include <stdint.h> #include <stdlib.h> static uint64_t load64_le ( uint8_t const * V ) { #if !defined(__LITTLE_ENDIAN__) #error This code only works with little endian systems #endif uint64_t Ret = * (( uint64_t const * ) V ); return Ret ; }","title":"Compile time"},{"location":"CPU-memory-access/Endianess/Endianness-operation/#dynamic","text":"\u8fd9\u4e2a\u4f8b\u5b50\u662f\u6e90\u81eacreference Objects and alignment#Strict aliasing #include <cstdio> int main () { int i = 7 ; //\u6700\u4f4e\u6709\u6548\u4f4d\u662f0x7 char * pc = ( char * ) ( & i ); if ( pc [ 0 ] == '\\x7' ) // aliasing through char is ok puts ( \"This system is little-endian\" ); else puts ( \"This system is big-endian\" ); } i \u7684\u6700\u4f4e\u6709\u6548\u4f4d\u662f 0x7 \uff0c\u6240\u4ee5\u5982\u679c\u4f4e\u5730\u5740 pc[0] \u7684\u503c\u7b49\u4e8e '\\x7' \uff0c\u5219\u662flittle-endian\u3002","title":"Dynamic"},{"location":"CPU-memory-access/Endianess/draft/Understanding%20Big%20and%20Little%20Endian%20Byte%20Order/","text":"Understanding Big and Little Endian Byte Order","title":"[Understanding Big and Little Endian Byte Order](https://betterexplained.com/articles/understanding-big-and-little-endian-byte-order/)"},{"location":"CPU-memory-access/Endianess/draft/Understanding%20Big%20and%20Little%20Endian%20Byte%20Order/#understanding#big#and#little#endian#byte#order","text":"","title":"Understanding Big and Little Endian Byte Order"},{"location":"CPU-memory-access/Endianess/draft/Unix-get-endianess/","text":"How to tell if a Linux system is big endian or little endian? Is there a system command, in Linux, that reports the endianness? Little and Big Endian Mystery What are these? Little and big endian are two ways of storing multibyte data-types ( int , float , etc). In little endian machines, last byte of binary representation of the multibyte data-type is stored first. On the other hand, in big endian machines, first byte of binary representation of the multibyte data-type is stored first. SUMMARY : \u53c2\u89c1 Endianness \uff0c\u5176\u4e2d\u7684\u89e3\u91ca\u6bd4\u8fd9\u91cc\u66f4\u597d\uff1aA little-endian ordering places the least significant byte first and the most significant byte last, while a big-endian ordering does the opposite. \u4e5f\u5c31\u662f\u5c06least significant byte\u7f6e\u4e8e\u4f4e\u5730\u5740\uff0c\u5c06most significant byte\u7f6e\u4e8e\u9ad8\u5730\u5740\uff1b Suppose integer is stored as 4 bytes (For those who are using DOS based compilers such as C++ 3.0 , integer is 2 bytes) then a variable x with value 0x01234567 will be stored as following. Memory representation of integer 0x01234567 inside Big and little endian machines How to see memory representation of multibyte data types on your machine? Here is a sample C code that shows the byte representation of int, float and pointer. #include <stdio.h> /* function to show bytes in memory, from location start to start+n*/ void show_mem_rep ( char * start , int n ) { int i ; for ( i = 0 ; i < n ; i ++ ) printf ( \" %.2x\" , start [ i ]); printf ( \" \\n \" ); } /*Main function to call above function for 0x01234567*/ int main () { int i = 0x01234567 ; // 67\u662fleast significant byte,01\u662fmost significant byte show_mem_rep (( char * ) & i , sizeof ( i )); return 0 ; } When above program is run on little endian machine, gives \u201c67 45 23 01\u201d as output , while if it is run on big endian machine, gives \u201c01 23 45 67\u201d as output. Is there a quick way to determine endianness of your machine? There are n no. of ways for determining endianness of your machine. Here is one quick way of doing the same. #include <stdio.h> int main () { unsigned int i = 1 ; char * c = ( char * ) & i ; if ( * c ) printf ( \"Little endian\" ); else printf ( \"Big endian\" ); getchar (); return 0 ; } #include <bits/stdc++.h> using namespace std ; int main () { unsigned int i = 1 ; char * c = ( char * ) & i ; if ( * c ) cout << \"Little endian\" ; else cout << \"Big endian\" ; return 0 ; } // This code is contributed by rathbhupendra Output: In the above program, a character pointer c is pointing to an integer i. Since size of character is 1 byte when the character pointer is de-referenced it will contain only first byte of integer. If machine is little endian then *c will be 1 (because last byte is stored first) and if machine is big endian then *c will be 0. Does endianness matter for programmers? Most of the times compiler takes care of endianness, however, endianness becomes an issue in following cases. It matters in network programming : Suppose you write integers to file on a little endian machine and you transfer this file to a big endian machine . Unless there is little endian to big endian transformation, big endian machine will read the file in reverse order. You can find such a practical example here. Standard byte order for networks is big endian , also known as network byte order . Before transferring data on network, data is first converted to network byte order ( big endian ). Sometimes it matters when you are using type casting , below program is an example. #include <stdio.h> int main () { unsigned char arr [ 2 ] = { 0x01 , 0x00 }; unsigned short int x = * ( unsigned short int * ) arr ; printf ( \"%d\" , x ); getchar (); return 0 ; } In the above program, a char array is typecasted to an unsigned short integer type. When I run above program on little endian machine, I get 1 as output, while if I run it on a big endian machine I get 256 . To make programs endianness independent, above programming style should be avoided. SUMMARY : unsigned short int x = *(unsigned short int *) arr; \u8bed\u53e5\u7684\u6548\u679c\u662f\u4f7f x \u7684\u5185\u5b58image\u4e3a 0X0100 \uff0c\u5bf9\u4e8e\u8fd9\u7bc7\u5185\u5b58\u7684\u89e3\u91ca\u662f\u673a\u5668\u76f8\u5173\u7684\uff0c\u5728big endian machine\u4e0a\uff0c\u5b83\u662f 256 \uff0c\u5728little endian machine\u4e0a\uff0c\u5b83\u662f 1 \u3002 SUMMARY : Writing endian-independent code in C [C program to check little vs. big endian duplicate] C Macro definition to determine big endian or little endian machine?","title":"[How to tell if a Linux system is big endian or little endian?](https://serverfault.com/questions/163487/how-to-tell-if-a-linux-system-is-big-endian-or-little-endian)"},{"location":"CPU-memory-access/Endianess/draft/Unix-get-endianess/#how#to#tell#if#a#linux#system#is#big#endian#or#little#endian","text":"","title":"How to tell if a Linux system is big endian or little endian?"},{"location":"CPU-memory-access/Endianess/draft/Unix-get-endianess/#is#there#a#system#command#in#linux#that#reports#the#endianness","text":"","title":"Is there a system command, in Linux, that reports the endianness?"},{"location":"CPU-memory-access/Endianess/draft/Unix-get-endianess/#little#and#big#endian#mystery","text":"What are these? Little and big endian are two ways of storing multibyte data-types ( int , float , etc). In little endian machines, last byte of binary representation of the multibyte data-type is stored first. On the other hand, in big endian machines, first byte of binary representation of the multibyte data-type is stored first. SUMMARY : \u53c2\u89c1 Endianness \uff0c\u5176\u4e2d\u7684\u89e3\u91ca\u6bd4\u8fd9\u91cc\u66f4\u597d\uff1aA little-endian ordering places the least significant byte first and the most significant byte last, while a big-endian ordering does the opposite. \u4e5f\u5c31\u662f\u5c06least significant byte\u7f6e\u4e8e\u4f4e\u5730\u5740\uff0c\u5c06most significant byte\u7f6e\u4e8e\u9ad8\u5730\u5740\uff1b Suppose integer is stored as 4 bytes (For those who are using DOS based compilers such as C++ 3.0 , integer is 2 bytes) then a variable x with value 0x01234567 will be stored as following. Memory representation of integer 0x01234567 inside Big and little endian machines How to see memory representation of multibyte data types on your machine? Here is a sample C code that shows the byte representation of int, float and pointer. #include <stdio.h> /* function to show bytes in memory, from location start to start+n*/ void show_mem_rep ( char * start , int n ) { int i ; for ( i = 0 ; i < n ; i ++ ) printf ( \" %.2x\" , start [ i ]); printf ( \" \\n \" ); } /*Main function to call above function for 0x01234567*/ int main () { int i = 0x01234567 ; // 67\u662fleast significant byte,01\u662fmost significant byte show_mem_rep (( char * ) & i , sizeof ( i )); return 0 ; } When above program is run on little endian machine, gives \u201c67 45 23 01\u201d as output , while if it is run on big endian machine, gives \u201c01 23 45 67\u201d as output. Is there a quick way to determine endianness of your machine? There are n no. of ways for determining endianness of your machine. Here is one quick way of doing the same. #include <stdio.h> int main () { unsigned int i = 1 ; char * c = ( char * ) & i ; if ( * c ) printf ( \"Little endian\" ); else printf ( \"Big endian\" ); getchar (); return 0 ; } #include <bits/stdc++.h> using namespace std ; int main () { unsigned int i = 1 ; char * c = ( char * ) & i ; if ( * c ) cout << \"Little endian\" ; else cout << \"Big endian\" ; return 0 ; } // This code is contributed by rathbhupendra Output: In the above program, a character pointer c is pointing to an integer i. Since size of character is 1 byte when the character pointer is de-referenced it will contain only first byte of integer. If machine is little endian then *c will be 1 (because last byte is stored first) and if machine is big endian then *c will be 0. Does endianness matter for programmers? Most of the times compiler takes care of endianness, however, endianness becomes an issue in following cases. It matters in network programming : Suppose you write integers to file on a little endian machine and you transfer this file to a big endian machine . Unless there is little endian to big endian transformation, big endian machine will read the file in reverse order. You can find such a practical example here. Standard byte order for networks is big endian , also known as network byte order . Before transferring data on network, data is first converted to network byte order ( big endian ). Sometimes it matters when you are using type casting , below program is an example. #include <stdio.h> int main () { unsigned char arr [ 2 ] = { 0x01 , 0x00 }; unsigned short int x = * ( unsigned short int * ) arr ; printf ( \"%d\" , x ); getchar (); return 0 ; } In the above program, a char array is typecasted to an unsigned short integer type. When I run above program on little endian machine, I get 1 as output, while if I run it on a big endian machine I get 256 . To make programs endianness independent, above programming style should be avoided. SUMMARY : unsigned short int x = *(unsigned short int *) arr; \u8bed\u53e5\u7684\u6548\u679c\u662f\u4f7f x \u7684\u5185\u5b58image\u4e3a 0X0100 \uff0c\u5bf9\u4e8e\u8fd9\u7bc7\u5185\u5b58\u7684\u89e3\u91ca\u662f\u673a\u5668\u76f8\u5173\u7684\uff0c\u5728big endian machine\u4e0a\uff0c\u5b83\u662f 256 \uff0c\u5728little endian machine\u4e0a\uff0c\u5b83\u662f 1 \u3002 SUMMARY : Writing endian-independent code in C","title":"Little and Big Endian Mystery"},{"location":"CPU-memory-access/Endianess/draft/Unix-get-endianess/#c#program#to#check#little#vs#big#endian#duplicate","text":"","title":"[C program to check little vs. big endian duplicate]"},{"location":"CPU-memory-access/Endianess/draft/Unix-get-endianess/#c#macro#definition#to#determine#big#endian#or#little#endian#machine","text":"","title":"C Macro definition to determine big endian or little endian machine?"},{"location":"CPU-memory-access/Endianess/draft/Writing%20endian-independent%20code%20in%20C/","text":"Writing endian-independent code in C","title":"[Writing endian-independent code in C](https://developer.ibm.com/articles/au-endianc/)"},{"location":"CPU-memory-access/Endianess/draft/Writing%20endian-independent%20code%20in%20C/#writing#endian-independent#code#in#c","text":"","title":"Writing endian-independent code in C"},{"location":"CPU-memory-access/Memory-access-instruction/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbamemory access instruction\uff0c\u5176\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9\u662f: atomic\uff0c\u8fd9\u5c06\u5728 ./Atomic \u7ae0\u8282\u4e2d\u8fdb\u884c\u8ba8\u8bba\u3002 Memory access instruction\u662f\u5426\u652f\u6301unaligned address\uff1f Intel X86 MOV \u662f\u652f\u6301 unalignend address\u7684\uff0c\u4f46\u662f\u65e0\u6cd5\u4fdd\u8bc1atomic\uff1b ARM \u4ece\u4e0b\u9762\u94fe\u63a5\u4e2d\u7684\u5185\u5bb9\u6765\u770b\uff0cARM\u662f\u53ef\u4ee5\u914d\u7f6e\u7684\u3002 1\u3001 https://developer.arm.com/documentation/dui0472/m/compiler-command-line-options/--unaligned-access----no-unaligned-access 2\u3001stackoverflow Take advantage of ARM unaligned memory access while writing clean C code","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/#_1","text":"\u672c\u7ae0\u8ba8\u8bbamemory access instruction\uff0c\u5176\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9\u662f: atomic\uff0c\u8fd9\u5c06\u5728 ./Atomic \u7ae0\u8282\u4e2d\u8fdb\u884c\u8ba8\u8bba\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/Memory-access-instruction/#memory#access#instructionunaligned#address","text":"","title":"Memory access instruction\u662f\u5426\u652f\u6301unaligned address\uff1f"},{"location":"CPU-memory-access/Memory-access-instruction/#intel","text":"X86 MOV \u662f\u652f\u6301 unalignend address\u7684\uff0c\u4f46\u662f\u65e0\u6cd5\u4fdd\u8bc1atomic\uff1b","title":"Intel"},{"location":"CPU-memory-access/Memory-access-instruction/#arm","text":"\u4ece\u4e0b\u9762\u94fe\u63a5\u4e2d\u7684\u5185\u5bb9\u6765\u770b\uff0cARM\u662f\u53ef\u4ee5\u914d\u7f6e\u7684\u3002 1\u3001 https://developer.arm.com/documentation/dui0472/m/compiler-command-line-options/--unaligned-access----no-unaligned-access 2\u3001stackoverflow Take advantage of ARM unaligned memory access while writing clean C code","title":"ARM"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbamemory access \u548c atomicity \u7684\u4e00\u4e9b\u8bdd\u9898\u3002 \u89e6\u53d1\u6211\u603b\u7ed3\u8fd9\u4e00\u7ae0\u8282\u7684\u51e0\u4e2a\u539f\u56e0 1\u3001preshing Atomic vs. Non-Atomic Operations \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u63a2\u5bfb\u4e86\u6700\u6700\u672c\u8d28\u7684\u539f\u56e0 2\u3001thread safety: \u4ece\u6307\u4ee4\u7ea7\u522b\u6765\u5206\u6790\u7ebf\u7a0b\u5b89\u5168\u6027 3\u3001\u6700\u5c0f\u5355\u4f4d: \u663e\u7136instruction\u662f\u6700\u5c0f\u5355\u4f4d\uff0c\u56e0\u6b64\u4e00\u6761instruction\u662fatomic\u7684 Memory access unit and atomic \u5728 ./Memory-access-unit-and-atomic \u7ae0\u8282\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u603b\u7ed3\u3002 Atomic instruction \u53c2\u89c1 ./Atomic-instruction \u7ae0\u8282\u3002 Application non-blocking algorithm\u3002 Implementation \u672c\u7ae0\u8bb2\u8ff0\u7684\u662f\u901a\u7528\u89c4\u5219\uff0c\u53e6\u5916\u5404\u4e2aCPU\u7684\u5382\u5546\u7684\u5b9e\u73b0\u6709\u6240\u4e0d\u540c\uff0c\u53c2\u89c1: 1\u3001Intel: Instruction-set-architectures\\Manufacturer\\Intel\\x86\\Atomicity-on-x86","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/#_1","text":"\u672c\u7ae0\u8ba8\u8bbamemory access \u548c atomicity \u7684\u4e00\u4e9b\u8bdd\u9898\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/#_2","text":"1\u3001preshing Atomic vs. Non-Atomic Operations \u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u597d\uff0c\u63a2\u5bfb\u4e86\u6700\u6700\u672c\u8d28\u7684\u539f\u56e0 2\u3001thread safety: \u4ece\u6307\u4ee4\u7ea7\u522b\u6765\u5206\u6790\u7ebf\u7a0b\u5b89\u5168\u6027 3\u3001\u6700\u5c0f\u5355\u4f4d: \u663e\u7136instruction\u662f\u6700\u5c0f\u5355\u4f4d\uff0c\u56e0\u6b64\u4e00\u6761instruction\u662fatomic\u7684","title":"\u89e6\u53d1\u6211\u603b\u7ed3\u8fd9\u4e00\u7ae0\u8282\u7684\u51e0\u4e2a\u539f\u56e0"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/#memory#access#unit#and#atomic","text":"\u5728 ./Memory-access-unit-and-atomic \u7ae0\u8282\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u603b\u7ed3\u3002","title":"Memory access unit and atomic"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/#atomic#instruction","text":"\u53c2\u89c1 ./Atomic-instruction \u7ae0\u8282\u3002","title":"Atomic instruction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/#application","text":"non-blocking algorithm\u3002","title":"Application"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/#implementation","text":"\u672c\u7ae0\u8bb2\u8ff0\u7684\u662f\u901a\u7528\u89c4\u5219\uff0c\u53e6\u5916\u5404\u4e2aCPU\u7684\u5382\u5546\u7684\u5b9e\u73b0\u6709\u6240\u4e0d\u540c\uff0c\u53c2\u89c1: 1\u3001Intel: Instruction-set-architectures\\Manufacturer\\Intel\\x86\\Atomicity-on-x86","title":"Implementation"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u6807\u9898\"instruction sequence\"\u7684\u542b\u4e49\u662f\u6307\u4ee4\u5e8f\u5217\uff0c\u5b83\u662f\u6307\u901a\u8fc7\u7279\u5b9a\u7684\u6307\u4ee4\u5e8f\u5217\u6765\u5b9e\u73b0concurrency control\u7684\u65b9\u5f0f\u3002 \u5b9e\u73b0\u601d\u8def \u4f9d\u636emultiple model\uff0c\u6211\u4eec\u53ef\u77e5\uff0c\u5bf9memory\u7684operation\u53ef\u4ee5\u5206\u4e3a\u4e24\u5927\u7c7b: 1\u3001read 2\u3001write atomic operation\u7684\u5b9e\u73b0\u601d\u8def\u662f\u5178\u578b\u7684\"assemble as atomic primitive\": \u5c06\u53ef\u80fd\u7684read\u3001write \u64cd\u4f5c\u8fdb\u884c**\u96c6\u6210**\uff0c\u63d0\u4f9b\u5b9e\u73b0**\u96c6\u6210\u529f\u80fd**\u7684atomic instruction\uff0c\u5178\u578b\u7684\u4f8b\u5b50\u5305\u62ec: Test-and-set Fetch-and-add Compare-and-swap","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/#_1","text":"\u672c\u7ae0\u6807\u9898\"instruction sequence\"\u7684\u542b\u4e49\u662f\u6307\u4ee4\u5e8f\u5217\uff0c\u5b83\u662f\u6307\u901a\u8fc7\u7279\u5b9a\u7684\u6307\u4ee4\u5e8f\u5217\u6765\u5b9e\u73b0concurrency control\u7684\u65b9\u5f0f\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/#_2","text":"\u4f9d\u636emultiple model\uff0c\u6211\u4eec\u53ef\u77e5\uff0c\u5bf9memory\u7684operation\u53ef\u4ee5\u5206\u4e3a\u4e24\u5927\u7c7b: 1\u3001read 2\u3001write atomic operation\u7684\u5b9e\u73b0\u601d\u8def\u662f\u5178\u578b\u7684\"assemble as atomic primitive\": \u5c06\u53ef\u80fd\u7684read\u3001write \u64cd\u4f5c\u8fdb\u884c**\u96c6\u6210**\uff0c\u63d0\u4f9b\u5b9e\u73b0**\u96c6\u6210\u529f\u80fd**\u7684atomic instruction\uff0c\u5178\u578b\u7684\u4f8b\u5b50\u5305\u62ec: Test-and-set Fetch-and-add Compare-and-swap","title":"\u5b9e\u73b0\u601d\u8def"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Conditional-Put-and-Delete/","text":"","title":"Conditional-Put-and-Delete"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/","text":"Read-modify-write Wikipedia Read-modify-write In computer science , read-modify-write is a class of atomic operations (such as test-and-set , fetch-and-add , and compare-and-swap ) that both read a memory location and write a new value into it simultaneously(\u540c\u65f6), either with a completely new value or some function of the previous value. NOTE: \"simultaneously\"\u5176\u5b9e\u5c31\u610f\u5473\u7740 \"\u539f\u5b50\u6027\" \u3002\"some function of previous value\" \u7684\u610f\u601d\u662f \u5c06previous value\u4f5c\u4e3a\u5165\u53c2\u8f93\u5165\u5230\u4e00\u4e2afunction\u4e2d\u800c\u5f97\u5230\u4e00\u4e2a\u65b0value\uff0c\u7136\u540e\u5c06\u8fd9\u4e2a\u65b0value\u5199\u5165\u5230memory\u4e2d These operations prevent race conditions in multi-threaded applications. Typically they are used to implement mutexes or semaphores . These atomic operations are also heavily used in non-blocking synchronization . NOTE: \u6211\u4eec\u5e73\u65f6\u6240\u4f7f\u7528\u7684mutex\u3001semaphore\u90fd\u662f\u57fa\u4e8e\u8fd9\u4e9batomic operation\u800c\u5b9e\u73b0\u7684\u3002 Maurice Herlihy (1991) ranks atomic operations by their consensus numbers, as follows: NOTE: \u5728CPU\u4e2d\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u56e0\u6b64\u5b83\u4e5f\u53ef\u4ee5\u4f7f\u7528consensus\u5f97\u5230\u6982\u5ff5 rank \u221e memory-to-memory move and swap, augmented queue, compare-and-swap , fetch-and-cons , sticky byte , load-link/store-conditional (LL/SC)[ 1] 2n - 2 : n-register assignment 2 test-and-set , swap, fetch-and-add , queue, stack 1 atomic read and atomic write NOTE:\u4e0a\u8ff0\u81ea\u5e95\u5411\u4e0a\u4f9d\u6b21\u589e\u957f It is impossible to implement an operation that requires a given consensus number with only operations with a lower consensus number , no matter how many of such operations one uses.[ 2] Read-modify-write instructions often produce unexpected results when used on I/O devices, as a write operation may not affect the same internal register that would be accessed in a read operation.[ 3] stackoverflow Why it's termed read-modify-write but not read-write? A Because that is exactly the sequence of events on a typical architecture such as X86 . read : The value is read from a memory location (cache) into a CPU register modify : The value is incremented inside the CPU register write : The updated register value is written back to memory (cache). In order to create the perception of atomicity, the cache line is locked between the read and the write operation. For example, incrementing a C++ atomic variable: g . fetch_add ( 1 ); Is compiled by gcc into: 0x00000000004006c0 <+0>: lock addl $0x1,0x200978(%rip) # 0x601040 <g> Despite being a single instruction, addl is not atomic by itself. The lock prefix is necessary to guarantee that the updated register value is written back to the cache line before it can be accessed by other cores (the store buffer is flushed, but bypassed for RMW operations). The MESI cache coherence protocol ensures that all cores observe the updated memory value after the lock on the cache line has been released. This guarantees that all threads observe the latest value in the modification order which is required for RMW operations by the C++ standard. NOTE: \u89e3\u91ca\u5f97\u975e\u5e38\u597d\u3002 Compare-and-swap VS test-and-set stackoverflow compare and swap vs test and set A : test-and-set modifies the contents of a memory location and returns its old value as a single atomic operation. compare-and-swap atomically compares the contents of a memory location to a given value and, only if they are the same , modifies the contents of that memory location to a given new value. The difference marked in bold. A : Test and set operates on a bit, compare and swap operates on a 32-bit field. TODO: \u539f\u5b50\u64cd\u4f5c\uff1ablog CAS\u3001TAS\u3001TTAS\u3001FAA\u6d45\u6790","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/#read-modify-write","text":"","title":"Read-modify-write"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/#wikipedia#read-modify-write","text":"In computer science , read-modify-write is a class of atomic operations (such as test-and-set , fetch-and-add , and compare-and-swap ) that both read a memory location and write a new value into it simultaneously(\u540c\u65f6), either with a completely new value or some function of the previous value. NOTE: \"simultaneously\"\u5176\u5b9e\u5c31\u610f\u5473\u7740 \"\u539f\u5b50\u6027\" \u3002\"some function of previous value\" \u7684\u610f\u601d\u662f \u5c06previous value\u4f5c\u4e3a\u5165\u53c2\u8f93\u5165\u5230\u4e00\u4e2afunction\u4e2d\u800c\u5f97\u5230\u4e00\u4e2a\u65b0value\uff0c\u7136\u540e\u5c06\u8fd9\u4e2a\u65b0value\u5199\u5165\u5230memory\u4e2d These operations prevent race conditions in multi-threaded applications. Typically they are used to implement mutexes or semaphores . These atomic operations are also heavily used in non-blocking synchronization . NOTE: \u6211\u4eec\u5e73\u65f6\u6240\u4f7f\u7528\u7684mutex\u3001semaphore\u90fd\u662f\u57fa\u4e8e\u8fd9\u4e9batomic operation\u800c\u5b9e\u73b0\u7684\u3002 Maurice Herlihy (1991) ranks atomic operations by their consensus numbers, as follows: NOTE: \u5728CPU\u4e2d\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u56e0\u6b64\u5b83\u4e5f\u53ef\u4ee5\u4f7f\u7528consensus\u5f97\u5230\u6982\u5ff5 rank \u221e memory-to-memory move and swap, augmented queue, compare-and-swap , fetch-and-cons , sticky byte , load-link/store-conditional (LL/SC)[ 1] 2n - 2 : n-register assignment 2 test-and-set , swap, fetch-and-add , queue, stack 1 atomic read and atomic write NOTE:\u4e0a\u8ff0\u81ea\u5e95\u5411\u4e0a\u4f9d\u6b21\u589e\u957f It is impossible to implement an operation that requires a given consensus number with only operations with a lower consensus number , no matter how many of such operations one uses.[ 2] Read-modify-write instructions often produce unexpected results when used on I/O devices, as a write operation may not affect the same internal register that would be accessed in a read operation.[ 3]","title":"Wikipedia Read-modify-write"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/#stackoverflow#why#its#termed#read-modify-write#but#not#read-write","text":"","title":"stackoverflow Why it's termed read-modify-write but not read-write?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/#a","text":"Because that is exactly the sequence of events on a typical architecture such as X86 . read : The value is read from a memory location (cache) into a CPU register modify : The value is incremented inside the CPU register write : The updated register value is written back to memory (cache). In order to create the perception of atomicity, the cache line is locked between the read and the write operation. For example, incrementing a C++ atomic variable: g . fetch_add ( 1 ); Is compiled by gcc into: 0x00000000004006c0 <+0>: lock addl $0x1,0x200978(%rip) # 0x601040 <g> Despite being a single instruction, addl is not atomic by itself. The lock prefix is necessary to guarantee that the updated register value is written back to the cache line before it can be accessed by other cores (the store buffer is flushed, but bypassed for RMW operations). The MESI cache coherence protocol ensures that all cores observe the updated memory value after the lock on the cache line has been released. This guarantees that all threads observe the latest value in the modification order which is required for RMW operations by the C++ standard. NOTE: \u89e3\u91ca\u5f97\u975e\u5e38\u597d\u3002","title":"A"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/#compare-and-swap#vs#test-and-set","text":"stackoverflow compare and swap vs test and set A : test-and-set modifies the contents of a memory location and returns its old value as a single atomic operation. compare-and-swap atomically compares the contents of a memory location to a given value and, only if they are the same , modifies the contents of that memory location to a given new value. The difference marked in bold. A : Test and set operates on a bit, compare and swap operates on a 32-bit field.","title":"Compare-and-swap VS test-and-set"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/#todo#blog#castasttasfaa","text":"","title":"TODO: \u539f\u5b50\u64cd\u4f5c\uff1ablog CAS\u3001TAS\u3001TTAS\u3001FAA\u6d45\u6790"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/","text":"Compare and swap \u4e00\u3001\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u4e5f\u6d89\u53ca\u4e86\u8fd9\u4e2atopic: 1\u3001preshing An Introduction to Lock-Free Programming \u4e8c\u3001\u5728\u9605\u8bfb\u4e86 1\u3001wikipedia Optimistic concurrency control \u5176\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u89c2\u70b9\u5c31\u662f: transaction \u662f optimistic concurrency control 2\u3001zhuanlan \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f\u540e\uff0c\u6211\u89c9\u5f97\u4f7f\u7528 \u4f7f\u7528CAS\u3001MVCC\u6765\u5b9e\u73b0optimistic concurrency control\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u672c\u8d28\u4e0a\u662f\u6709\u4e9b\u7c7b\u4f3c\u7684\u3002 \u540e\uff0c\u6211\u5bf9compare-and-swap\u7684\u8ba4\u77e5\u662f: \u4f7f\u7528transaction\u6765\u7406\u89e3compare-and-swap: \u901a\u8fc7\"compare\"\u6765\u5224\u65ad\u5728\u8fd9\u6bb5\u65f6\u95f4\u5185\u662f\u5426\u53d1\u751f\u4e86\u72b6\u6001\u6539\u53d8\uff0c\u5982\u679c\u72b6\u6001\u6539\u53d8\u4e86\uff0c\u5219\u7ec8\u6b62transaction\uff0c\u5426\u5219commit\u4fee\u6539\u3002 wikipedia Compare-and-swap In computer science , compare-and-swap ( CAS ) is an atomic instruction used in multithreading to achieve synchronization . It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation . The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple boolean response (this variant is often called compare-and-set ), or by returning the value read from the memory location ( not the value written to it). Overview A compare-and-swap operation is an atomic version of the following pseudocode , where * denotes access through a pointer :[ 1] function cas(p : pointer to int, old : int, new : int) returns bool { if *p \u2260 old { return false } *p \u2190 new return true } This operation is used to implement synchronization primitives like semaphores and mutexes ,[ 1] as well as more sophisticated lock-free and wait-free algorithms . Maurice Herlihy (1991) proved that CAS can implement more of these algorithms than atomic read, write, or fetch-and-add , and assuming a fairly large[ clarification needed ] amount of memory, that it can implement all of them.[ 2] CAS is equivalent to load-link/store-conditional , in the sense that a constant number of invocations of either primitive can be used to implement the other one in a wait-free manner.[ 3] NOTE : \u7ffb\u8bd1\u5982\u4e0b: \"\u6b64\u64cd\u4f5c\u7528\u4e8e\u5b9e\u73b0\u4fe1\u53f7\u91cf\u548c\u4e92\u65a5\u91cf\u7b49\u540c\u6b65\u539f\u8bed\uff0c\u4ee5\u53ca\u66f4\u590d\u6742\u7684\u65e0\u9501\u548c\u65e0\u7b49\u5f85\u7b97\u6cd5\u3002 Maurice Herlihy\uff081991\uff09\u8bc1\u660e\u4e86CAS\u53ef\u4ee5\u5b9e\u73b0\u66f4\u591a\u8fd9\u4e9b\u7b97\u6cd5\u800c\u4e0d\u662f\u539f\u5b50\u8bfb\u53d6\uff0c\u5199\u5165\u6216\u83b7\u53d6\u548c\u6dfb\u52a0\uff0c\u5e76\u4e14\u5047\u8bbe\u5b58\u50a8\u91cf\u76f8\u5f53\u5927\uff0c\u5b83\u53ef\u4ee5\u5b9e\u73b0\u6240\u6709\u8fd9\u4e9b\u7b97\u6cd5.CAS\u7b49\u540c\u4e8e\u52a0\u8f7d - link / store-conditional\uff0c\u5728\u67d0\u79cd\u610f\u4e49\u4e0a\uff0c\u53ef\u4ee5\u4f7f\u7528\u4efb\u4e00\u539f\u8bed\u7684\u5e38\u91cf\u8c03\u7528\u6765\u4ee5\u65e0\u7b49\u5f85\u7684\u65b9\u5f0f\u5b9e\u73b0\u53e6\u4e00\u4e2a\u539f\u8bed\u3002\" Algorithms built around CAS typically read some key memory location and remember the old value. Based on that old value, they compute some new value. Then they try to swap in the new value using CAS, where the comparison checks for the location still being equal to the old value. If CAS indicates that the attempt has failed, it has to be repeated from the beginning: the location is re-read, a new value is re-computed and the CAS is tried again. Instead of immediately retrying after a CAS operation fails, researchers have found that total system performance can be improved in multiprocessor systems\u2014where many threads constantly update some particular shared variable\u2014if threads that see their CAS fail use exponential backoff \u2014in other words, wait a little before retrying the CAS.[ 4] Example application: atomic adder function add ( p : pointer to int , a : int ) returns int { done \u2190 false while not done { value \u2190 * p // Even this operation doesn't need to be atomic. done \u2190 cas ( p , value , value + a ) } return value + a } NOTE: 1\u3001\u5982\u679c p \u548c value\u76f8\u7b49(\u8bf4\u660e\u8fd9\u6bb5\u65f6\u95f4\u5185\u6ca1\u6709\u5176\u4ed6\u7684transaction\u53d1\u751f)\uff0c\u5219\u5c06value + p \u9012\u4ea4\uff1b\u5426\u5219\u4e0d\u9012\u4ea4\uff0c\u7ec8\u6b62\u3002 In this algorithm, if the value of *p changes after (or while!) it is fetched and before the CAS does the store, CAS will notice and report this fact, causing the algorithm to retry.[ 5] \u52a3\u52bf \u548c \u5c40\u9650\u6027 1\u3001zhihu \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f \u4e0b\u9762\u662fCAS\u4e00\u4e9b\u4e0d\u90a3\u4e48\u5b8c\u7f8e\u7684\u5730\u65b9\uff1a 1\u3001ABA\u95ee\u9898 \u5728 AtomicInteger \u7684\u4f8b\u5b50\u4e2d\uff0cABA\u4f3c\u4e4e\u6ca1\u6709\u4ec0\u4e48\u5371\u5bb3\u3002\u4f46\u662f\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0cABA\u5374\u4f1a\u5e26\u6765\u9690\u60a3\uff0c\u4f8b\u5982\u6808\u9876\u95ee\u9898\uff1a\u4e00\u4e2a\u6808\u7684\u6808\u9876\u7ecf\u8fc7\u4e24\u6b21(\u6216\u591a\u6b21)\u53d8\u5316\u53c8\u6062\u590d\u4e86\u539f\u503c\uff0c\u4f46\u662f\u6808\u53ef\u80fd\u5df2\u53d1\u751f\u4e86\u53d8\u5316\u3002 \u5bf9\u4e8eABA\u95ee\u9898\uff0c\u6bd4\u8f83\u6709\u6548\u7684\u65b9\u6848\u662f\u5f15\u5165\u7248\u672c\u53f7\uff0c\u5185\u5b58\u4e2d\u7684\u503c\u6bcf\u53d1\u751f\u4e00\u6b21\u53d8\u5316\uff0c\u7248\u672c\u53f7\u90fd+1\uff1b\u5728\u8fdb\u884cCAS\u64cd\u4f5c\u65f6\uff0c\u4e0d\u4ec5\u6bd4\u8f83\u5185\u5b58\u4e2d\u7684\u503c\uff0c\u4e5f\u4f1a\u6bd4\u8f83\u7248\u672c\u53f7\uff0c\u53ea\u6709\u5f53\u4e8c\u8005\u90fd\u6ca1\u6709\u53d8\u5316\u65f6\uff0cCAS\u624d\u80fd\u6267\u884c\u6210\u529f\u3002Java\u4e2d\u7684 AtomicStampedReference \u7c7b\u4fbf\u662f\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898\u7684\u3002 2\u3001\u9ad8\u7ade\u4e89\u4e0b\u7684\u5f00\u9500\u95ee\u9898 \u5728\u5e76\u53d1\u51b2\u7a81\u6982\u7387\u5927\u7684\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\uff0c\u5982\u679cCAS\u4e00\u76f4\u5931\u8d25\uff0c\u4f1a\u4e00\u76f4\u91cd\u8bd5\uff0cCPU\u5f00\u9500\u8f83\u5927\u3002\u9488\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u601d\u8def\u662f\u5f15\u5165\u9000\u51fa\u673a\u5236\uff0c\u5982\u91cd\u8bd5\u6b21\u6570\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u540e\u5931\u8d25\u9000\u51fa\u3002\u5f53\u7136\uff0c\u66f4\u91cd\u8981\u7684\u662f\u907f\u514d\u5728\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\u4f7f\u7528\u4e50\u89c2\u9501\u3002 3\u3001\u529f\u80fd\u9650\u5236 CAS\u7684\u529f\u80fd\u662f\u6bd4\u8f83\u53d7\u9650\u7684\uff0c\u4f8b\u5982CAS\u53ea\u80fd\u4fdd\u8bc1\u5355\u4e2a\u53d8\u91cf\uff08\u6216\u8005\u8bf4\u5355\u4e2a\u5185\u5b58\u503c\uff09\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u8fd9\u610f\u5473\u7740\uff1a(1)\u539f\u5b50\u6027\u4e0d\u4e00\u5b9a\u80fd\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff0c\u4f8b\u5982\u5728Java\u4e2d\u9700\u8981\u4e0evolatile\u914d\u5408\u6765\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff1b(2)\u5f53\u6d89\u53ca\u5230\u591a\u4e2a\u53d8\u91cf(\u5185\u5b58\u503c)\u65f6\uff0cCAS\u4e5f\u65e0\u80fd\u4e3a\u529b\u3002 NOTE: 1\u3001\u591a\u4e2aCAS optimistic lock\u65e0\u6cd5\u5de5\u4f5c \u9664\u6b64\u4e4b\u5916\uff0cCAS\u7684\u5b9e\u73b0\u9700\u8981\u786c\u4ef6\u5c42\u9762\u5904\u7406\u5668\u7684\u652f\u6301\uff0c\u5728Java\u4e2d\u666e\u901a\u7528\u6237\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528\uff0c\u53ea\u80fd\u501f\u52a9atomic\u5305\u4e0b\u7684\u539f\u5b50\u7c7b\u4f7f\u7528\uff0c\u7075\u6d3b\u6027\u53d7\u5230\u9650\u5236\u3002","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#compare#and#swap","text":"\u4e00\u3001\u5728\u4e0b\u9762\u6587\u7ae0\u4e2d\uff0c\u4e5f\u6d89\u53ca\u4e86\u8fd9\u4e2atopic: 1\u3001preshing An Introduction to Lock-Free Programming \u4e8c\u3001\u5728\u9605\u8bfb\u4e86 1\u3001wikipedia Optimistic concurrency control \u5176\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u89c2\u70b9\u5c31\u662f: transaction \u662f optimistic concurrency control 2\u3001zhuanlan \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f\u540e\uff0c\u6211\u89c9\u5f97\u4f7f\u7528 \u4f7f\u7528CAS\u3001MVCC\u6765\u5b9e\u73b0optimistic concurrency control\uff0c\u8fd9\u4e24\u79cd\u65b9\u5f0f\u672c\u8d28\u4e0a\u662f\u6709\u4e9b\u7c7b\u4f3c\u7684\u3002 \u540e\uff0c\u6211\u5bf9compare-and-swap\u7684\u8ba4\u77e5\u662f: \u4f7f\u7528transaction\u6765\u7406\u89e3compare-and-swap: \u901a\u8fc7\"compare\"\u6765\u5224\u65ad\u5728\u8fd9\u6bb5\u65f6\u95f4\u5185\u662f\u5426\u53d1\u751f\u4e86\u72b6\u6001\u6539\u53d8\uff0c\u5982\u679c\u72b6\u6001\u6539\u53d8\u4e86\uff0c\u5219\u7ec8\u6b62transaction\uff0c\u5426\u5219commit\u4fee\u6539\u3002","title":"Compare and swap"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#wikipedia#compare-and-swap","text":"In computer science , compare-and-swap ( CAS ) is an atomic instruction used in multithreading to achieve synchronization . It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation . The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple boolean response (this variant is often called compare-and-set ), or by returning the value read from the memory location ( not the value written to it).","title":"wikipedia Compare-and-swap"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#overview","text":"A compare-and-swap operation is an atomic version of the following pseudocode , where * denotes access through a pointer :[ 1] function cas(p : pointer to int, old : int, new : int) returns bool { if *p \u2260 old { return false } *p \u2190 new return true } This operation is used to implement synchronization primitives like semaphores and mutexes ,[ 1] as well as more sophisticated lock-free and wait-free algorithms . Maurice Herlihy (1991) proved that CAS can implement more of these algorithms than atomic read, write, or fetch-and-add , and assuming a fairly large[ clarification needed ] amount of memory, that it can implement all of them.[ 2] CAS is equivalent to load-link/store-conditional , in the sense that a constant number of invocations of either primitive can be used to implement the other one in a wait-free manner.[ 3] NOTE : \u7ffb\u8bd1\u5982\u4e0b: \"\u6b64\u64cd\u4f5c\u7528\u4e8e\u5b9e\u73b0\u4fe1\u53f7\u91cf\u548c\u4e92\u65a5\u91cf\u7b49\u540c\u6b65\u539f\u8bed\uff0c\u4ee5\u53ca\u66f4\u590d\u6742\u7684\u65e0\u9501\u548c\u65e0\u7b49\u5f85\u7b97\u6cd5\u3002 Maurice Herlihy\uff081991\uff09\u8bc1\u660e\u4e86CAS\u53ef\u4ee5\u5b9e\u73b0\u66f4\u591a\u8fd9\u4e9b\u7b97\u6cd5\u800c\u4e0d\u662f\u539f\u5b50\u8bfb\u53d6\uff0c\u5199\u5165\u6216\u83b7\u53d6\u548c\u6dfb\u52a0\uff0c\u5e76\u4e14\u5047\u8bbe\u5b58\u50a8\u91cf\u76f8\u5f53\u5927\uff0c\u5b83\u53ef\u4ee5\u5b9e\u73b0\u6240\u6709\u8fd9\u4e9b\u7b97\u6cd5.CAS\u7b49\u540c\u4e8e\u52a0\u8f7d - link / store-conditional\uff0c\u5728\u67d0\u79cd\u610f\u4e49\u4e0a\uff0c\u53ef\u4ee5\u4f7f\u7528\u4efb\u4e00\u539f\u8bed\u7684\u5e38\u91cf\u8c03\u7528\u6765\u4ee5\u65e0\u7b49\u5f85\u7684\u65b9\u5f0f\u5b9e\u73b0\u53e6\u4e00\u4e2a\u539f\u8bed\u3002\" Algorithms built around CAS typically read some key memory location and remember the old value. Based on that old value, they compute some new value. Then they try to swap in the new value using CAS, where the comparison checks for the location still being equal to the old value. If CAS indicates that the attempt has failed, it has to be repeated from the beginning: the location is re-read, a new value is re-computed and the CAS is tried again. Instead of immediately retrying after a CAS operation fails, researchers have found that total system performance can be improved in multiprocessor systems\u2014where many threads constantly update some particular shared variable\u2014if threads that see their CAS fail use exponential backoff \u2014in other words, wait a little before retrying the CAS.[ 4]","title":"Overview"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#example#application#atomic#adder","text":"function add ( p : pointer to int , a : int ) returns int { done \u2190 false while not done { value \u2190 * p // Even this operation doesn't need to be atomic. done \u2190 cas ( p , value , value + a ) } return value + a } NOTE: 1\u3001\u5982\u679c p \u548c value\u76f8\u7b49(\u8bf4\u660e\u8fd9\u6bb5\u65f6\u95f4\u5185\u6ca1\u6709\u5176\u4ed6\u7684transaction\u53d1\u751f)\uff0c\u5219\u5c06value + p \u9012\u4ea4\uff1b\u5426\u5219\u4e0d\u9012\u4ea4\uff0c\u7ec8\u6b62\u3002 In this algorithm, if the value of *p changes after (or while!) it is fetched and before the CAS does the store, CAS will notice and report this fact, causing the algorithm to retry.[ 5]","title":"Example application: atomic adder"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#_1","text":"","title":"\u52a3\u52bf \u548c \u5c40\u9650\u6027"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#1zhihu#bat","text":"\u4e0b\u9762\u662fCAS\u4e00\u4e9b\u4e0d\u90a3\u4e48\u5b8c\u7f8e\u7684\u5730\u65b9\uff1a","title":"1\u3001zhihu \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#1aba","text":"\u5728 AtomicInteger \u7684\u4f8b\u5b50\u4e2d\uff0cABA\u4f3c\u4e4e\u6ca1\u6709\u4ec0\u4e48\u5371\u5bb3\u3002\u4f46\u662f\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0cABA\u5374\u4f1a\u5e26\u6765\u9690\u60a3\uff0c\u4f8b\u5982\u6808\u9876\u95ee\u9898\uff1a\u4e00\u4e2a\u6808\u7684\u6808\u9876\u7ecf\u8fc7\u4e24\u6b21(\u6216\u591a\u6b21)\u53d8\u5316\u53c8\u6062\u590d\u4e86\u539f\u503c\uff0c\u4f46\u662f\u6808\u53ef\u80fd\u5df2\u53d1\u751f\u4e86\u53d8\u5316\u3002 \u5bf9\u4e8eABA\u95ee\u9898\uff0c\u6bd4\u8f83\u6709\u6548\u7684\u65b9\u6848\u662f\u5f15\u5165\u7248\u672c\u53f7\uff0c\u5185\u5b58\u4e2d\u7684\u503c\u6bcf\u53d1\u751f\u4e00\u6b21\u53d8\u5316\uff0c\u7248\u672c\u53f7\u90fd+1\uff1b\u5728\u8fdb\u884cCAS\u64cd\u4f5c\u65f6\uff0c\u4e0d\u4ec5\u6bd4\u8f83\u5185\u5b58\u4e2d\u7684\u503c\uff0c\u4e5f\u4f1a\u6bd4\u8f83\u7248\u672c\u53f7\uff0c\u53ea\u6709\u5f53\u4e8c\u8005\u90fd\u6ca1\u6709\u53d8\u5316\u65f6\uff0cCAS\u624d\u80fd\u6267\u884c\u6210\u529f\u3002Java\u4e2d\u7684 AtomicStampedReference \u7c7b\u4fbf\u662f\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898\u7684\u3002","title":"1\u3001ABA\u95ee\u9898"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#2","text":"\u5728\u5e76\u53d1\u51b2\u7a81\u6982\u7387\u5927\u7684\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\uff0c\u5982\u679cCAS\u4e00\u76f4\u5931\u8d25\uff0c\u4f1a\u4e00\u76f4\u91cd\u8bd5\uff0cCPU\u5f00\u9500\u8f83\u5927\u3002\u9488\u5bf9\u8fd9\u4e2a\u95ee\u9898\u7684\u4e00\u4e2a\u601d\u8def\u662f\u5f15\u5165\u9000\u51fa\u673a\u5236\uff0c\u5982\u91cd\u8bd5\u6b21\u6570\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u540e\u5931\u8d25\u9000\u51fa\u3002\u5f53\u7136\uff0c\u66f4\u91cd\u8981\u7684\u662f\u907f\u514d\u5728\u9ad8\u7ade\u4e89\u73af\u5883\u4e0b\u4f7f\u7528\u4e50\u89c2\u9501\u3002","title":"2\u3001\u9ad8\u7ade\u4e89\u4e0b\u7684\u5f00\u9500\u95ee\u9898"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/#3","text":"CAS\u7684\u529f\u80fd\u662f\u6bd4\u8f83\u53d7\u9650\u7684\uff0c\u4f8b\u5982CAS\u53ea\u80fd\u4fdd\u8bc1\u5355\u4e2a\u53d8\u91cf\uff08\u6216\u8005\u8bf4\u5355\u4e2a\u5185\u5b58\u503c\uff09\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff0c\u8fd9\u610f\u5473\u7740\uff1a(1)\u539f\u5b50\u6027\u4e0d\u4e00\u5b9a\u80fd\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff0c\u4f8b\u5982\u5728Java\u4e2d\u9700\u8981\u4e0evolatile\u914d\u5408\u6765\u4fdd\u8bc1\u7ebf\u7a0b\u5b89\u5168\uff1b(2)\u5f53\u6d89\u53ca\u5230\u591a\u4e2a\u53d8\u91cf(\u5185\u5b58\u503c)\u65f6\uff0cCAS\u4e5f\u65e0\u80fd\u4e3a\u529b\u3002 NOTE: 1\u3001\u591a\u4e2aCAS optimistic lock\u65e0\u6cd5\u5de5\u4f5c \u9664\u6b64\u4e4b\u5916\uff0cCAS\u7684\u5b9e\u73b0\u9700\u8981\u786c\u4ef6\u5c42\u9762\u5904\u7406\u5668\u7684\u652f\u6301\uff0c\u5728Java\u4e2d\u666e\u901a\u7528\u6237\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528\uff0c\u53ea\u80fd\u501f\u52a9atomic\u5305\u4e0b\u7684\u539f\u5b50\u7c7b\u4f7f\u7528\uff0c\u7075\u6d3b\u6027\u53d7\u5230\u9650\u5236\u3002","title":"3\u3001\u529f\u80fd\u9650\u5236"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/","text":"ABA problem \u5728\u9605\u8bfb preshing An Introduction to Lock-Free Programming \u65f6\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86ABA problem\u3002 \u603b\u7ed3 1\u3001ABA\u4e5f\u662f\u7531\u4e8erace\u800c\u5f15\u53d1\u7684 2\u3001ABA\u548ctime of check to time of use\u662f\u975e\u5e38\u7c7b\u4f3c\u7684 Wikipedia ABA problem In multithreaded computing , the ABA problem occurs during synchronization, when a location is read twice, has the same value for both reads, and \"value is the same\" is used to indicate \"nothing has changed\". However, another thread can execute between the two reads and change the value, do other work, then change the value back, thus fooling the first thread into thinking \"nothing has changed\" even though the second thread did work that violates that assumption. In multithreaded computing , the ABA problem occurs during synchronization, when a location is read twice, has the same value for both reads, and \"value is the same\" is used to indicate \"nothing has changed\". However, another thread can execute between the two reads and change the value, do other work, then change the value back, thus fooling the first thread into thinking \"nothing has changed\" even though the second thread did work that violates that assumption. The ABA problem occurs when multiple threads (or processes ) accessing shared data interleave. Below is the sequence of events that will result in the ABA problem: Process {P_{1}} {P_{1}} reads value A from shared memory, {P_{1}} {P_{1}} is preempted , allowing process {P_{2}} {P_{2}} to run, {P_{2}} {P_{2}} modifies the shared memory value A to value B and back to A before preemption, {P_{1}} {P_{1}} begins execution again, sees that the shared memory value has not changed and continues. Although {P_{1}} {P_{1}} can continue executing, it is possible that the behavior will not be correct due to the \"hidden\" modification in shared memory. A common case of the ABA problem is encountered when implementing a lock-free data structure. If an item is removed from the list, deleted, and then a new item is allocated and added to the list, it is common for the allocated object to be at the same location as the deleted object due to MRU memory allocation. A pointer to the new item is thus often equal to a pointer to the old item, causing an ABA problem. NOTE: MRU \u6240\u8868\u793a\u7684\u662fMost recently used Examples Consider a software example of ABA using a lock-free stack : #include <iostream> #include <utility> #include <vector> #include <atomic> using namespace std ; class Obj { public : Obj * next { nullptr }; }; /* Naive lock-free stack which suffers from ABA problem.*/ class Stack { std :: atomic < Obj *> top_ptr ; // // Pops the top object and returns a pointer to it. // Obj * Pop () { while ( 1 ) { Obj * ret_ptr = top_ptr ; if ( ! ret_ptr ) return nullptr ; // For simplicity, suppose that we can ensure that this dereference is safe // (i.e., that no other thread has popped the stack in the meantime). Obj * next_ptr = ret_ptr -> next ; // If the top node is still ret, then assume no one has changed the stack. // (That statement is not always true because of the ABA problem) // Atomically replace top with next. if ( top_ptr . compare_exchange_weak ( ret_ptr , next_ptr )) { return ret_ptr ; } // The stack has changed, start over. } } // // Pushes the object specified by obj_ptr to stack. // void Push ( Obj * obj_ptr ) { while ( 1 ) { Obj * next_ptr = top_ptr ; obj_ptr -> next = next_ptr ; // If the top node is still next, then assume no one has changed the stack. // (That statement is not always true because of the ABA problem) // Atomically replace top with obj. if ( top_ptr . compare_exchange_weak ( next_ptr , obj_ptr )) { return ; } // The stack has changed, start over. } } }; int main () { } // g++ -std=c++11 -Wall -pedantic -pthread main.cpp && ./a.out Workarounds Tagged state reference Intermediate nodes Deferred reclamation Workarounds\u603b\u7ed3 zhuanlan \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f NOTE: 1\u3001\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898 \u5bf9\u4e8eABA\u95ee\u9898\uff0c\u6bd4\u8f83\u6709\u6548\u7684\u65b9\u6848\u662f\u5f15\u5165\u7248\u672c\u53f7\uff0c\u5185\u5b58\u4e2d\u7684\u503c\u6bcf\u53d1\u751f\u4e00\u6b21\u53d8\u5316\uff0c\u7248\u672c\u53f7\u90fd+1\uff1b\u5728\u8fdb\u884cCAS\u64cd\u4f5c\u65f6\uff0c\u4e0d\u4ec5\u6bd4\u8f83\u5185\u5b58\u4e2d\u7684\u503c\uff0c\u4e5f\u4f1a\u6bd4\u8f83\u7248\u672c\u53f7\uff0c\u53ea\u6709\u5f53\u4e8c\u8005\u90fd\u6ca1\u6709\u53d8\u5316\u65f6\uff0cCAS\u624d\u80fd\u6267\u884c\u6210\u529f\u3002Java\u4e2d\u7684 AtomicStampedReference \u7c7b\u4fbf\u662f\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898\u7684\u3002 Herb Sutter Lock-Free Programming or, How to Juggle Razor Blades \u5728 Herb Sutter Lock-Free Programming or, How to Juggle Razor Blades \u4e2d\uff0c\u7ed9\u51fa\u4e86\u603b\u7ed3\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6536\u5f55\u4e8e\u5de5\u7a0bprogramming-language\u7684 Lock-Free-Programming-or-How-to-Juggle-Razor-Blades \u7ae0\u8282\u3002 ABA Solutions (sketch) We need to solve the ABA issue: Two nodes with the same address, but different identities (existing at different times). Option 1: Use lazy garbage collection. Solves the problem. Memory can\u2019t be reused while pointers to it exist. But: Not an option (yet) in portable C++ code, and destruction of nodes becomes nondeterministic. Option 2: Use reference counting (garbage collection). Solves the problem in cases without cycles. Again, avoids memory reuse. Option 3: Make each pointer unique by appending a serial number, and increment the serial number each time it\u2019s set. This way we can always distinguish between A and A\u2019. But: Requires an atomic compare and swap on a value that\u2019s larger than the size of a pointer. Not available on all hardware & bit nesses. Option 4: Use hazard pointers. Maged Michael and Andrei Alexandrescu have covered this in detail. But: It\u2019s very intricate(\u590d\u6742\u7684). Tread(\u8e29\u8e0f) with caution. ticki Fearless concurrency with hazard pointers","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#aba#problem","text":"\u5728\u9605\u8bfb preshing An Introduction to Lock-Free Programming \u65f6\uff0c\u5176\u4e2d\u4ecb\u7ecd\u4e86ABA problem\u3002","title":"ABA problem"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#_1","text":"1\u3001ABA\u4e5f\u662f\u7531\u4e8erace\u800c\u5f15\u53d1\u7684 2\u3001ABA\u548ctime of check to time of use\u662f\u975e\u5e38\u7c7b\u4f3c\u7684","title":"\u603b\u7ed3"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#wikipedia#aba#problem","text":"In multithreaded computing , the ABA problem occurs during synchronization, when a location is read twice, has the same value for both reads, and \"value is the same\" is used to indicate \"nothing has changed\". However, another thread can execute between the two reads and change the value, do other work, then change the value back, thus fooling the first thread into thinking \"nothing has changed\" even though the second thread did work that violates that assumption. In multithreaded computing , the ABA problem occurs during synchronization, when a location is read twice, has the same value for both reads, and \"value is the same\" is used to indicate \"nothing has changed\". However, another thread can execute between the two reads and change the value, do other work, then change the value back, thus fooling the first thread into thinking \"nothing has changed\" even though the second thread did work that violates that assumption. The ABA problem occurs when multiple threads (or processes ) accessing shared data interleave. Below is the sequence of events that will result in the ABA problem: Process {P_{1}} {P_{1}} reads value A from shared memory, {P_{1}} {P_{1}} is preempted , allowing process {P_{2}} {P_{2}} to run, {P_{2}} {P_{2}} modifies the shared memory value A to value B and back to A before preemption, {P_{1}} {P_{1}} begins execution again, sees that the shared memory value has not changed and continues. Although {P_{1}} {P_{1}} can continue executing, it is possible that the behavior will not be correct due to the \"hidden\" modification in shared memory. A common case of the ABA problem is encountered when implementing a lock-free data structure. If an item is removed from the list, deleted, and then a new item is allocated and added to the list, it is common for the allocated object to be at the same location as the deleted object due to MRU memory allocation. A pointer to the new item is thus often equal to a pointer to the old item, causing an ABA problem. NOTE: MRU \u6240\u8868\u793a\u7684\u662fMost recently used","title":"Wikipedia ABA problem"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#examples","text":"Consider a software example of ABA using a lock-free stack : #include <iostream> #include <utility> #include <vector> #include <atomic> using namespace std ; class Obj { public : Obj * next { nullptr }; }; /* Naive lock-free stack which suffers from ABA problem.*/ class Stack { std :: atomic < Obj *> top_ptr ; // // Pops the top object and returns a pointer to it. // Obj * Pop () { while ( 1 ) { Obj * ret_ptr = top_ptr ; if ( ! ret_ptr ) return nullptr ; // For simplicity, suppose that we can ensure that this dereference is safe // (i.e., that no other thread has popped the stack in the meantime). Obj * next_ptr = ret_ptr -> next ; // If the top node is still ret, then assume no one has changed the stack. // (That statement is not always true because of the ABA problem) // Atomically replace top with next. if ( top_ptr . compare_exchange_weak ( ret_ptr , next_ptr )) { return ret_ptr ; } // The stack has changed, start over. } } // // Pushes the object specified by obj_ptr to stack. // void Push ( Obj * obj_ptr ) { while ( 1 ) { Obj * next_ptr = top_ptr ; obj_ptr -> next = next_ptr ; // If the top node is still next, then assume no one has changed the stack. // (That statement is not always true because of the ABA problem) // Atomically replace top with obj. if ( top_ptr . compare_exchange_weak ( next_ptr , obj_ptr )) { return ; } // The stack has changed, start over. } } }; int main () { } // g++ -std=c++11 -Wall -pedantic -pthread main.cpp && ./a.out","title":"Examples"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#workarounds","text":"","title":"Workarounds"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#tagged#state#reference","text":"","title":"Tagged state reference"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#intermediate#nodes","text":"","title":"Intermediate nodes"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#deferred#reclamation","text":"","title":"Deferred reclamation"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#workarounds_1","text":"","title":"Workarounds\u603b\u7ed3"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#zhuanlan#bat","text":"NOTE: 1\u3001\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898 \u5bf9\u4e8eABA\u95ee\u9898\uff0c\u6bd4\u8f83\u6709\u6548\u7684\u65b9\u6848\u662f\u5f15\u5165\u7248\u672c\u53f7\uff0c\u5185\u5b58\u4e2d\u7684\u503c\u6bcf\u53d1\u751f\u4e00\u6b21\u53d8\u5316\uff0c\u7248\u672c\u53f7\u90fd+1\uff1b\u5728\u8fdb\u884cCAS\u64cd\u4f5c\u65f6\uff0c\u4e0d\u4ec5\u6bd4\u8f83\u5185\u5b58\u4e2d\u7684\u503c\uff0c\u4e5f\u4f1a\u6bd4\u8f83\u7248\u672c\u53f7\uff0c\u53ea\u6709\u5f53\u4e8c\u8005\u90fd\u6ca1\u6709\u53d8\u5316\u65f6\uff0cCAS\u624d\u80fd\u6267\u884c\u6210\u529f\u3002Java\u4e2d\u7684 AtomicStampedReference \u7c7b\u4fbf\u662f\u4f7f\u7528\u7248\u672c\u53f7\u6765\u89e3\u51b3ABA\u95ee\u9898\u7684\u3002","title":"zhuanlan \u3010BAT\u9762\u8bd5\u9898\u7cfb\u5217\u3011\u9762\u8bd5\u5b98\uff1a\u4f60\u4e86\u89e3\u4e50\u89c2\u9501\u548c\u60b2\u89c2\u9501\u5417\uff1f"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#herb#sutter#lock-free#programming#or#how#to#juggle#razor#blades","text":"\u5728 Herb Sutter Lock-Free Programming or, How to Juggle Razor Blades \u4e2d\uff0c\u7ed9\u51fa\u4e86\u603b\u7ed3\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6536\u5f55\u4e8e\u5de5\u7a0bprogramming-language\u7684 Lock-Free-Programming-or-How-to-Juggle-Razor-Blades \u7ae0\u8282\u3002 ABA Solutions (sketch) We need to solve the ABA issue: Two nodes with the same address, but different identities (existing at different times). Option 1: Use lazy garbage collection. Solves the problem. Memory can\u2019t be reused while pointers to it exist. But: Not an option (yet) in portable C++ code, and destruction of nodes becomes nondeterministic. Option 2: Use reference counting (garbage collection). Solves the problem in cases without cycles. Again, avoids memory reuse. Option 3: Make each pointer unique by appending a serial number, and increment the serial number each time it\u2019s set. This way we can always distinguish between A and A\u2019. But: Requires an atomic compare and swap on a value that\u2019s larger than the size of a pointer. Not available on all hardware & bit nesses. Option 4: Use hazard pointers. Maged Michael and Andrei Alexandrescu have covered this in detail. But: It\u2019s very intricate(\u590d\u6742\u7684). Tread(\u8e29\u8e0f) with caution.","title":"Herb Sutter Lock-Free Programming or, How to Juggle Razor Blades"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/ABA-problem/#ticki#fearless#concurrency#with#hazard#pointers","text":"","title":"ticki Fearless concurrency with hazard pointers"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/CAS-loop-and-spin/","text":"CAS loop and spin \u611f\u89c9 CAS loop \u5c31\u662f spin\u3002 stackoverflow Is CAS a loop like spin?","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/CAS-loop-and-spin/#cas#loop#and#spin","text":"\u611f\u89c9 CAS loop \u5c31\u662f spin\u3002","title":"CAS loop and spin"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/CAS-loop-and-spin/#stackoverflow#is#cas#a#loop#like#spin","text":"","title":"stackoverflow Is CAS a loop like spin?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/Intel-x86-cmpxchg/","text":"x86 cmpxchg https://stackoverflow.com/questions/6935442/x86-spinlock-using-cmpxchg https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html https://www.felixcloutier.com/x86/cmpxchg","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Compare-and-swap/Intel-x86-cmpxchg/#x86#cmpxchg","text":"https://stackoverflow.com/questions/6935442/x86-spinlock-using-cmpxchg https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html https://www.felixcloutier.com/x86/cmpxchg","title":"x86 cmpxchg"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/","text":"Fetch-and-add wikipedia Fetch-and-add In computer science , the fetch-and-add CPU instruction (FAA) atomically increments the contents of a memory location by a specified value. That is, fetch-and-add performs the operation increment the value at address x by a , where x is a memory location and a is some value, and return the original value at x in such a way that if this operation is executed by one process in a concurrent system, no other process will ever see an intermediate result. Fetch-and-add can be used to implement concurrency control structures such as mutex locks and semaphores . Overview The motivation for having an atomic fetch-and-add is that operations that appear in programming languages as x = x + a are not safe in a concurrent system, where multiple processes or threads are running concurrently (either in a multi-processor system, or preemptively scheduled onto some single-core systems). The reason is that such an operation is actually implemented as multiple machine instructions: Fetch the value at the location x , say x_{old} x_{old} , into a register; add a to x_{old} x_{old} in the register; store the new value of the register back into x . When one process is doing x = x + a and another is doing x = x + b concurrently, there is a race condition . They might both fetch x_{old} x_{old} and operate on that, then both store their results with the effect that one overwrites the other and the stored value becomes either x_{old} x_{old} + a or x_{old} x_{old} + b , not x_{old} x_{old} + a + b as might be expected. In uniprocessor systems with no kernel preemption supported, it is sufficient to disable interrupts before accessing a critical section . However, in multiprocessor systems (even with interrupts disabled) two or more processors could be attempting to access the same memory at the same time. The fetch-and-add instruction allows any processor to atomically increment a value in memory, preventing such multiple processor collisions. Maurice Herlihy (1991) proved that fetch-and-add has a finite consensus number, in contrast to the compare-and-swap operation. The fetch-and-add operation can solve the wait-free consensus problem for no more than two concurrent processes.[ 1] Implementation The fetch-and-add instruction behaves like the following function. Crucially, the entire function is executed atomically : no process can interrupt the function mid-execution and hence see a state that only exists during the execution of the function. This code only serves to help explain the behaviour of fetch-and-add; atomicity requires explicit hardware support and hence can not be implemented as a simple high level function. << atomic >> function FetchAndAdd(address location, int inc) { int value := *location *location := value + inc return value } To implement a mutual exclusion lock, we define the operation FetchAndIncrement , which is equivalent to FetchAndAdd with inc=1. With this operation, a mutual exclusion lock can be implemented using the ticket lock algorithm as: record locktype { int ticketnumber int turn } procedure LockInit ( locktype * lock ) { lock . ticketnumber : = 0 lock . turn : = 0 } procedure Lock ( locktype * lock ) { int myturn : = FetchAndIncrement ( & lock . ticketnumber ) //must be atomic, since many threads might ask for a lock at the same time while lock . turn \u2260 myturn skip // spin until lock is acquired } procedure UnLock ( locktype * lock ) { FetchAndIncrement ( & lock . turn ) //this need not be atomic, since only the possessor of the lock will execute this } These routines provide a mutual-exclusion lock when following conditions are met: Locktype data structure is initialized with function LockInit before use Number of tasks waiting for the lock does not exceed INT_MAX at any time Integer datatype used in lock values can 'wrap around' when continuously incremented Hardware and software support An atomic fetch_add function appears in the C++11 standard.[ 2] It is available as a proprietary extension to C in the Itanium ABI specification,[ 3] and (with the same syntax) in GCC .[ 4] x86 implementation In the x86 architecture, the instruction ADD with the destination operand specifying a memory location is a fetch-and-add instruction that has been there since the 8086 (it just wasn't called that then), and with the LOCK prefix, is atomic across multiple processors. However, it could not return the original value of the memory location (though it returned some flags) until the 486 introduced the XADD instruction. The following is a C implementation for the GCC compiler, for both 32 and 64 bit x86 Intel platforms, based on extended asm syntax: static inline int fetch_and_add ( int * variable , int value ) { __asm__ volatile ( \"lock; xaddl %0, %1\" : \"+r\" ( value ), \"+m\" ( * variable ) // input+output : // No input-only : \"memory\" ); return value ; } History fetch-and-add was introduced by the Ultracomputer project, which also produced a multiprocessor supporting fetch-and-add and containing custom vlsi switches that were able to combine concurrent memory references (including fetch-and-adds) to prevent them from serializing at the memory module containing the destination operand. See also Test-and-set Test and Test-and-set Compare-and-swap Load-Link/Store-Conditional","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#fetch-and-add","text":"","title":"Fetch-and-add"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#wikipedia#fetch-and-add","text":"In computer science , the fetch-and-add CPU instruction (FAA) atomically increments the contents of a memory location by a specified value. That is, fetch-and-add performs the operation increment the value at address x by a , where x is a memory location and a is some value, and return the original value at x in such a way that if this operation is executed by one process in a concurrent system, no other process will ever see an intermediate result. Fetch-and-add can be used to implement concurrency control structures such as mutex locks and semaphores .","title":"wikipedia Fetch-and-add"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#overview","text":"The motivation for having an atomic fetch-and-add is that operations that appear in programming languages as x = x + a are not safe in a concurrent system, where multiple processes or threads are running concurrently (either in a multi-processor system, or preemptively scheduled onto some single-core systems). The reason is that such an operation is actually implemented as multiple machine instructions: Fetch the value at the location x , say x_{old} x_{old} , into a register; add a to x_{old} x_{old} in the register; store the new value of the register back into x . When one process is doing x = x + a and another is doing x = x + b concurrently, there is a race condition . They might both fetch x_{old} x_{old} and operate on that, then both store their results with the effect that one overwrites the other and the stored value becomes either x_{old} x_{old} + a or x_{old} x_{old} + b , not x_{old} x_{old} + a + b as might be expected. In uniprocessor systems with no kernel preemption supported, it is sufficient to disable interrupts before accessing a critical section . However, in multiprocessor systems (even with interrupts disabled) two or more processors could be attempting to access the same memory at the same time. The fetch-and-add instruction allows any processor to atomically increment a value in memory, preventing such multiple processor collisions. Maurice Herlihy (1991) proved that fetch-and-add has a finite consensus number, in contrast to the compare-and-swap operation. The fetch-and-add operation can solve the wait-free consensus problem for no more than two concurrent processes.[ 1]","title":"Overview"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#implementation","text":"The fetch-and-add instruction behaves like the following function. Crucially, the entire function is executed atomically : no process can interrupt the function mid-execution and hence see a state that only exists during the execution of the function. This code only serves to help explain the behaviour of fetch-and-add; atomicity requires explicit hardware support and hence can not be implemented as a simple high level function. << atomic >> function FetchAndAdd(address location, int inc) { int value := *location *location := value + inc return value } To implement a mutual exclusion lock, we define the operation FetchAndIncrement , which is equivalent to FetchAndAdd with inc=1. With this operation, a mutual exclusion lock can be implemented using the ticket lock algorithm as: record locktype { int ticketnumber int turn } procedure LockInit ( locktype * lock ) { lock . ticketnumber : = 0 lock . turn : = 0 } procedure Lock ( locktype * lock ) { int myturn : = FetchAndIncrement ( & lock . ticketnumber ) //must be atomic, since many threads might ask for a lock at the same time while lock . turn \u2260 myturn skip // spin until lock is acquired } procedure UnLock ( locktype * lock ) { FetchAndIncrement ( & lock . turn ) //this need not be atomic, since only the possessor of the lock will execute this } These routines provide a mutual-exclusion lock when following conditions are met: Locktype data structure is initialized with function LockInit before use Number of tasks waiting for the lock does not exceed INT_MAX at any time Integer datatype used in lock values can 'wrap around' when continuously incremented","title":"Implementation"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#hardware#and#software#support","text":"An atomic fetch_add function appears in the C++11 standard.[ 2] It is available as a proprietary extension to C in the Itanium ABI specification,[ 3] and (with the same syntax) in GCC .[ 4]","title":"Hardware and software support"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#x86#implementation","text":"In the x86 architecture, the instruction ADD with the destination operand specifying a memory location is a fetch-and-add instruction that has been there since the 8086 (it just wasn't called that then), and with the LOCK prefix, is atomic across multiple processors. However, it could not return the original value of the memory location (though it returned some flags) until the 486 introduced the XADD instruction. The following is a C implementation for the GCC compiler, for both 32 and 64 bit x86 Intel platforms, based on extended asm syntax: static inline int fetch_and_add ( int * variable , int value ) { __asm__ volatile ( \"lock; xaddl %0, %1\" : \"+r\" ( value ), \"+m\" ( * variable ) // input+output : // No input-only : \"memory\" ); return value ; }","title":"x86 implementation"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#history","text":"fetch-and-add was introduced by the Ultracomputer project, which also produced a multiprocessor supporting fetch-and-add and containing custom vlsi switches that were able to combine concurrent memory references (including fetch-and-adds) to prevent them from serializing at the memory module containing the destination operand.","title":"History"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Fetch-and-add/#see#also","text":"Test-and-set Test and Test-and-set Compare-and-swap Load-Link/Store-Conditional","title":"See also"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-set/","text":"Test-and-set wikipedia Test-and-set In computer science, the test-and-set instruction is an instruction used to write 1 (set) to a memory location and return its old value as a single atomic (i.e., non-interruptible) operation. If multiple processes may access the same memory location, and if a process is currently performing a test-and-set, no other process may begin another test-and-set until the first process's test-and-set is finished. A CPU may use a test-and-set instruction offered by another electronic component, such as dual-port RAM; a CPU itself may also offer a test-and-set instruction. NOTE: 1\u3001shared data\u4f1a\u4e0d\u65ad\u5730\u88ab\u5199\u5165\uff0c\u4f1a\u5bfc\u81f4cache coherence flood\u3001high interconnect contention function Lock ( boolean * lock ) { while ( test_and_set ( lock ) == 1 ); } The calling process obtains the lock if the old value was 0, otherwise the while-loop spins waiting to acquire the lock. This is called a spinlock . \" Test and Test-and-set \" is another example. Mutual exclusion using test-and-set Pseudo-C implementation volatile int lock = 0 ; void critical () { while ( test_and_set ( & lock ) == 1 ); critical section // only one process can be in this section at a time lock = 0 // release lock when finished with the critical section } Performance evaluation of test-and-set locks Example std::atomic_flag","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-set/#test-and-set","text":"","title":"Test-and-set"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-set/#wikipedia#test-and-set","text":"In computer science, the test-and-set instruction is an instruction used to write 1 (set) to a memory location and return its old value as a single atomic (i.e., non-interruptible) operation. If multiple processes may access the same memory location, and if a process is currently performing a test-and-set, no other process may begin another test-and-set until the first process's test-and-set is finished. A CPU may use a test-and-set instruction offered by another electronic component, such as dual-port RAM; a CPU itself may also offer a test-and-set instruction. NOTE: 1\u3001shared data\u4f1a\u4e0d\u65ad\u5730\u88ab\u5199\u5165\uff0c\u4f1a\u5bfc\u81f4cache coherence flood\u3001high interconnect contention function Lock ( boolean * lock ) { while ( test_and_set ( lock ) == 1 ); } The calling process obtains the lock if the old value was 0, otherwise the while-loop spins waiting to acquire the lock. This is called a spinlock . \" Test and Test-and-set \" is another example.","title":"wikipedia Test-and-set"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-set/#mutual#exclusion#using#test-and-set","text":"","title":"Mutual exclusion using test-and-set"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-set/#pseudo-c#implementation","text":"volatile int lock = 0 ; void critical () { while ( test_and_set ( & lock ) == 1 ); critical section // only one process can be in this section at a time lock = 0 // release lock when finished with the critical section }","title":"Pseudo-C implementation"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-set/#performance#evaluation#of#test-and-set#locks","text":"","title":"Performance evaluation of test-and-set locks"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-set/#example","text":"std::atomic_flag","title":"Example"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-test-and-set/","text":"Test and test-and-set wikipedia Test and test-and-set","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-test-and-set/#test#and#test-and-set","text":"","title":"Test and test-and-set"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Atomic-instruction/Read%E2%80%93modify%E2%80%93write/Test-and-test-and-set/#wikipedia#test#and#test-and-set","text":"","title":"wikipedia Test and test-and-set"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/","text":"Memory access unit and atomic \u5728 ../../Memory-alignment \u7ae0\u8282\u4e2d\uff0c\u5df2\u7ecf\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u8282\u5bf9\u8fd9\u4e2atopic\u8fdb\u884c\u603b\u7ed3\u3002 Unit and atomic Memory access unit\u662fWord\uff0c\u56e0\u6b64\uff0c\u4e00\u6b21memory write\u3001\u4e00\u6b21memory read\u662fatomic\u7684\u3002 \u4e00\u4e2aoperation\uff0c\u5982\u679c\u5b83\u53ea\u9700\u8981\u6267\u884c\u4e00\u6b21memory write\u3001\u4e00\u6b21memory read\uff0c\u90a3\u4e48\u5b83\u5c31\u662fatomic\u7684\u3002\u4e00\u822c\uff0c\u6761\u4ef6\u5982\u4e0b 1\u3001aligned 2\u3001length <= length of Word NOTE: \u4e0a\u8ff0\u4e24\u4e2a\u6761\u4ef6\u90fd\u662f\u4e3a\u4e86\u4fdd\u8bc1\u53ea\u8fdb\u884c\u4e00\u6b21memory access\u3002 \u53e6\u5916\u8fd8\u9700\u8981\u7ed3\u5408\u5982\u4e0b\u77e5\u8bc6\u6765\u8fdb\u884c\u7406\u89e3: 1\u3001Memory processor transfer\u673a\u5236 2\u3001alignment \u5bf9\u4e8e length < length of Word \u7684\uff0c\u5178\u578b\u7684\u662fbool\uff0c\u5728 stackoverflow is assignment operator '=' atomic? \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 Atomic operation and thread safe \u5728multiple-thread\u60c5\u51b5\u4e0b\uff0catomic operation\u4e0d\u4e00\u5b9a\u80fd\u591f\u4fdd\u8bc1thread safe\uff0c\u53cd\u4f8b\u5305\u62ec: 1\u3001stackoverflow is assignment operator '=' atomic? \u5728\u4e0b\u9762\u7684\"Assignment(=): bool\"\u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba \u4e0b\u9762\u7ed3\u5408\u51e0\u79cd\u5177\u4f53\u7684\u60c5\u51b5\u6765\u8fdb\u884c\u5206\u6790: stackoverflow Purpose of memory alignment # A Atomicity The CPU can operate on an aligned word of memory atomically , meaning that no other instruction can interrupt that operation. This is critical to the correct operation of many lock-free data structures and other concurrency paradigms. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u53ef\u4ee5\u8fd9\u6837\u7406\u89e3: 1\u3001\"unit and atomic\": Memory processor transfer\u7684unit\u662fWord\uff0c\u56e0\u6b64\"The CPU can operate on an aligned word of memory atomically\" Assignment(=): pointer Write to single aligned pointers are atomic on modern CPUs \u8fd9\u662f\u6211\u5728\u9605\u8bfb wikipedia Read-copy-update \u65f6\uff0c\u5176\u4e2d\u63d0\u51fa\u7684: In contrast, RCU-based updaters typically take advantage of the fact that writes to single aligned pointers are atomic on modern CPUs, allowing atomic insertion, removal, and replacement of data in a linked structure without disrupting(\u6253\u6270) readers. \u5176\u5b9e\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u5f15\u8d77\u4e86\u6211\u601d\u8003\u8fd9\u6837\u7684\u95ee\u9898: length of pointer and length of Word\uff1f \u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u770b\u51fa\uff0clength of pointer == length of Word\uff0c\u90a3\u5b9e\u9645\u7684\u60c5\u51b5\u662f\u8fd9\u6837\u7684\u5417\uff1f stackoverflow Is pointer assignment atomic in C++? A C++03 does not know about the existance of threads, therefore the concept of atomicity doesn't make much sense for C++03, meaning that it doesn't say anything about that. C++11 does know about threads, but once again doesn't say anything about the atomicity of assigning pointers. However C++11 does contain std::atomic<T*> , which is guaranteed to be atomic. Note that even if writing to a raw pointer is atomic on your platform the compiler is still free to move that assingment around, so that doesn't really buy you anything. If you need to write to a pointer which is shared between threads use either std::atomic<T*> (or the not yet official boost::atomic<T*> , gccs atomic intrinsics or windows Interlocked*) or wrap all accesses to that pointer in mutexes. Assignment(=): int \u5728 preshing Atomic vs. Non-Atomic Operations \u4e2d\u4e5f\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002 TODO stackoverflow Why is integer assignment on a naturally aligned variable atomic on x86? stackoverflow Are C++ Reads and Writes of an int Atomic? A Boy, what a question. The answer to which is: Yes, no, hmmm, well, it depends It all comes down to the architecture of the system. On an IA32 a correctly aligned address will be an atomic operation. Unaligned writes might be atomic, it depends on the caching system in use. If the memory lies within a single L1 cache line then it is atomic, otherwise it's not. The width of the bus between the CPU and RAM can affect the atomic nature: a correctly aligned 16bit write on an 8086 was atomic whereas the same write on an 8088 wasn't because the 8088 only had an 8 bit bus whereas the 8086 had a 16 bit bus. Also, if you're using C/C++ don't forget to mark the shared value as volatile, otherwise the optimiser will think the variable is never updated in one of your threads. Why is integer assignment on a naturally aligned variable atomic on x86? \"Natural\" alignment means aligned to it's own type width . Thus, the load/store will never be split across any kind of boundary wider than itself (e.g. page, cache-line, or an even narrower chunk size used for data transfers between different caches). CPUs often do things like cache-access, or cache-line transfers between cores, in power-of-2 sized chunks, so alignment boundaries smaller than a cache line do matter. (See @BeeOnRope 's comments below). See also Atomicity on x86 for more details on how CPUs implement atomic loads or stores internally, and Can num++ be atomic for 'int num'? for more about how atomic RMW operations like atomic<int>::fetch_add() / lock xadd are implemented internally. Assignment(=): bool \u53c2\u89c1 ./Byte-granular-memory-access \u7ae0\u8282\u3002","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#memory#access#unit#and#atomic","text":"\u5728 ../../Memory-alignment \u7ae0\u8282\u4e2d\uff0c\u5df2\u7ecf\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u8282\u5bf9\u8fd9\u4e2atopic\u8fdb\u884c\u603b\u7ed3\u3002","title":"Memory access unit and atomic"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#unit#and#atomic","text":"Memory access unit\u662fWord\uff0c\u56e0\u6b64\uff0c\u4e00\u6b21memory write\u3001\u4e00\u6b21memory read\u662fatomic\u7684\u3002 \u4e00\u4e2aoperation\uff0c\u5982\u679c\u5b83\u53ea\u9700\u8981\u6267\u884c\u4e00\u6b21memory write\u3001\u4e00\u6b21memory read\uff0c\u90a3\u4e48\u5b83\u5c31\u662fatomic\u7684\u3002\u4e00\u822c\uff0c\u6761\u4ef6\u5982\u4e0b 1\u3001aligned 2\u3001length <= length of Word NOTE: \u4e0a\u8ff0\u4e24\u4e2a\u6761\u4ef6\u90fd\u662f\u4e3a\u4e86\u4fdd\u8bc1\u53ea\u8fdb\u884c\u4e00\u6b21memory access\u3002 \u53e6\u5916\u8fd8\u9700\u8981\u7ed3\u5408\u5982\u4e0b\u77e5\u8bc6\u6765\u8fdb\u884c\u7406\u89e3: 1\u3001Memory processor transfer\u673a\u5236 2\u3001alignment \u5bf9\u4e8e length < length of Word \u7684\uff0c\u5178\u578b\u7684\u662fbool\uff0c\u5728 stackoverflow is assignment operator '=' atomic? \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Unit and atomic"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#atomic#operation#and#thread#safe","text":"\u5728multiple-thread\u60c5\u51b5\u4e0b\uff0catomic operation\u4e0d\u4e00\u5b9a\u80fd\u591f\u4fdd\u8bc1thread safe\uff0c\u53cd\u4f8b\u5305\u62ec: 1\u3001stackoverflow is assignment operator '=' atomic? \u5728\u4e0b\u9762\u7684\"Assignment(=): bool\"\u7ae0\u8282\u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba \u4e0b\u9762\u7ed3\u5408\u51e0\u79cd\u5177\u4f53\u7684\u60c5\u51b5\u6765\u8fdb\u884c\u5206\u6790:","title":"Atomic operation and thread safe"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#stackoverflow#purpose#of#memory#alignment#a","text":"","title":"stackoverflow Purpose of memory alignment # A"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#atomicity","text":"The CPU can operate on an aligned word of memory atomically , meaning that no other instruction can interrupt that operation. This is critical to the correct operation of many lock-free data structures and other concurrency paradigms. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u53ef\u4ee5\u8fd9\u6837\u7406\u89e3: 1\u3001\"unit and atomic\": Memory processor transfer\u7684unit\u662fWord\uff0c\u56e0\u6b64\"The CPU can operate on an aligned word of memory atomically\"","title":"Atomicity"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#assignment#pointer","text":"","title":"Assignment(=): pointer"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#write#to#single#aligned#pointers#are#atomic#on#modern#cpus","text":"\u8fd9\u662f\u6211\u5728\u9605\u8bfb wikipedia Read-copy-update \u65f6\uff0c\u5176\u4e2d\u63d0\u51fa\u7684: In contrast, RCU-based updaters typically take advantage of the fact that writes to single aligned pointers are atomic on modern CPUs, allowing atomic insertion, removal, and replacement of data in a linked structure without disrupting(\u6253\u6270) readers. \u5176\u5b9e\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\uff0c\u5f15\u8d77\u4e86\u6211\u601d\u8003\u8fd9\u6837\u7684\u95ee\u9898: length of pointer and length of Word\uff1f \u4ece\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u770b\u51fa\uff0clength of pointer == length of Word\uff0c\u90a3\u5b9e\u9645\u7684\u60c5\u51b5\u662f\u8fd9\u6837\u7684\u5417\uff1f","title":"Write to single aligned pointers are atomic on modern CPUs"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#stackoverflow#is#pointer#assignment#atomic#in#c","text":"A C++03 does not know about the existance of threads, therefore the concept of atomicity doesn't make much sense for C++03, meaning that it doesn't say anything about that. C++11 does know about threads, but once again doesn't say anything about the atomicity of assigning pointers. However C++11 does contain std::atomic<T*> , which is guaranteed to be atomic. Note that even if writing to a raw pointer is atomic on your platform the compiler is still free to move that assingment around, so that doesn't really buy you anything. If you need to write to a pointer which is shared between threads use either std::atomic<T*> (or the not yet official boost::atomic<T*> , gccs atomic intrinsics or windows Interlocked*) or wrap all accesses to that pointer in mutexes.","title":"stackoverflow Is pointer assignment atomic in C++?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#assignment#int","text":"\u5728 preshing Atomic vs. Non-Atomic Operations \u4e2d\u4e5f\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002","title":"Assignment(=): int"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#todo#stackoverflow#why#is#integer#assignment#on#a#naturally#aligned#variable#atomic#on#x86","text":"","title":"TODO stackoverflow Why is integer assignment on a naturally aligned variable atomic on x86?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#stackoverflow#are#c#reads#and#writes#of#an#int#atomic","text":"A Boy, what a question. The answer to which is: Yes, no, hmmm, well, it depends It all comes down to the architecture of the system. On an IA32 a correctly aligned address will be an atomic operation. Unaligned writes might be atomic, it depends on the caching system in use. If the memory lies within a single L1 cache line then it is atomic, otherwise it's not. The width of the bus between the CPU and RAM can affect the atomic nature: a correctly aligned 16bit write on an 8086 was atomic whereas the same write on an 8088 wasn't because the 8088 only had an 8 bit bus whereas the 8086 had a 16 bit bus. Also, if you're using C/C++ don't forget to mark the shared value as volatile, otherwise the optimiser will think the variable is never updated in one of your threads.","title":"stackoverflow Are C++ Reads and Writes of an int Atomic?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#why#is#integer#assignment#on#a#naturally#aligned#variable#atomic#on#x86","text":"\"Natural\" alignment means aligned to it's own type width . Thus, the load/store will never be split across any kind of boundary wider than itself (e.g. page, cache-line, or an even narrower chunk size used for data transfers between different caches). CPUs often do things like cache-access, or cache-line transfers between cores, in power-of-2 sized chunks, so alignment boundaries smaller than a cache line do matter. (See @BeeOnRope 's comments below). See also Atomicity on x86 for more details on how CPUs implement atomic loads or stores internally, and Can num++ be atomic for 'int num'? for more about how atomic RMW operations like atomic<int>::fetch_add() / lock xadd are implemented internally.","title":"Why is integer assignment on a naturally aligned variable atomic on x86?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/#assignment#bool","text":"\u53c2\u89c1 ./Byte-granular-memory-access \u7ae0\u8282\u3002","title":"Assignment(=): bool"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/Byte-granular-memory-access/","text":"Byte-granular-memory-access \u672c\u6587\u8ba8\u8bba\u5b57\u8282\u7c92\u5ea6\u7684memory access\u3002 Byte-granular update atomic? \u8bbf\u95ee\u5355\u5b57\u8282\u662f\u5426\u662fatomic\u7684\uff1f stackoverflow is assignment operator '=' atomic? I'm implementing Inter-Thread Communication using global variable. //global var volatile bool is_true = true ; //thread 1 void thread_1 () { while ( 1 ){ int rint = rand () % 10 ; if ( is_true ) { cout << \"thread_1: \" << rint << endl ; //thread_1 prints some stuff if ( rint == 3 ) is_true = false ; //here, tells thread_2 to start printing stuff } } } //thread 2 void thread_2 () { while ( 1 ){ int rint = rand () % 10 ; if ( ! is_true ) { //if is_true == false cout << \"thread_1: \" << rint << endl ; //thread_2 prints some stuff if ( rint == 7 ) //7 is_true = true ; //here, tells thread_1 to start printing stuff } } } int main () { HANDLE t1 = CreateThread ( 0 , 0 , thread_1 , 0 , 0 , 0 ); HANDLE t2 = CreateThread ( 0 , 0 , thread_2 , 0 , 0 , 0 ); Sleep ( 9999999 ); return 0 ; } Question In the code above, I use a global var volatile bool is_true to switch printing between thread_1 and thread_2. I wonder whether it is thread-safe to use assignment operation here ? A This code is not guaranteed to be thread-safe on Win32, since Win32 guarantees atomicity only for properly-aligned 4-byte and pointer-sized values. bool is not guaranteed to be one of those types. (It is typically a 1-byte type.) NOTE: memory access bool \u662f\u80fd\u591f\u4fdd\u8bc1atomicity\uff0c\u4f46\u662f\u5e76\u4e0d\u80fd\u4fdd\u8bc1thread safe\uff0c\u5728\u524d\u9762\u7684\"Atomic operation and thread safe\"\u4e2d\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u8fdb\u884c\u4e86\u8ba8\u8bba For those who demand an actual example of how this could fail: Suppose that bool is a 1-byte type. Suppose also that your is_true variable happens to be stored adjacent to another bool variable (let's call it other_bool ), so that both of them share the same 4-byte line. For concreteness, let's say that is_true is at address 0x1000 and other_bool is at address 0x1001 . Suppose that both values are initially false , and one thread decides to update is_true at the same time another thread tries to update other_bool . The following sequence of operations can occur: NOTE: is_true \u3001 other_bool \u662fadjacent\u7684\uff0c\u56e0\u6b64CPU\u4e00\u6b21read\uff0c\u80fd\u591f\u5c06\u4e24\u8005\u540c\u65f6\u8bfb\u5230register\u4e2d\u3002 1\u3001Thread 1 prepares to set is_true to true by loading the 4-byte value containing is_true and other_bool . Thread 1 reads 0x00000000 . 2\u3001Thread 2 prepares to set other_bool to true by loading the 4-byte value containing is_true and other_bool . Thread 2 reads 0x00000000 . 3\u3001Thread 1 updates the byte in the 4-byte value corresponding to is_true , producing 0x00000001 . 4\u3001Thread 2 updates the byte in the 4-byte value corresponding to other_bool , producing 0x00000100 . 5\u3001Thread 1 stores the updated value to memory. is_true is now true and other_bool is now false . 6\u3001Thread 2 stores the updated value to memory. is_true is now false and other_bool is now true . Observe that at the end this sequence, the update to is_true was lost, because it was overwritten by thread 2, which captured an old value of is_true . It so happens that x86 is very forgiving of this type of error because it supports byte-granular updates and has a very tight memory model. Other Win32 processors are not as forgiving. RISC chips, for example, often do not support byte-granular updates, and even if they do, they usually have very weak memory models. stroustrup C++11FAQ Memory model NOTE: \u5176\u5b9e\u5217\u4e3e\u4e86\u548c stackoverflow is assignment operator '=' atomic? \u4e2d\u7c7b\u4f3c\u7684\u4f8b\u5b50\u3002\u5728\u5176\u4e2d\u7ed9\u51fa\u4e86C++\u4e2d\u7684\u89c4\u5b9a So, C++11 guarantees that no such problems occur for \" separate memory locations \". More precisely: A memory location cannot be safely accessed by two threads without some form of locking unless they are both read accesses . stackoverflow Can modern x86 hardware not store a single byte to memory? A NOTE: \u8fd9\u4e2a\u56de\u7b54\u975e\u5e38\u8be6\u7ec6 TL:DR: On every modern ISA that has byte-store instructions (including x86), they're atomic and don't disturb surrounding bytes. (I'm not aware of any older ISAs where byte-store instructions could \"invent writes\" to neighbouring bytes either.) NOTE: \u4e0a\u8ff0\u662f\u7ed3\u8bba A Not only are x86 CPUs capable of reading and writing a single byte, all modern general purpose CPUs are capable of it. More importantly most modern CPUs (including x86, ARM, MIPS, PowerPC, and SPARC) are capable of atomically reading and writing single bytes.","title":"Introduction"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/Byte-granular-memory-access/#byte-granular-memory-access","text":"\u672c\u6587\u8ba8\u8bba\u5b57\u8282\u7c92\u5ea6\u7684memory access\u3002","title":"Byte-granular-memory-access"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/Byte-granular-memory-access/#byte-granular#update#atomic","text":"\u8bbf\u95ee\u5355\u5b57\u8282\u662f\u5426\u662fatomic\u7684\uff1f","title":"Byte-granular update atomic?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/Byte-granular-memory-access/#stackoverflow#is#assignment#operator#atomic","text":"I'm implementing Inter-Thread Communication using global variable. //global var volatile bool is_true = true ; //thread 1 void thread_1 () { while ( 1 ){ int rint = rand () % 10 ; if ( is_true ) { cout << \"thread_1: \" << rint << endl ; //thread_1 prints some stuff if ( rint == 3 ) is_true = false ; //here, tells thread_2 to start printing stuff } } } //thread 2 void thread_2 () { while ( 1 ){ int rint = rand () % 10 ; if ( ! is_true ) { //if is_true == false cout << \"thread_1: \" << rint << endl ; //thread_2 prints some stuff if ( rint == 7 ) //7 is_true = true ; //here, tells thread_1 to start printing stuff } } } int main () { HANDLE t1 = CreateThread ( 0 , 0 , thread_1 , 0 , 0 , 0 ); HANDLE t2 = CreateThread ( 0 , 0 , thread_2 , 0 , 0 , 0 ); Sleep ( 9999999 ); return 0 ; } Question In the code above, I use a global var volatile bool is_true to switch printing between thread_1 and thread_2. I wonder whether it is thread-safe to use assignment operation here ? A This code is not guaranteed to be thread-safe on Win32, since Win32 guarantees atomicity only for properly-aligned 4-byte and pointer-sized values. bool is not guaranteed to be one of those types. (It is typically a 1-byte type.) NOTE: memory access bool \u662f\u80fd\u591f\u4fdd\u8bc1atomicity\uff0c\u4f46\u662f\u5e76\u4e0d\u80fd\u4fdd\u8bc1thread safe\uff0c\u5728\u524d\u9762\u7684\"Atomic operation and thread safe\"\u4e2d\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u8fdb\u884c\u4e86\u8ba8\u8bba For those who demand an actual example of how this could fail: Suppose that bool is a 1-byte type. Suppose also that your is_true variable happens to be stored adjacent to another bool variable (let's call it other_bool ), so that both of them share the same 4-byte line. For concreteness, let's say that is_true is at address 0x1000 and other_bool is at address 0x1001 . Suppose that both values are initially false , and one thread decides to update is_true at the same time another thread tries to update other_bool . The following sequence of operations can occur: NOTE: is_true \u3001 other_bool \u662fadjacent\u7684\uff0c\u56e0\u6b64CPU\u4e00\u6b21read\uff0c\u80fd\u591f\u5c06\u4e24\u8005\u540c\u65f6\u8bfb\u5230register\u4e2d\u3002 1\u3001Thread 1 prepares to set is_true to true by loading the 4-byte value containing is_true and other_bool . Thread 1 reads 0x00000000 . 2\u3001Thread 2 prepares to set other_bool to true by loading the 4-byte value containing is_true and other_bool . Thread 2 reads 0x00000000 . 3\u3001Thread 1 updates the byte in the 4-byte value corresponding to is_true , producing 0x00000001 . 4\u3001Thread 2 updates the byte in the 4-byte value corresponding to other_bool , producing 0x00000100 . 5\u3001Thread 1 stores the updated value to memory. is_true is now true and other_bool is now false . 6\u3001Thread 2 stores the updated value to memory. is_true is now false and other_bool is now true . Observe that at the end this sequence, the update to is_true was lost, because it was overwritten by thread 2, which captured an old value of is_true . It so happens that x86 is very forgiving of this type of error because it supports byte-granular updates and has a very tight memory model. Other Win32 processors are not as forgiving. RISC chips, for example, often do not support byte-granular updates, and even if they do, they usually have very weak memory models.","title":"stackoverflow is assignment operator '=' atomic?"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/Byte-granular-memory-access/#stroustrup#c11faq#memory#model","text":"NOTE: \u5176\u5b9e\u5217\u4e3e\u4e86\u548c stackoverflow is assignment operator '=' atomic? \u4e2d\u7c7b\u4f3c\u7684\u4f8b\u5b50\u3002\u5728\u5176\u4e2d\u7ed9\u51fa\u4e86C++\u4e2d\u7684\u89c4\u5b9a So, C++11 guarantees that no such problems occur for \" separate memory locations \". More precisely: A memory location cannot be safely accessed by two threads without some form of locking unless they are both read accesses .","title":"stroustrup C++11FAQ Memory model"},{"location":"CPU-memory-access/Memory-access-instruction/Atomic/Memory-access-unit-and-atomic/Byte-granular-memory-access/#stackoverflow#can#modern#x86#hardware#not#store#a#single#byte#to#memory","text":"A NOTE: \u8fd9\u4e2a\u56de\u7b54\u975e\u5e38\u8be6\u7ec6 TL:DR: On every modern ISA that has byte-store instructions (including x86), they're atomic and don't disturb surrounding bytes. (I'm not aware of any older ISAs where byte-store instructions could \"invent writes\" to neighbouring bytes either.) NOTE: \u4e0a\u8ff0\u662f\u7ed3\u8bba A Not only are x86 CPUs capable of reading and writing a single byte, all modern general purpose CPUs are capable of it. More importantly most modern CPUs (including x86, ARM, MIPS, PowerPC, and SPARC) are capable of atomically reading and writing single bytes.","title":"stackoverflow Can modern x86 hardware not store a single byte to memory?"},{"location":"CPU-memory-access/Memory-alignment/","text":"\u5173\u4e8e\u672c\u7ae0 \u5728\u4e0a\u4e00\u8282\u4e2d\u7ed9\u51fa\u4e86Memory\u2013processor transfer\u7684\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\uff0c\u672c\u8282\u63d0\u4f9bmemory alignment\u7684\u89d2\u5ea6\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\uff0c\u901a\u8fc7\u8fd9\u4e9b\u5206\u6790\uff0c\u8bfb\u8005\u5e94\u8be5\u80fd\u591f\u5bf9\u6b64\u6709\u66f4\u52a0\u6df1\u523b\u7684\u8ba4\u77e5\u3002 cse.bgu Memory Alignment When a computer reads from or writes to a memory address, it will do this in word sized chunks (for example, 4 byte (32-bit) chunks on the MPC8360). Data alignment means putting the data at a memory offset equal to some multiple of the word size , which increases the system's performance due to the way the CPU handles memory. Most CPUs can access only memory aligned addresses . Table 1 show an example of some memory addresses and their alignment on different architectures. Memory Alignment in the MPC8360 The MPC8360 read/write operations are in 4 byte (32-bit) chunks. Thus, only memory addresses that are some multiple of four are considered aligned. The MPC8360 can access only aligned addresses. Reading a word (32 bit) from an aligned address in the MPC8360 (for example 0x0000_0008) can be achieved in a single load instruction. However, reading a word (32 bit) from an unaligned address (for example 0x0000_0009) will take at least two read instructions. Formal Definitions A memory address a , is said to be n-byte aligned when n is a power of two and a is a multiple of n bytes. In this context a byte is the smallest unit of memory access, i.e. each memory address specifies a different byte. An n-byte aligned address would have log2 n log2 n least-significant zeros when expressed in binary. A memory access is said to be aligned when the data being accessed is n bytes long and the address is n-byte aligned. When a memory access is not aligned, it is said to be misaligned or Non Aligned. Note that by definition byte memory accesses are always aligned.","title":"Introduction"},{"location":"CPU-memory-access/Memory-alignment/#_1","text":"\u5728\u4e0a\u4e00\u8282\u4e2d\u7ed9\u51fa\u4e86Memory\u2013processor transfer\u7684\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\uff0c\u672c\u8282\u63d0\u4f9bmemory alignment\u7684\u89d2\u5ea6\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\uff0c\u901a\u8fc7\u8fd9\u4e9b\u5206\u6790\uff0c\u8bfb\u8005\u5e94\u8be5\u80fd\u591f\u5bf9\u6b64\u6709\u66f4\u52a0\u6df1\u523b\u7684\u8ba4\u77e5\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"CPU-memory-access/Memory-alignment/#csebgu#memory#alignment","text":"When a computer reads from or writes to a memory address, it will do this in word sized chunks (for example, 4 byte (32-bit) chunks on the MPC8360). Data alignment means putting the data at a memory offset equal to some multiple of the word size , which increases the system's performance due to the way the CPU handles memory. Most CPUs can access only memory aligned addresses . Table 1 show an example of some memory addresses and their alignment on different architectures.","title":"cse.bgu Memory Alignment"},{"location":"CPU-memory-access/Memory-alignment/#memory#alignment#in#the#mpc8360","text":"The MPC8360 read/write operations are in 4 byte (32-bit) chunks. Thus, only memory addresses that are some multiple of four are considered aligned. The MPC8360 can access only aligned addresses. Reading a word (32 bit) from an aligned address in the MPC8360 (for example 0x0000_0008) can be achieved in a single load instruction. However, reading a word (32 bit) from an unaligned address (for example 0x0000_0009) will take at least two read instructions.","title":"Memory Alignment in the MPC8360"},{"location":"CPU-memory-access/Memory-alignment/#formal#definitions","text":"A memory address a , is said to be n-byte aligned when n is a power of two and a is a multiple of n bytes. In this context a byte is the smallest unit of memory access, i.e. each memory address specifies a different byte. An n-byte aligned address would have log2 n log2 n least-significant zeros when expressed in binary. A memory access is said to be aligned when the data being accessed is n bytes long and the address is n-byte aligned. When a memory access is not aligned, it is said to be misaligned or Non Aligned. Note that by definition byte memory accesses are always aligned.","title":"Formal Definitions"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/","text":"Purpose of memory alignment stackoverflow Purpose of memory alignment A \u8fd9\u4e2a\u56de\u7b54\u603b\u7ed3\u5730\u6bd4\u8f83\u597d The memory subsystem on a modern processor is restricted to accessing memory at the granularity and alignment of its word size; this is the case for a number of reasons. Speed Modern processors have multiple levels of cache memory that data must be pulled through; supporting single-byte reads would make the memory subsystem throughput tightly bound to the execution unit throughput (aka cpu-bound); this is all reminiscent of how PIO mode was surpassed by DMA for many of the same reasons in hard drives. The CPU always reads at its word size (4 bytes on a 32-bit processor), so when you do an unaligned address access \u2014 on a processor that supports it \u2014 the processor is going to read multiple words. The CPU will read each word of memory that your requested address straddles(\u8de8\u5f0f\u7ec4\u5408). This causes an amplification(\u653e\u5927) of up to 2X the number of memory transactions required to access the requested data. Because of this, it can very easily be slower to read two bytes than four. For example, say you have a struct in memory that looks like this: struct mystruct { char c ; // one byte int i ; // four bytes short s ; // two bytes } On a 32-bit processor it would most likely be aligned like shown here: The processor can read each of these members in one transaction. Say you had a packed version of the struct, maybe from the network where it was packed for transmission efficiency; it might look something like this: Reading the first byte is going to be the same. When you ask the processor to give you 16 bits from 0x0005 it will have to read a word from from 0x0004 and shift left 1 byte to place it in a 16-bit register; some extra work, but most can handle that in one cycle. When you ask for 32 bits from 0x0001 you'll get a 2X amplification. The processor will read from 0x0000 into the result register and shift left 1 byte, then read again from 0x0004 into a temporary register, shift right 3 bytes, then OR it with the result register. Range For any given address space, if the architecture can assume that the 2 LSBs are always 0 (e.g., 32-bit machines) then it can access 4 times more memory (the 2 saved bits can represent 4 distinct states), or the same amount of memory with 2 bits for something like flags. Taking the 2 LSBs off of an address would give you a 4-byte alignment; also referred to as a stride of 4 bytes. Each time an address is incremented it is effectively incrementing bit 2, not bit 0, i.e., the last 2 bits will always continue to be 00 . This can even affect the physical design of the system. If the address bus needs 2 fewer bits, there can be 2 fewer pins on the CPU, and 2 fewer traces on the circuit board. Atomicity The CPU can operate on an aligned word of memory atomically, meaning that no other instruction can interrupt that operation. This is critical to the correct operation of many lock-free data structures and other concurrency paradigms. NOTE: \u5728 ./CPU-memory-access/Atomic/Memory-access-unit-and-atomic \u7ae0\u8282\u4e2d\u4f1a\u5bf9\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8fdb\u884c\u89e3\u91ca\u3002 Conclusion The memory system of a processor is quite a bit more complex and involved than described here; a discussion on how an x86 processor actually addresses memory can help (many processors work similarly). There are many more benefits to adhering to memory alignment that you can read at this IBM article . A computer's primary use is to transform data. Modern memory architectures and technologies have been optimized over decades to facilitate getting more data, in, out, and between more and faster execution units\u2013in a highly reliable way. Bonus: Caches Another alignment-for-performance that I alluded to previously is alignment on cache lines which are (for example, on some CPUs) 64B. For more info on how much performance can be gained by leveraging caches, take a look at Gallery of Processor Cache Effects ; from this question on cache-line sizes Understanding of cache lines can be important for certain types of program optimizations. For example, alignment of data may determine whether an operation touches one or two cache lines. As we saw in the example above, this can easily mean that in the misaligned case, the operation will be twice slower.","title":"Introduction"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#purpose#of#memory#alignment","text":"","title":"Purpose of memory alignment"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#stackoverflow#purpose#of#memory#alignment","text":"","title":"stackoverflow Purpose of memory alignment"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#a","text":"\u8fd9\u4e2a\u56de\u7b54\u603b\u7ed3\u5730\u6bd4\u8f83\u597d The memory subsystem on a modern processor is restricted to accessing memory at the granularity and alignment of its word size; this is the case for a number of reasons.","title":"A"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#speed","text":"Modern processors have multiple levels of cache memory that data must be pulled through; supporting single-byte reads would make the memory subsystem throughput tightly bound to the execution unit throughput (aka cpu-bound); this is all reminiscent of how PIO mode was surpassed by DMA for many of the same reasons in hard drives. The CPU always reads at its word size (4 bytes on a 32-bit processor), so when you do an unaligned address access \u2014 on a processor that supports it \u2014 the processor is going to read multiple words. The CPU will read each word of memory that your requested address straddles(\u8de8\u5f0f\u7ec4\u5408). This causes an amplification(\u653e\u5927) of up to 2X the number of memory transactions required to access the requested data. Because of this, it can very easily be slower to read two bytes than four. For example, say you have a struct in memory that looks like this: struct mystruct { char c ; // one byte int i ; // four bytes short s ; // two bytes } On a 32-bit processor it would most likely be aligned like shown here: The processor can read each of these members in one transaction. Say you had a packed version of the struct, maybe from the network where it was packed for transmission efficiency; it might look something like this: Reading the first byte is going to be the same. When you ask the processor to give you 16 bits from 0x0005 it will have to read a word from from 0x0004 and shift left 1 byte to place it in a 16-bit register; some extra work, but most can handle that in one cycle. When you ask for 32 bits from 0x0001 you'll get a 2X amplification. The processor will read from 0x0000 into the result register and shift left 1 byte, then read again from 0x0004 into a temporary register, shift right 3 bytes, then OR it with the result register.","title":"Speed"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#range","text":"For any given address space, if the architecture can assume that the 2 LSBs are always 0 (e.g., 32-bit machines) then it can access 4 times more memory (the 2 saved bits can represent 4 distinct states), or the same amount of memory with 2 bits for something like flags. Taking the 2 LSBs off of an address would give you a 4-byte alignment; also referred to as a stride of 4 bytes. Each time an address is incremented it is effectively incrementing bit 2, not bit 0, i.e., the last 2 bits will always continue to be 00 . This can even affect the physical design of the system. If the address bus needs 2 fewer bits, there can be 2 fewer pins on the CPU, and 2 fewer traces on the circuit board.","title":"Range"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#atomicity","text":"The CPU can operate on an aligned word of memory atomically, meaning that no other instruction can interrupt that operation. This is critical to the correct operation of many lock-free data structures and other concurrency paradigms. NOTE: \u5728 ./CPU-memory-access/Atomic/Memory-access-unit-and-atomic \u7ae0\u8282\u4e2d\u4f1a\u5bf9\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8fdb\u884c\u89e3\u91ca\u3002","title":"Atomicity"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#conclusion","text":"The memory system of a processor is quite a bit more complex and involved than described here; a discussion on how an x86 processor actually addresses memory can help (many processors work similarly). There are many more benefits to adhering to memory alignment that you can read at this IBM article . A computer's primary use is to transform data. Modern memory architectures and technologies have been optimized over decades to facilitate getting more data, in, out, and between more and faster execution units\u2013in a highly reliable way.","title":"Conclusion"},{"location":"CPU-memory-access/Memory-alignment/Purpose-of-memory-alignment/#bonus#caches","text":"Another alignment-for-performance that I alluded to previously is alignment on cache lines which are (for example, on some CPUs) 64B. For more info on how much performance can be gained by leveraging caches, take a look at Gallery of Processor Cache Effects ; from this question on cache-line sizes Understanding of cache lines can be important for certain types of program optimizations. For example, alignment of data may determine whether an operation touches one or two cache lines. As we saw in the example above, this can easily mean that in the misaligned case, the operation will be twice slower.","title":"Bonus: Caches"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/","text":"developer.ibm Data alignment: Straighten up and fly right NOTE: \u672c\u6587\u7ed9\u51fa\u4e86memory access\u8be6\u7ec6\u7ec6\u8282\u4ee5\u53ca\u76f8\u5173\u95ee\u9898\u3002\u5176\u4e2d\u7684*memory access granularity*\u6982\u5ff5\u5c24\u5176\u91cd\u8981\uff0c\u7ed3\u5408\u524d\u9762\u7ae0\u8282\u7684\u5185\u5bb9\uff0c\u5728\u6b64\u53ef\u4ee5\u9884\u544a\u8bfb\u8005\u76ee\u524d\u5927\u591a\u6570CPU\u7684*memory access granularity*\u53d6 word size \u3002 Memory access granularity Programmers are conditioned to think of memory as a simple array of bytes. Among C and its descendants, char* is ubiquitous(\u666e\u904d\u5b58\u5728\u7684) as meaning \u201ca block of memory\u201d, and even Java\u2122 has its byte[] type to represent raw memory. Figure 1. How programmers see memory However, your computer\u2019s processor does not read from and write to memory in byte-sized chunks. Instead, it accesses memory in two-, four-, eight- 16- or even 32-byte chunks. We\u2019ll call the size in which a processor accesses memory its memory access granularity . NOTE: \u9700\u8981\u7406\u89e3granularity\u7684\u542b\u4e49\uff0c\u5b83\u7684\u4e2d\u6587\u610f\u601d\u662f**\u7c92\u5ea6**\uff0c\u53ef\u4ee5\u628a\u5b83\u770b\u505a\u662f**\u5355\u4f4d**\u7684\u610f\u601d\uff0c\u5b83\u5177\u6709**\u539f\u5b50\u6027**(\u4e0d\u53ef\u5206\u7684)\u3002 memory access granularity**\u662fCPU\u4ecememory\u4e2d\u8bfb\u53d6\u6570\u636e\u7684**\u5355\u4f4d \uff0c\u6240\u4ee5CPU\u65e0\u6cd5\u4ecememory\u4e2d\u8bfb\u53d6\u534a\u4e2a\u5355\u4f4d\u7684\u6570\u636e\uff0c\u53ea\u80fd\u591f\u8bfb\u53d6\u4e00\u4e2a\u5355\u4f4d\u7684\u6570\u636e\u3002\u4e0b\u9762\u7684Figure 2\u5c31\u975e\u5e38\u76f4\u89c2\u5730\u5c55\u793a\u4e86CPU\u89c6\u89d2\u7684memory\u3002\u901a\u8fc7\u524d\u9762\u7684\u8fd9\u4e9b\u5206\u6790\uff0c\u6211\u4eec\u5e94\u8be5\u77e5\u9053\u7684\u662f\uff0cCPU\u53ea\u80fd\u591f\u4ece\u4f4d\u7f6e 0 \u3001 4 \u3001 8 \u7b49\u4f4d\u7f6e\u5f00\u59cb\u8bfb\u53d6\u6570\u636e\uff0c\u8fd9\u4e9b\u4f4d\u7f6e\u6211\u4eec\u901a\u5e38\u5c06\u5b83\u4eec\u79f0\u4e3amemory access boundary\uff08\u5728\u540e\u7eed\u7ae0\u8282\u4f1a\u4ecb\u7ecd\uff09\uff0c\u663e\u7136CPU access boundary\u662f**memory access granularity**\u7684\u6574\u6570\u500d\uff0c\u5982\u679c\u4e00\u4e2a\u6570\u636e\u7684\u5b58\u50a8\u4f4d\u7f6e\u662fmemory access boundary\uff0c\u5219\u79f0\u4e3a**align to memory access granularity**\uff08\u5bf9\u9f50\uff09\uff0c\u4f7f\u7528aligned\u6765\u5f62\u5bb9\u8fd9\u6837\u7684\u5730\u5740\uff0c\u5426\u5219\u5c31\u662funaligned\u3002\u901a\u8fc7\u540e\u9762\u7684\u7ae0\u8282\uff0c\u6211\u4eec\u4f1a\u770b\u5230\uff0caligned address\u76f8\u6bd4\u4e8eunaligned address\u6709\u7740\u8bf8\u591a\u4f18\u52bf\u3002 Figure 2. How processors see memory If you don\u2019t understand and address alignment issues in your software, the following scenarios, in increasing order of severity, are all possible: Your software will run slower. Your application will lock up. Your operating system will crash. Your software will silently fail, yielding incorrect results. Alignment fundamentals To illustrate the principles behind alignment, examine a constant task, and how it\u2019s affected by a processor\u2019s memory access granularity . The task is simple: first read four bytes from address 0 into the processor\u2019s register. Then read four bytes from address 1 into the same register. Single-byte memory access granularity First examine what would happen on a processor with a one-byte memory access granularity: Figure 3. Single-byte memory access granularity This fits in with the naive programmer\u2019s model of how memory works: it takes the same four memory accesses to read from address 0 as it does from address 1. Now see what would happen on a processor with two-byte granularity, like the original 68000: Double-byte memory access granularity Figure 4. Double-byte memory access granularity When reading from address 0, a processor with two-byte granularity takes half the number of memory accesses as a processor with one-byte granularity. Because each memory access entails a fixed amount overhead, minimizing the number of accesses can really help performance. NOTE: \u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u662fa processor with two-byte granularity\u8bfb\u53d64\u4e2a\u5b57\u8282\u82b1\u8d39\u7684\u65f6\u95f4\u6bd4a processor with one-byte granularity\u82b1\u8d39\u7684\u65f6\u95f4\u8981\u5c11\u4e00\u534a\uff1b However, notice what happens when reading from address 1. Because the address doesn\u2019t fall evenly on the processor\u2019s memory access boundary , the processor has extra work to do. Such an address is known as an unaligned address . Because address 1 is unaligned , a processor with two-byte granularity must perform an extra memory access, slowing down the operation Finally, examine what would happen on a processor with four-byte memory access granularity, like the 68030 or PowerPC\u00ae 601: Quad-byte memory access granularity Figure 5. Quad-byte memory access granularity A processor with four-byte granularity can slurp(\u5927\u53e3\u5403) up four bytes from an aligned address with one read. Also note that reading from an unaligned address doubles the access count. Now that you understand the fundamentals behind aligned data access , you can explore some of the issues related to alignment. Lazy processors A processor has to perform some tricks when instructed to access an unaligned address . Going back to the example of reading four bytes from address 1 on a processor with four-byte granularity , you can work out exactly what needs to be done: Figure 6. How processors handle unaligned memory access The processor needs to read the first chunk of the unaligned address and shift out the \u201cunwanted\u201d bytes from the first chunk. Then it needs to read the second chunk of the unaligned address and shift out some of its information. Finally, the two are merged together for placement in the register. It\u2019s a lot of work. Some processors just aren\u2019t willing to do all of that work for you. The original 68000 was a processor with two-byte granularity and lacked the circuitry to cope with unaligned addresses . When presented with such an address, the processor would throw an exception. The original Mac OS didn\u2019t take very kindly to this exception, and would usually demand the user restart the machine. Ouch. Later processors in the 680\u00d70 series, such as the 68020, lifted this restriction and performed the necessary work for you. This explains why some old software that works on the 68020 crashes on the 68000. It also explains why, way back when, some old Mac coders initialized pointers with odd addresses. On the original Mac, if the pointer was accessed without being reassigned to a valid address, the Mac would immediately drop into the debugger. Often they could then examine the calling chain stack and figure out where the mistake was. All processors have a finite number of transistors to get work done. Adding unaligned address access support cuts into this \u201ctransistor budget.\u201d These transistors could otherwise be used to make other portions of the processor work faster, or add new functionality altogether. An example of a processor that sacrifices unaligned address access support in the name of speed is MIPS. MIPS is a great example of a processor that does away with almost all frivolity in the name of getting real work done faster. The PowerPC takes a hybrid approach. Every PowerPC processor to date has hardware support for unaligned 32-bit integer access. While you still pay a performance penalty for unaligned access, it tends to be small. On the other hand, modern PowerPC processors lack hardware support for unaligned 64-bit floating-point access. When asked to load an unaligned floating-point number from memory, modern PowerPC processors will throw an exception and have the operating system perform the alignment chores in software . Performing alignment in software is much slower than performing it in hardware. NOTE : \u6709\u7684processor\u538b\u6839\u5c31\u4e0d\u652f\u6301unaligned address access Speed Writing some tests illustrates the performance penalties of unaligned memory access . The test is simple: you read, negate, and write back the numbers in a ten-megabyte buffer. These tests have two variables: 1\u3001 The size, in bytes, in which you process the buffer. First you\u2019ll process the buffer one byte at a time. Then you\u2019ll move onto two-, four- and eight-bytes at a time. 2\u3001 The alignment of the buffer. You\u2019ll stagger the alignment of the buffer by incrementing the pointer to the buffer and running each test again. These tests were performed on a 800 MHz PowerBook G4. To help normalize performance fluctuations from interrupt processing, each test was run ten times, keeping the average of the runs. First up is the test that operates on a single byte at a time: Listing 1. Munging data one byte at a time void Munge8 ( void \u2217 data , uint32_t size ) { uint8_t \u2217 data8 = ( uint8_t \u2217 ) data ; uint8_t \u2217 data8End = data8 + size ; while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } It took an average of 67,364 microseconds to execute this function. Now modify it to work on two bytes at a time instead of one byte at a time \u2014 which will halve the number of memory accesses: Listing 2. Munging data two bytes at a time void Munge16 ( void \u2217 data , uint32_t size ) { uint16_t \u2217 data16 = ( uint16_t \u2217 ) data ; uint16_t \u2217 data16End = data16 + ( size >> 1 ); / \u2217 Divide size by 2. \u2217 / uint8_t \u2217 data8 = ( uint8_t \u2217 ) data16End ; uint8_t \u2217 data8End = data8 + ( size & 0x00000001 ); / \u2217 Strip upper 31 bits . \u2217 / while ( data16 != data16End ) { \u2217 data16 ++ = \u2011\u2217 data16 ; } while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } This function took 48,765 microseconds to process the same ten-megabyte buffer \u2014 38% faster than Munge8 . However, that buffer was aligned. If the buffer is unaligned, the time required increases to 66,385 microseconds \u2014 about a 27% speed penalty. The following chart illustrates the performance pattern of aligned memory accesses versus unaligned accesses: Figure 7. Single-byte access versus double-byte access The first thing you notice is that accessing memory one byte at a time is uniformly slow. The second item of interest is that when accessing memory two bytes at a time, whenever the address is not evenly divisible by two, that 27% speed penalty rears its ugly head. Now up the ante, and process the buffer four bytes at a time: Listing 3. Munging data four bytes at a time void Munge32 ( void \u2217 data , uint32_t size ) { uint32_t \u2217 data32 = ( uint32_t \u2217 ) data ; uint32_t \u2217 data32End = data32 + ( size >> 2 ); / \u2217 Divide size by 4. \u2217 / uint8_t \u2217 data8 = ( uint8_t \u2217 ) data32End ; uint8_t \u2217 data8End = data8 + ( size & 0x00000003 ); / \u2217 Strip upper 30 bits . \u2217 / while ( data32 != data32End ) { \u2217 data32 ++ = \u2011\u2217 data32 ; } while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } This function processes an aligned buffer in 43,043 microseconds and an unaligned buffer in 55,775 microseconds, respectively. Thus, on this test machine, accessing unaligned memory four bytes at a time is slower than accessing aligned memory two bytes at a time: Figure 8. Single- versus double- versus quad-byte access Now for the horror story: processing the buffer eight bytes at a time. Listing 4. Munging data eight bytes at a time void Munge64 ( void \u2217 data , uint32_t size ) { double \u2217 data64 = ( double \u2217 ) data ; double \u2217 data64End = data64 + ( size >> 3 ); / \u2217 Divide size by 8. \u2217 / uint8_t \u2217 data8 = ( uint8_t \u2217 ) data64End ; uint8_t \u2217 data8End = data8 + ( size & 0x00000007 ); / \u2217 Strip upper 29 bits . \u2217 / while ( data64 != data64End ) { \u2217 data64 ++ = \u2011\u2217 data64 ; } while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } Munge64 processes an aligned buffer in 39,085 microseconds \u2014 about 10% faster than processing the buffer four bytes at a time. However, processing an unaligned buffer takes an amazing 1,841,155 microseconds \u2014 two orders of magnitude slower than aligned access, an outstanding 4,610% performance penalty! What happened? Because modern PowerPC processors lack hardware support for unaligned floating-point access, the processor throws an exception for each unaligned access. The operating system catches this exception and performs the alignment in software. Here\u2019s a chart illustrating the penalty, and when it occurs: Figure 9. Multiple-byte access comparison The penalties for one-, two- and four-byte unaligned access are dwarfed by the horrendous unaligned eight-byte penalty. Maybe this chart, removing the top (and thus the tremendous gulf between the two numbers), will be clearer: Figure 10. Multiple-byte access comparison #2 There\u2019s another subtle insight hidden in this data. Compare eight-byte access speeds on four-byte boundaries: Figure 11. Multiple-byte access comparison #3 Notice accessing memory eight bytes at a time on four- and twelve- byte boundaries is slower than reading the same memory four or even two bytes at a time. While PowerPCs have hardware support for four-byte aligned eight-byte doubles, you still pay a performance penalty if you use that support. Granted, it\u2019s no where near the 4,610% penalty, but it\u2019s certainly noticeable. Moral of the story: accessing memory in large chunks can be slower than accessing memory in small chunks, if that access is not aligned. Atomicity All modern processors offer atomic instructions . These special instructions are crucial for synchronizing two or more concurrent tasks. As the name implies, atomic instructions must be indivisible \u2014 that\u2019s why they\u2019re so handy for synchronization: they can\u2019t be preempted. It turns out that in order for atomic instructions to perform correctly, the addresses you pass them must be at least four-byte aligned. This is because of a subtle interaction between atomic instructions and virtual memory . If an address is unaligned, it requires at least two memory accesses. But what happens if the desired data spans two pages of virtual memory ? This could lead to a situation where the first page is resident while the last page is not. Upon access, in the middle of the instruction, a page fault would be generated, executing the virtual memory management swap-in code, destroying the atomicity of the instruction . To keep things simple and correct, both the 68K and PowerPC require that atomically manipulated addresses always be at least four-byte aligned. Unfortunately, the PowerPC does not throw an exception when atomically storing to an unaligned address. Instead, the store simply always fails. This is bad because most atomic functions are written to retry upon a failed store, under the assumption they were preempted. These two circumstances combine to where your program will go into an infinite loop if you attempt to atomically store to an unaligned address. Oops. Altivec Altivec is all about speed. Unaligned memory access slows down the processor and costs precious transistors. Thus, the Altivec engineers took a page from the MIPS playbook and simply don\u2019t support unaligned memory access. Because Altivec works with sixteen-byte chunks at a time, all addresses passed to Altivec must be sixteen-byte aligned. What\u2019s scary is what happens if your address is not aligned. Altivec won\u2019t throw an exception to warn you about the unaligned address. Instead, Altivec simply ignores the lower four bits of the address and charges ahead, operating on the wrong address . This means your program may silently corrupt memory or return incorrect results if you don\u2019t explicitly make sure all your data is aligned. There is an advantage to Altivec\u2019s bit-stripping ways. Because you don\u2019t need to explicitly truncate (align-down) an address, this behavior can save you an instruction or two when handing addresses to the processor. This is not to say Altivec can\u2019t process unaligned memory. You can find detailed instructions how to do so on the Altivec Programming Environments Manual (see resources on the right). It requires more work, but because memory is so slow compared to the processor, the overhead for such shenanigans is surprisingly low. Structure alignment Examine the following structure: Listing 5. An innocent structure void Munge64 ( void \u2217 data , uint32_t size ) { typedef struct { char a ; long b ; char c ; } Struct ; What is the size of this structure in bytes? Many programmers will answer \u201c6 bytes.\u201d It makes sense: one byte for a , four bytes for b and another byte for c . 1 + 4 + 1 equals 6. Here\u2019s how it would lay out in memory: Table 1. Structure size in bytes Field Type Field Name Field Offset Field Size Field End char a 0 1 1 long b 1 4 5 char c 5 1 6 Total size in bytes: 6 However, if you were to ask your compiler to sizeof( Struct ) , chances are the answer you\u2019d get back would be greater than six, perhaps eight or even twenty-four. There\u2019s two reasons for this: backwards compatibility and efficiency. First, backwards compatibility. Remember the 68000 was a processor with two-byte memory access granularity, and would throw an exception upon encountering an odd address. If you were to read from or write to field b , you\u2019d attempt to access an odd address. If a debugger weren\u2019t installed, the old Mac OS would throw up a System Error dialog box with one button: Restart. Yikes! So, instead of laying out your fields just the way you wrote them, the compiler padded the structure so that b and c would reside at even addresses: Table 2. Structure with compiler padding Field Type Field Name Field Offset Field Size Field End char a 0 1 1 padding 1 1 2 long b 2 4 6 char c 6 1 7 padding 7 1 8 Total Size in Bytes: 8 Padding is the act of adding otherwise unused space to a structure to make fields line up in a desired way. Now, when the 68020 came out with built-in hardware support for unaligned memory access, this padding was unnecessary. However, it didn\u2019t hurt anything, and it even helped a little in performance. The second reason is efficiency. Nowadays, on PowerPC machines, two-byte alignment is nice, but four-byte or eight-byte is better. You probably don\u2019t care anymore that the original 68000 choked on unaligned structures, but you probably care about potential 4,610% performance penalties, which can happen if a double field doesn\u2019t sit aligned in a structure of your devising. Conclusion If you don\u2019t understand and explicitly code for data alignment: Your software may hit performance-killing unaligned memory access exceptions, which invoke very expensive alignment exception handlers. Your application may attempt to atomically store to an unaligned address, causing your application to lock up. Your application may attempt to pass an unaligned address to Altivec, resulting in Altivec reading from and/or writing to the wrong part of memory, silently corrupting data or yielding incorrect results. Credits Thanks to Alex Rosenberg and Ian Ollmann for feedback, Matt Slot for his FastTimes timing library, and Duane Hayes for providing a bevy of testing machines.","title":"Introduction"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#developeribm#data#alignment#straighten#up#and#fly#right","text":"NOTE: \u672c\u6587\u7ed9\u51fa\u4e86memory access\u8be6\u7ec6\u7ec6\u8282\u4ee5\u53ca\u76f8\u5173\u95ee\u9898\u3002\u5176\u4e2d\u7684*memory access granularity*\u6982\u5ff5\u5c24\u5176\u91cd\u8981\uff0c\u7ed3\u5408\u524d\u9762\u7ae0\u8282\u7684\u5185\u5bb9\uff0c\u5728\u6b64\u53ef\u4ee5\u9884\u544a\u8bfb\u8005\u76ee\u524d\u5927\u591a\u6570CPU\u7684*memory access granularity*\u53d6 word size \u3002","title":"developer.ibm Data alignment: Straighten up and fly right"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#memory#access#granularity","text":"Programmers are conditioned to think of memory as a simple array of bytes. Among C and its descendants, char* is ubiquitous(\u666e\u904d\u5b58\u5728\u7684) as meaning \u201ca block of memory\u201d, and even Java\u2122 has its byte[] type to represent raw memory. Figure 1. How programmers see memory However, your computer\u2019s processor does not read from and write to memory in byte-sized chunks. Instead, it accesses memory in two-, four-, eight- 16- or even 32-byte chunks. We\u2019ll call the size in which a processor accesses memory its memory access granularity . NOTE: \u9700\u8981\u7406\u89e3granularity\u7684\u542b\u4e49\uff0c\u5b83\u7684\u4e2d\u6587\u610f\u601d\u662f**\u7c92\u5ea6**\uff0c\u53ef\u4ee5\u628a\u5b83\u770b\u505a\u662f**\u5355\u4f4d**\u7684\u610f\u601d\uff0c\u5b83\u5177\u6709**\u539f\u5b50\u6027**(\u4e0d\u53ef\u5206\u7684)\u3002 memory access granularity**\u662fCPU\u4ecememory\u4e2d\u8bfb\u53d6\u6570\u636e\u7684**\u5355\u4f4d \uff0c\u6240\u4ee5CPU\u65e0\u6cd5\u4ecememory\u4e2d\u8bfb\u53d6\u534a\u4e2a\u5355\u4f4d\u7684\u6570\u636e\uff0c\u53ea\u80fd\u591f\u8bfb\u53d6\u4e00\u4e2a\u5355\u4f4d\u7684\u6570\u636e\u3002\u4e0b\u9762\u7684Figure 2\u5c31\u975e\u5e38\u76f4\u89c2\u5730\u5c55\u793a\u4e86CPU\u89c6\u89d2\u7684memory\u3002\u901a\u8fc7\u524d\u9762\u7684\u8fd9\u4e9b\u5206\u6790\uff0c\u6211\u4eec\u5e94\u8be5\u77e5\u9053\u7684\u662f\uff0cCPU\u53ea\u80fd\u591f\u4ece\u4f4d\u7f6e 0 \u3001 4 \u3001 8 \u7b49\u4f4d\u7f6e\u5f00\u59cb\u8bfb\u53d6\u6570\u636e\uff0c\u8fd9\u4e9b\u4f4d\u7f6e\u6211\u4eec\u901a\u5e38\u5c06\u5b83\u4eec\u79f0\u4e3amemory access boundary\uff08\u5728\u540e\u7eed\u7ae0\u8282\u4f1a\u4ecb\u7ecd\uff09\uff0c\u663e\u7136CPU access boundary\u662f**memory access granularity**\u7684\u6574\u6570\u500d\uff0c\u5982\u679c\u4e00\u4e2a\u6570\u636e\u7684\u5b58\u50a8\u4f4d\u7f6e\u662fmemory access boundary\uff0c\u5219\u79f0\u4e3a**align to memory access granularity**\uff08\u5bf9\u9f50\uff09\uff0c\u4f7f\u7528aligned\u6765\u5f62\u5bb9\u8fd9\u6837\u7684\u5730\u5740\uff0c\u5426\u5219\u5c31\u662funaligned\u3002\u901a\u8fc7\u540e\u9762\u7684\u7ae0\u8282\uff0c\u6211\u4eec\u4f1a\u770b\u5230\uff0caligned address\u76f8\u6bd4\u4e8eunaligned address\u6709\u7740\u8bf8\u591a\u4f18\u52bf\u3002 Figure 2. How processors see memory If you don\u2019t understand and address alignment issues in your software, the following scenarios, in increasing order of severity, are all possible: Your software will run slower. Your application will lock up. Your operating system will crash. Your software will silently fail, yielding incorrect results.","title":"Memory access granularity"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#alignment#fundamentals","text":"To illustrate the principles behind alignment, examine a constant task, and how it\u2019s affected by a processor\u2019s memory access granularity . The task is simple: first read four bytes from address 0 into the processor\u2019s register. Then read four bytes from address 1 into the same register.","title":"Alignment fundamentals"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#single-byte#memory#access#granularity","text":"First examine what would happen on a processor with a one-byte memory access granularity: Figure 3. Single-byte memory access granularity This fits in with the naive programmer\u2019s model of how memory works: it takes the same four memory accesses to read from address 0 as it does from address 1. Now see what would happen on a processor with two-byte granularity, like the original 68000:","title":"Single-byte memory access granularity"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#double-byte#memory#access#granularity","text":"Figure 4. Double-byte memory access granularity When reading from address 0, a processor with two-byte granularity takes half the number of memory accesses as a processor with one-byte granularity. Because each memory access entails a fixed amount overhead, minimizing the number of accesses can really help performance. NOTE: \u8fd9\u53e5\u8bdd\u7684\u610f\u601d\u662fa processor with two-byte granularity\u8bfb\u53d64\u4e2a\u5b57\u8282\u82b1\u8d39\u7684\u65f6\u95f4\u6bd4a processor with one-byte granularity\u82b1\u8d39\u7684\u65f6\u95f4\u8981\u5c11\u4e00\u534a\uff1b However, notice what happens when reading from address 1. Because the address doesn\u2019t fall evenly on the processor\u2019s memory access boundary , the processor has extra work to do. Such an address is known as an unaligned address . Because address 1 is unaligned , a processor with two-byte granularity must perform an extra memory access, slowing down the operation Finally, examine what would happen on a processor with four-byte memory access granularity, like the 68030 or PowerPC\u00ae 601:","title":"Double-byte memory access granularity"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#quad-byte#memory#access#granularity","text":"Figure 5. Quad-byte memory access granularity A processor with four-byte granularity can slurp(\u5927\u53e3\u5403) up four bytes from an aligned address with one read. Also note that reading from an unaligned address doubles the access count. Now that you understand the fundamentals behind aligned data access , you can explore some of the issues related to alignment.","title":"Quad-byte memory access granularity"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#lazy#processors","text":"A processor has to perform some tricks when instructed to access an unaligned address . Going back to the example of reading four bytes from address 1 on a processor with four-byte granularity , you can work out exactly what needs to be done: Figure 6. How processors handle unaligned memory access The processor needs to read the first chunk of the unaligned address and shift out the \u201cunwanted\u201d bytes from the first chunk. Then it needs to read the second chunk of the unaligned address and shift out some of its information. Finally, the two are merged together for placement in the register. It\u2019s a lot of work. Some processors just aren\u2019t willing to do all of that work for you. The original 68000 was a processor with two-byte granularity and lacked the circuitry to cope with unaligned addresses . When presented with such an address, the processor would throw an exception. The original Mac OS didn\u2019t take very kindly to this exception, and would usually demand the user restart the machine. Ouch. Later processors in the 680\u00d70 series, such as the 68020, lifted this restriction and performed the necessary work for you. This explains why some old software that works on the 68020 crashes on the 68000. It also explains why, way back when, some old Mac coders initialized pointers with odd addresses. On the original Mac, if the pointer was accessed without being reassigned to a valid address, the Mac would immediately drop into the debugger. Often they could then examine the calling chain stack and figure out where the mistake was. All processors have a finite number of transistors to get work done. Adding unaligned address access support cuts into this \u201ctransistor budget.\u201d These transistors could otherwise be used to make other portions of the processor work faster, or add new functionality altogether. An example of a processor that sacrifices unaligned address access support in the name of speed is MIPS. MIPS is a great example of a processor that does away with almost all frivolity in the name of getting real work done faster. The PowerPC takes a hybrid approach. Every PowerPC processor to date has hardware support for unaligned 32-bit integer access. While you still pay a performance penalty for unaligned access, it tends to be small. On the other hand, modern PowerPC processors lack hardware support for unaligned 64-bit floating-point access. When asked to load an unaligned floating-point number from memory, modern PowerPC processors will throw an exception and have the operating system perform the alignment chores in software . Performing alignment in software is much slower than performing it in hardware. NOTE : \u6709\u7684processor\u538b\u6839\u5c31\u4e0d\u652f\u6301unaligned address access","title":"Lazy processors"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#speed","text":"Writing some tests illustrates the performance penalties of unaligned memory access . The test is simple: you read, negate, and write back the numbers in a ten-megabyte buffer. These tests have two variables: 1\u3001 The size, in bytes, in which you process the buffer. First you\u2019ll process the buffer one byte at a time. Then you\u2019ll move onto two-, four- and eight-bytes at a time. 2\u3001 The alignment of the buffer. You\u2019ll stagger the alignment of the buffer by incrementing the pointer to the buffer and running each test again. These tests were performed on a 800 MHz PowerBook G4. To help normalize performance fluctuations from interrupt processing, each test was run ten times, keeping the average of the runs. First up is the test that operates on a single byte at a time: Listing 1. Munging data one byte at a time void Munge8 ( void \u2217 data , uint32_t size ) { uint8_t \u2217 data8 = ( uint8_t \u2217 ) data ; uint8_t \u2217 data8End = data8 + size ; while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } It took an average of 67,364 microseconds to execute this function. Now modify it to work on two bytes at a time instead of one byte at a time \u2014 which will halve the number of memory accesses: Listing 2. Munging data two bytes at a time void Munge16 ( void \u2217 data , uint32_t size ) { uint16_t \u2217 data16 = ( uint16_t \u2217 ) data ; uint16_t \u2217 data16End = data16 + ( size >> 1 ); / \u2217 Divide size by 2. \u2217 / uint8_t \u2217 data8 = ( uint8_t \u2217 ) data16End ; uint8_t \u2217 data8End = data8 + ( size & 0x00000001 ); / \u2217 Strip upper 31 bits . \u2217 / while ( data16 != data16End ) { \u2217 data16 ++ = \u2011\u2217 data16 ; } while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } This function took 48,765 microseconds to process the same ten-megabyte buffer \u2014 38% faster than Munge8 . However, that buffer was aligned. If the buffer is unaligned, the time required increases to 66,385 microseconds \u2014 about a 27% speed penalty. The following chart illustrates the performance pattern of aligned memory accesses versus unaligned accesses: Figure 7. Single-byte access versus double-byte access The first thing you notice is that accessing memory one byte at a time is uniformly slow. The second item of interest is that when accessing memory two bytes at a time, whenever the address is not evenly divisible by two, that 27% speed penalty rears its ugly head. Now up the ante, and process the buffer four bytes at a time: Listing 3. Munging data four bytes at a time void Munge32 ( void \u2217 data , uint32_t size ) { uint32_t \u2217 data32 = ( uint32_t \u2217 ) data ; uint32_t \u2217 data32End = data32 + ( size >> 2 ); / \u2217 Divide size by 4. \u2217 / uint8_t \u2217 data8 = ( uint8_t \u2217 ) data32End ; uint8_t \u2217 data8End = data8 + ( size & 0x00000003 ); / \u2217 Strip upper 30 bits . \u2217 / while ( data32 != data32End ) { \u2217 data32 ++ = \u2011\u2217 data32 ; } while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } This function processes an aligned buffer in 43,043 microseconds and an unaligned buffer in 55,775 microseconds, respectively. Thus, on this test machine, accessing unaligned memory four bytes at a time is slower than accessing aligned memory two bytes at a time: Figure 8. Single- versus double- versus quad-byte access Now for the horror story: processing the buffer eight bytes at a time. Listing 4. Munging data eight bytes at a time void Munge64 ( void \u2217 data , uint32_t size ) { double \u2217 data64 = ( double \u2217 ) data ; double \u2217 data64End = data64 + ( size >> 3 ); / \u2217 Divide size by 8. \u2217 / uint8_t \u2217 data8 = ( uint8_t \u2217 ) data64End ; uint8_t \u2217 data8End = data8 + ( size & 0x00000007 ); / \u2217 Strip upper 29 bits . \u2217 / while ( data64 != data64End ) { \u2217 data64 ++ = \u2011\u2217 data64 ; } while ( data8 != data8End ) { \u2217 data8 ++ = \u2011\u2217 data8 ; } } Munge64 processes an aligned buffer in 39,085 microseconds \u2014 about 10% faster than processing the buffer four bytes at a time. However, processing an unaligned buffer takes an amazing 1,841,155 microseconds \u2014 two orders of magnitude slower than aligned access, an outstanding 4,610% performance penalty! What happened? Because modern PowerPC processors lack hardware support for unaligned floating-point access, the processor throws an exception for each unaligned access. The operating system catches this exception and performs the alignment in software. Here\u2019s a chart illustrating the penalty, and when it occurs: Figure 9. Multiple-byte access comparison The penalties for one-, two- and four-byte unaligned access are dwarfed by the horrendous unaligned eight-byte penalty. Maybe this chart, removing the top (and thus the tremendous gulf between the two numbers), will be clearer: Figure 10. Multiple-byte access comparison #2 There\u2019s another subtle insight hidden in this data. Compare eight-byte access speeds on four-byte boundaries: Figure 11. Multiple-byte access comparison #3 Notice accessing memory eight bytes at a time on four- and twelve- byte boundaries is slower than reading the same memory four or even two bytes at a time. While PowerPCs have hardware support for four-byte aligned eight-byte doubles, you still pay a performance penalty if you use that support. Granted, it\u2019s no where near the 4,610% penalty, but it\u2019s certainly noticeable. Moral of the story: accessing memory in large chunks can be slower than accessing memory in small chunks, if that access is not aligned.","title":"Speed"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#atomicity","text":"All modern processors offer atomic instructions . These special instructions are crucial for synchronizing two or more concurrent tasks. As the name implies, atomic instructions must be indivisible \u2014 that\u2019s why they\u2019re so handy for synchronization: they can\u2019t be preempted. It turns out that in order for atomic instructions to perform correctly, the addresses you pass them must be at least four-byte aligned. This is because of a subtle interaction between atomic instructions and virtual memory . If an address is unaligned, it requires at least two memory accesses. But what happens if the desired data spans two pages of virtual memory ? This could lead to a situation where the first page is resident while the last page is not. Upon access, in the middle of the instruction, a page fault would be generated, executing the virtual memory management swap-in code, destroying the atomicity of the instruction . To keep things simple and correct, both the 68K and PowerPC require that atomically manipulated addresses always be at least four-byte aligned. Unfortunately, the PowerPC does not throw an exception when atomically storing to an unaligned address. Instead, the store simply always fails. This is bad because most atomic functions are written to retry upon a failed store, under the assumption they were preempted. These two circumstances combine to where your program will go into an infinite loop if you attempt to atomically store to an unaligned address. Oops.","title":"Atomicity"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#altivec","text":"Altivec is all about speed. Unaligned memory access slows down the processor and costs precious transistors. Thus, the Altivec engineers took a page from the MIPS playbook and simply don\u2019t support unaligned memory access. Because Altivec works with sixteen-byte chunks at a time, all addresses passed to Altivec must be sixteen-byte aligned. What\u2019s scary is what happens if your address is not aligned. Altivec won\u2019t throw an exception to warn you about the unaligned address. Instead, Altivec simply ignores the lower four bits of the address and charges ahead, operating on the wrong address . This means your program may silently corrupt memory or return incorrect results if you don\u2019t explicitly make sure all your data is aligned. There is an advantage to Altivec\u2019s bit-stripping ways. Because you don\u2019t need to explicitly truncate (align-down) an address, this behavior can save you an instruction or two when handing addresses to the processor. This is not to say Altivec can\u2019t process unaligned memory. You can find detailed instructions how to do so on the Altivec Programming Environments Manual (see resources on the right). It requires more work, but because memory is so slow compared to the processor, the overhead for such shenanigans is surprisingly low.","title":"Altivec"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#structure#alignment","text":"Examine the following structure: Listing 5. An innocent structure void Munge64 ( void \u2217 data , uint32_t size ) { typedef struct { char a ; long b ; char c ; } Struct ; What is the size of this structure in bytes? Many programmers will answer \u201c6 bytes.\u201d It makes sense: one byte for a , four bytes for b and another byte for c . 1 + 4 + 1 equals 6. Here\u2019s how it would lay out in memory: Table 1. Structure size in bytes Field Type Field Name Field Offset Field Size Field End char a 0 1 1 long b 1 4 5 char c 5 1 6 Total size in bytes: 6 However, if you were to ask your compiler to sizeof( Struct ) , chances are the answer you\u2019d get back would be greater than six, perhaps eight or even twenty-four. There\u2019s two reasons for this: backwards compatibility and efficiency. First, backwards compatibility. Remember the 68000 was a processor with two-byte memory access granularity, and would throw an exception upon encountering an odd address. If you were to read from or write to field b , you\u2019d attempt to access an odd address. If a debugger weren\u2019t installed, the old Mac OS would throw up a System Error dialog box with one button: Restart. Yikes! So, instead of laying out your fields just the way you wrote them, the compiler padded the structure so that b and c would reside at even addresses: Table 2. Structure with compiler padding Field Type Field Name Field Offset Field Size Field End char a 0 1 1 padding 1 1 2 long b 2 4 6 char c 6 1 7 padding 7 1 8 Total Size in Bytes: 8 Padding is the act of adding otherwise unused space to a structure to make fields line up in a desired way. Now, when the 68020 came out with built-in hardware support for unaligned memory access, this padding was unnecessary. However, it didn\u2019t hurt anything, and it even helped a little in performance. The second reason is efficiency. Nowadays, on PowerPC machines, two-byte alignment is nice, but four-byte or eight-byte is better. You probably don\u2019t care anymore that the original 68000 choked on unaligned structures, but you probably care about potential 4,610% performance penalties, which can happen if a double field doesn\u2019t sit aligned in a structure of your devising.","title":"Structure alignment"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#conclusion","text":"If you don\u2019t understand and explicitly code for data alignment: Your software may hit performance-killing unaligned memory access exceptions, which invoke very expensive alignment exception handlers. Your application may attempt to atomically store to an unaligned address, causing your application to lock up. Your application may attempt to pass an unaligned address to Altivec, resulting in Altivec reading from and/or writing to the wrong part of memory, silently corrupting data or yielding incorrect results.","title":"Conclusion"},{"location":"CPU-memory-access/Memory-alignment/developer.ibm-Data-alignment-Straighten-up-and-fly-right/#credits","text":"Thanks to Alex Rosenberg and Ian Ollmann for feedback, Matt Slot for his FastTimes timing library, and Duane Hayes for providing a bevy of testing machines.","title":"Credits"},{"location":"CPU-memory-access/Memory-disambiguation/","text":"Memory disambiguation \"disambiguation\"\u7684\u542b\u4e49\u662f\"\u6d88\u9664\u6b67\u4e49\"\u3002 Memory disambiguation\u4e5f\u662f\u7531out-of-order-execution\u5f15\u53d1\u7684\u3002 wikipedia Memory disambiguation Memory disambiguation is a set of techniques employed by high-performance out-of-order execution microprocessors that execute memory access instructions (loads and stores) out of program order. The mechanisms for performing memory disambiguation, implemented using digital logic inside the microprocessor core, detect true dependencies between memory operations at execution time and allow the processor to recover when a dependence has been violated. They also eliminate spurious memory dependencies and allow for greater instruction-level parallelism by allowing safe out-of-order execution of loads and stores.","title":"Introduction"},{"location":"CPU-memory-access/Memory-disambiguation/#memory#disambiguation","text":"\"disambiguation\"\u7684\u542b\u4e49\u662f\"\u6d88\u9664\u6b67\u4e49\"\u3002 Memory disambiguation\u4e5f\u662f\u7531out-of-order-execution\u5f15\u53d1\u7684\u3002","title":"Memory disambiguation"},{"location":"CPU-memory-access/Memory-disambiguation/#wikipedia#memory#disambiguation","text":"Memory disambiguation is a set of techniques employed by high-performance out-of-order execution microprocessors that execute memory access instructions (loads and stores) out of program order. The mechanisms for performing memory disambiguation, implemented using digital logic inside the microprocessor core, detect true dependencies between memory operations at execution time and allow the processor to recover when a dependence has been violated. They also eliminate spurious memory dependencies and allow for greater instruction-level parallelism by allowing safe out-of-order execution of loads and stores.","title":"wikipedia Memory disambiguation"},{"location":"CPU-memory-access/What-Every-Programmer-Should-Know-About-Memory/","text":"What Every Programmer Should Know About Memory","title":"Introduction"},{"location":"CPU-memory-access/What-Every-Programmer-Should-Know-About-Memory/#what#every#programmer#should#know#about#memory","text":"","title":"What Every Programmer Should Know About Memory"},{"location":"CPU-memory-access/Word/","text":"Word CPU memory access\u7684unit\u3002 wikipedia Word (computer architecture) In computing , a word is the natural unit of data used by a particular processor design. A word is a fixed-sized piece of data handled as a unit by the instruction set or the hardware of the processor. The number of bits in a word (the word size , word width , or word length ) is an important characteristic of any specific processor design or computer architecture . NOTE: \u6309\u7167Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76841.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807 \u4e2d\u7ed9\u51fa\u7684\u5b9a\u4e49\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\uff1a \u6307\u5904\u7406\u673a**\u8fd0\u7b97\u5668**\uff08ALU\uff09\u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684**\u4f4d\u6570** The size of a word is reflected in many aspects of a computer's structure and operation; the majority of the registers in a processor are usually word sized and the largest piece of data that can be transferred to and from the working memory in a single operation is a word in many (not all) architectures. The largest possible address size, used to designate a location in memory, is typically a hardware word (here, \"hardware word\" means the full-sized natural word of the processor, as opposed to any other definition used). Uses of words Depending on how a computer is organized, word-size units may be used for: Fixed point numbers Holders for fixed point , usually integer , numerical values may be available in one or in several different sizes, but one of the sizes available will almost always be the word \uff08programmer\u53ef\u4ee5\u636e\u6b64\u6765\u786e\u5b9aword size\uff09. The other sizes, if any, are likely to be multiples or fractions of the word size. The smaller sizes are normally used only for efficient use of memory; when loaded into the processor, their values usually go into a larger, word sized holder. Floating point numbers Holders for floating point numerical values are typically either a word or a multiple of a word. Addresses Holders for memory addresses must be of a size capable of expressing the needed range of values but not be excessively large, so often the size used is the word though it can also be a multiple or fraction of the word size. NOTE: 1\u3001\u5373\u6307\u9488\u7c7b\u578b\u7684\u957f\u5ea6(pointer) Registers Processor registers are designed with a size appropriate for the type of data they hold, e.g. integers, floating-point numbers, or addresses. Many computer architectures use general-purpose registers that are capable of storing data in multiple representations. These registers must be sized to hold the largest of the available types. Historically, this determined the word size of the architecture. NOTE: 1\u3001 general-purpose registers \u7684\u957f\u5ea6\u548cWord\u7684\u957f\u5ea6\u76f8\u7b49 Memory\u2013processor transfer When the processor reads from the memory subsystem into a register or writes a register's value to memory, the amount of data transferred is often a word . Historically, this amount of bits which could be transferred in one cycle was also called a catena in some environments. In simple memory subsystems, the word is transferred over the memory data bus , which typically has a width of a word or half-word. In memory subsystems that use caches , the word-sized transfer is the one between the processor and the first level of cache; at lower levels of the memory hierarchy larger transfers (which are a multiple of the word size) are normally used. NOTE: \u8fd9\u4e2a\u975e\u5e38\u91cd\u8981\uff0c\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba Unit of address resolution In a given architecture, successive address values designate successive units of memory; this unit is the unit of address resolution. In most computers, the unit is either a character (e.g. a byte) or a word. (A few computers have used bit resolution.) If the unit is a word, then a larger amount of memory can be accessed using an address of a given size at the cost of added complexity to access individual characters. On the other hand, if the unit is a byte, then individual characters can be addressed (i.e. selected during the memory operation). NOTE: \u8fd9\u4e2a\u548c\u524d\u9762\u7684**Memory\u2013processor transfer**\u662f\u7d27\u5bc6\u76f8\u5173\u7684\uff0c\u540e\u9762\u7ae0\u8282\u4f1a\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba\u3002 Instructions Machine instructions are normally the size of the architecture's word, such as in RISC architectures , or a multiple of the \"char\" size that is a fraction of it. This is a natural choice since instructions and data usually share the same memory subsystem. In Harvard architectures the word sizes of instructions and data need not be related, as instructions and data are stored in different memories; for example, the processor in the 1ESS electronic telephone switch had 37-bit instructions and 23-bit data words.","title":"Introduction"},{"location":"CPU-memory-access/Word/#word","text":"CPU memory access\u7684unit\u3002","title":"Word"},{"location":"CPU-memory-access/Word/#wikipedia#word#computer#architecture","text":"In computing , a word is the natural unit of data used by a particular processor design. A word is a fixed-sized piece of data handled as a unit by the instruction set or the hardware of the processor. The number of bits in a word (the word size , word width , or word length ) is an important characteristic of any specific processor design or computer architecture . NOTE: \u6309\u7167Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76841.2.4 \u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6307\u6807 \u4e2d\u7ed9\u51fa\u7684\u5b9a\u4e49\u6bd4\u8f83\u5bb9\u6613\u7406\u89e3\uff1a \u6307\u5904\u7406\u673a**\u8fd0\u7b97\u5668**\uff08ALU\uff09\u4e2d\u4e00\u6b21\u80fd\u591f\u5b8c\u6210\u4e8c\u8fdb\u5236\u6570\u8fd0\u7b97\u7684**\u4f4d\u6570** The size of a word is reflected in many aspects of a computer's structure and operation; the majority of the registers in a processor are usually word sized and the largest piece of data that can be transferred to and from the working memory in a single operation is a word in many (not all) architectures. The largest possible address size, used to designate a location in memory, is typically a hardware word (here, \"hardware word\" means the full-sized natural word of the processor, as opposed to any other definition used).","title":"wikipedia Word (computer architecture)"},{"location":"CPU-memory-access/Word/#uses#of#words","text":"Depending on how a computer is organized, word-size units may be used for:","title":"Uses of words"},{"location":"CPU-memory-access/Word/#fixed#point#numbers","text":"Holders for fixed point , usually integer , numerical values may be available in one or in several different sizes, but one of the sizes available will almost always be the word \uff08programmer\u53ef\u4ee5\u636e\u6b64\u6765\u786e\u5b9aword size\uff09. The other sizes, if any, are likely to be multiples or fractions of the word size. The smaller sizes are normally used only for efficient use of memory; when loaded into the processor, their values usually go into a larger, word sized holder.","title":"Fixed point numbers"},{"location":"CPU-memory-access/Word/#floating#point#numbers","text":"Holders for floating point numerical values are typically either a word or a multiple of a word.","title":"Floating point numbers"},{"location":"CPU-memory-access/Word/#addresses","text":"Holders for memory addresses must be of a size capable of expressing the needed range of values but not be excessively large, so often the size used is the word though it can also be a multiple or fraction of the word size. NOTE: 1\u3001\u5373\u6307\u9488\u7c7b\u578b\u7684\u957f\u5ea6(pointer)","title":"Addresses"},{"location":"CPU-memory-access/Word/#registers","text":"Processor registers are designed with a size appropriate for the type of data they hold, e.g. integers, floating-point numbers, or addresses. Many computer architectures use general-purpose registers that are capable of storing data in multiple representations. These registers must be sized to hold the largest of the available types. Historically, this determined the word size of the architecture. NOTE: 1\u3001 general-purpose registers \u7684\u957f\u5ea6\u548cWord\u7684\u957f\u5ea6\u76f8\u7b49","title":"Registers"},{"location":"CPU-memory-access/Word/#memoryprocessor#transfer","text":"When the processor reads from the memory subsystem into a register or writes a register's value to memory, the amount of data transferred is often a word . Historically, this amount of bits which could be transferred in one cycle was also called a catena in some environments. In simple memory subsystems, the word is transferred over the memory data bus , which typically has a width of a word or half-word. In memory subsystems that use caches , the word-sized transfer is the one between the processor and the first level of cache; at lower levels of the memory hierarchy larger transfers (which are a multiple of the word size) are normally used. NOTE: \u8fd9\u4e2a\u975e\u5e38\u91cd\u8981\uff0c\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u5bf9\u6b64\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba","title":"Memory\u2013processor transfer"},{"location":"CPU-memory-access/Word/#unit#of#address#resolution","text":"In a given architecture, successive address values designate successive units of memory; this unit is the unit of address resolution. In most computers, the unit is either a character (e.g. a byte) or a word. (A few computers have used bit resolution.) If the unit is a word, then a larger amount of memory can be accessed using an address of a given size at the cost of added complexity to access individual characters. On the other hand, if the unit is a byte, then individual characters can be addressed (i.e. selected during the memory operation). NOTE: \u8fd9\u4e2a\u548c\u524d\u9762\u7684**Memory\u2013processor transfer**\u662f\u7d27\u5bc6\u76f8\u5173\u7684\uff0c\u540e\u9762\u7ae0\u8282\u4f1a\u8fdb\u884c\u8be6\u7ec6\u8ba8\u8bba\u3002","title":"Unit of address resolution"},{"location":"CPU-memory-access/Word/#instructions","text":"Machine instructions are normally the size of the architecture's word, such as in RISC architectures , or a multiple of the \"char\" size that is a fraction of it. This is a natural choice since instructions and data usually share the same memory subsystem. In Harvard architectures the word sizes of instructions and data need not be related, as instructions and data are stored in different memories; for example, the processor in the 1ESS electronic telephone switch had 37-bit instructions and 23-bit data words.","title":"Instructions"},{"location":"CPU-memory-access/Word/Get-word-length/","text":"How to get Word length stackoverflow How to determine processor word length in C? A As @holgac mentioned , the long datatype is always the same size as the machine's native word size: \"A word is the amount of data that a machine can process at one time.\" \"The size of a processor\u2019s general-purpose registers (GPRs) is equal to its word size.\" \"Additionally, the size of the C type long is equal to the word size, whereas the size of the int type is sometimes less than that of the word size\" -- Linux Kernel Development, Ch 17 (3 rd edition, pg 381) As indicated by Thomas Matthews however, this may not apply to machines with small word lengths. To determine the size of long on your compiler, just use sizeof(long) : #include \"stdio.h\" #include <limits.h> int main ( void ) { printf ( \"long is %d bits on this system \\n \" , ( int ) sizeof ( long ) * CHAR_BIT ); return 0 ; } See also: 1\u3001 What is CHAR_BIT? C program that get the architecture\uff0832 or 64 bit ) void initServerConfig ( void ) { server . arch_bits = ( sizeof ( long ) == 8 ) ? 64 : 32 ; } \u8fd9\u6bb5\u4ee3\u7801\u4e2d\u4f7f\u7528\u7684\u65b9\u6cd5\u662f\u4eceredis\u7684 server.c \u7684 initServerConfig \u51fd\u6570\u4e2d\u622a\u53d6\u8fc7\u6765\u7684\uff0c\u4e0b\u9762\u662f\u6d4b\u8bd5\u7a0b\u5e8f: #include <iostream> int main () { int arch_bits = ( sizeof ( long ) == 8 ) ? 64 : 32 ; std :: cout << arch_bits << std :: endl ; }","title":"Introduction"},{"location":"CPU-memory-access/Word/Get-word-length/#how#to#get#word#length","text":"","title":"How to get Word length"},{"location":"CPU-memory-access/Word/Get-word-length/#stackoverflow#how#to#determine#processor#word#length#in#c","text":"","title":"stackoverflow How to determine processor word length in C?"},{"location":"CPU-memory-access/Word/Get-word-length/#a","text":"As @holgac mentioned , the long datatype is always the same size as the machine's native word size: \"A word is the amount of data that a machine can process at one time.\" \"The size of a processor\u2019s general-purpose registers (GPRs) is equal to its word size.\" \"Additionally, the size of the C type long is equal to the word size, whereas the size of the int type is sometimes less than that of the word size\" -- Linux Kernel Development, Ch 17 (3 rd edition, pg 381) As indicated by Thomas Matthews however, this may not apply to machines with small word lengths. To determine the size of long on your compiler, just use sizeof(long) : #include \"stdio.h\" #include <limits.h> int main ( void ) { printf ( \"long is %d bits on this system \\n \" , ( int ) sizeof ( long ) * CHAR_BIT ); return 0 ; } See also: 1\u3001 What is CHAR_BIT?","title":"A"},{"location":"CPU-memory-access/Word/Get-word-length/#c#program#that#get#the#architecture32#or#64#bit","text":"void initServerConfig ( void ) { server . arch_bits = ( sizeof ( long ) == 8 ) ? 64 : 32 ; } \u8fd9\u6bb5\u4ee3\u7801\u4e2d\u4f7f\u7528\u7684\u65b9\u6cd5\u662f\u4eceredis\u7684 server.c \u7684 initServerConfig \u51fd\u6570\u4e2d\u622a\u53d6\u8fc7\u6765\u7684\uff0c\u4e0b\u9762\u662f\u6d4b\u8bd5\u7a0b\u5e8f: #include <iostream> int main () { int arch_bits = ( sizeof ( long ) == 8 ) ? 64 : 32 ; std :: cout << arch_bits << std :: endl ; }","title":"C program that get the architecture\uff0832 or 64 bit )"},{"location":"Computer-architecture/","text":"Computer architecture \u672c\u7ae0\u8ba8\u8bbacomputer architecture\uff0c\u5b83\u662f\u7406\u8bba\u6a21\u578b\uff0c\u79d1\u5b66\u7684\u53d1\u5c55\u5f80\u5f80\u662f\u5148\u6709\u7406\u8bba\u6a21\u578b\uff0c\u7136\u540e\u5de5\u4e1a\u754c\u4f9d\u636e\u7406\u8bba\u6a21\u578b\u8fdb\u884c\u751f\u4ea7\uff0c\u56e0\u6b64\u638c\u63e1\u7406\u8bba\u6a21\u578b\uff0c\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u3002 wikipedia Computer architecture","title":"Computer-architecture"},{"location":"Computer-architecture/#computer#architecture","text":"\u672c\u7ae0\u8ba8\u8bbacomputer architecture\uff0c\u5b83\u662f\u7406\u8bba\u6a21\u578b\uff0c\u79d1\u5b66\u7684\u53d1\u5c55\u5f80\u5f80\u662f\u5148\u6709\u7406\u8bba\u6a21\u578b\uff0c\u7136\u540e\u5de5\u4e1a\u754c\u4f9d\u636e\u7406\u8bba\u6a21\u578b\u8fdb\u884c\u751f\u4ea7\uff0c\u56e0\u6b64\u638c\u63e1\u7406\u8bba\u6a21\u578b\uff0c\u80fd\u591f\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u3002","title":"Computer architecture"},{"location":"Computer-architecture/#wikipedia#computer#architecture","text":"","title":"wikipedia Computer architecture"},{"location":"Computer-architecture/TODO-Turing-machine-VS-Von-Neuman-machine/","text":"","title":"TODO-Turing-machine-VS-Von-Neuman-machine"},{"location":"Computer-architecture/TODO-Von-neumann-architecture-VS-Harvard-architecture/","text":"","title":"TODO-Von-neumann-architecture-VS-Harvard-architecture"},{"location":"Computer-architecture/Universal-Turing-machine/","text":"Universal Turing machine","title":"Universal-Turing-machine"},{"location":"Computer-architecture/Universal-Turing-machine/#universal#turing#machine","text":"","title":"Universal Turing machine"},{"location":"Computer-architecture/Harvard-architecture/Harvard-architecture/","text":"Harvard architecture wikipedia Harvard architecture The Harvard architecture is a computer architecture with physically separate storage and signal pathways for instructions and data. The term originated from the Harvard Mark I relay-based computer, which stored instructions on punched tape (24 bits wide) and data in electro-mechanical counters. These early machines had data storage entirely contained within the central processing unit , and provided no access to the instruction storage as data. Programs needed to be loaded by an operator; the processor could not initialize itself. TRANSLATION : \u54c8\u4f5b\u4f53\u7cfb\u7ed3\u6784\u662f\u4e00\u79cd\u8ba1\u7b97\u673a\u4f53\u7cfb\u7ed3\u6784\uff0c\u5b83\u4e3a\u6307\u4ee4\u548c\u6570\u636e\u5206\u522b\u63d0\u4f9b\u4e86\u7269\u7406\u4e0a\u72ec\u7acb\u7684\u5b58\u50a8\u548c\u4fe1\u53f7\u901a\u8def\u3002\u8fd9\u4e2a\u672f\u8bed\u8d77\u6e90\u4e8e\u57fa\u4e8e\u54c8\u4f5bMark I\u4e2d\u7ee7\u7684\u8ba1\u7b97\u673a\uff0c\u5b83\u5c06\u6307\u4ee4\u5b58\u50a8\u5728\u7a7f\u5b54\u5e26(24\u4f4d\u5bbd)\u4e0a\uff0c\u5e76\u5c06\u6570\u636e\u5b58\u50a8\u5728\u673a\u7535\u8ba1\u6570\u5668\u4e2d\u3002\u8fd9\u4e9b\u65e9\u671f\u673a\u5668\u7684\u6570\u636e\u5b58\u50a8\u5b8c\u5168\u5305\u542b\u5728\u4e2d\u592e\u5904\u7406\u5355\u5143\u4e2d\uff0c\u4e0d\u63d0\u4f9b\u5bf9\u6307\u4ee4\u5b58\u50a8\u4f5c\u4e3a\u6570\u636e\u7684\u8bbf\u95ee\u3002\u64cd\u4f5c\u4eba\u5458\u9700\u8981\u52a0\u8f7d\u7684\u7a0b\u5e8f;\u5904\u7406\u5668\u65e0\u6cd5\u521d\u59cb\u5316\u81ea\u5df1\u3002 SUMMARY : \u7b2c\u4e00\u53e5\u8bdd\u5c31\u5c31\u70b9\u660e\u4e86Harvard architecture\u4e0eVon Neumann architecture\u6700\u5927\u7684\u5dee\u522b\uff0c\u5b83\u5728\u4e8e\u5982\u4f55\u5bf9\u5f85instruction \u548c data\uff1bVon Neumann architecture\u5e76\u4e0d\u533a\u5206instruction\u548cdata\uff0c\u5b83\u5c06\u5b83\u4eec\u90fd\u89c6\u4e3adata\uff1b\u800cHarvard architecture\u5219\u4e25\u683c\u533a\u5206\u4e24\u8005\uff1b Today, most processors implement such separate signal pathways for performance reasons, but actually implement a modified Harvard architecture , so they can support tasks like loading a program from disk storage as data and then executing it. Harvard architecture","title":"Harvard-architecture"},{"location":"Computer-architecture/Harvard-architecture/Harvard-architecture/#harvard#architecture","text":"","title":"Harvard architecture"},{"location":"Computer-architecture/Harvard-architecture/Harvard-architecture/#wikipedia#harvard#architecture","text":"The Harvard architecture is a computer architecture with physically separate storage and signal pathways for instructions and data. The term originated from the Harvard Mark I relay-based computer, which stored instructions on punched tape (24 bits wide) and data in electro-mechanical counters. These early machines had data storage entirely contained within the central processing unit , and provided no access to the instruction storage as data. Programs needed to be loaded by an operator; the processor could not initialize itself. TRANSLATION : \u54c8\u4f5b\u4f53\u7cfb\u7ed3\u6784\u662f\u4e00\u79cd\u8ba1\u7b97\u673a\u4f53\u7cfb\u7ed3\u6784\uff0c\u5b83\u4e3a\u6307\u4ee4\u548c\u6570\u636e\u5206\u522b\u63d0\u4f9b\u4e86\u7269\u7406\u4e0a\u72ec\u7acb\u7684\u5b58\u50a8\u548c\u4fe1\u53f7\u901a\u8def\u3002\u8fd9\u4e2a\u672f\u8bed\u8d77\u6e90\u4e8e\u57fa\u4e8e\u54c8\u4f5bMark I\u4e2d\u7ee7\u7684\u8ba1\u7b97\u673a\uff0c\u5b83\u5c06\u6307\u4ee4\u5b58\u50a8\u5728\u7a7f\u5b54\u5e26(24\u4f4d\u5bbd)\u4e0a\uff0c\u5e76\u5c06\u6570\u636e\u5b58\u50a8\u5728\u673a\u7535\u8ba1\u6570\u5668\u4e2d\u3002\u8fd9\u4e9b\u65e9\u671f\u673a\u5668\u7684\u6570\u636e\u5b58\u50a8\u5b8c\u5168\u5305\u542b\u5728\u4e2d\u592e\u5904\u7406\u5355\u5143\u4e2d\uff0c\u4e0d\u63d0\u4f9b\u5bf9\u6307\u4ee4\u5b58\u50a8\u4f5c\u4e3a\u6570\u636e\u7684\u8bbf\u95ee\u3002\u64cd\u4f5c\u4eba\u5458\u9700\u8981\u52a0\u8f7d\u7684\u7a0b\u5e8f;\u5904\u7406\u5668\u65e0\u6cd5\u521d\u59cb\u5316\u81ea\u5df1\u3002 SUMMARY : \u7b2c\u4e00\u53e5\u8bdd\u5c31\u5c31\u70b9\u660e\u4e86Harvard architecture\u4e0eVon Neumann architecture\u6700\u5927\u7684\u5dee\u522b\uff0c\u5b83\u5728\u4e8e\u5982\u4f55\u5bf9\u5f85instruction \u548c data\uff1bVon Neumann architecture\u5e76\u4e0d\u533a\u5206instruction\u548cdata\uff0c\u5b83\u5c06\u5b83\u4eec\u90fd\u89c6\u4e3adata\uff1b\u800cHarvard architecture\u5219\u4e25\u683c\u533a\u5206\u4e24\u8005\uff1b Today, most processors implement such separate signal pathways for performance reasons, but actually implement a modified Harvard architecture , so they can support tasks like loading a program from disk storage as data and then executing it. Harvard architecture","title":"wikipedia Harvard architecture"},{"location":"Computer-architecture/Harvard-architecture/Modified-Harvard-architecture/","text":"wikipedia Modified Harvard architecture The modified Harvard architecture is a variation of the Harvard computer architecture that allows the contents of the instruction memory to be accessed as if it were data . Most modern computers that are documented as Harvard architecture are, in fact, modified Harvard architecture. SUMMARY : \u663e\u7136modified Harvard architeture\u5438\u6536\u4e86Von Neumann architecture\u7684\u5e76\u4e0d\u533a\u5206instruction\u548cdata\uff0c\u5c06\u5b83\u4eec\u90fd\u89c6\u4e3adata \u7684\u601d\u60f3\uff1b Harvard architecture Main article: Harvard architecture The original Harvard architecture computer, the Harvard Mark I , employed entirely separate memory systems to store instructions and data. The CPU fetched the next instruction and loaded or stored data simultaneously and independently\uff08\u6709\u4e9bparallel\u7684\u601d\u60f3\uff09. This is in contrast to a von Neumann architecture computer, in which both instructions and data are stored in the same memory system and (without the complexity of a CPU cache ) must be accessed in turn . The physical separation of instruction and data memory is sometimes held to be the distinguishing feature of modern Harvard architecture computers. With microcontrollers (entire computer systems integrated onto single chips), the use of different memory technologies for instructions (e.g. flash memory ) and data (typically read/write memory ) in von Neumann machines is becoming popular\uff08\u5229\u7528\u5fae\u63a7\u5236\u5668\uff08\u6574\u4e2a\u8ba1\u7b97\u673a\u7cfb\u7edf\u96c6\u6210\u5230\u5355\u4e2a\u82af\u7247\u4e0a\uff09\uff0c\u5728\u51af\u00b7\u8bfa\u4f9d\u66fc\u673a\u5668\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684\u5b58\u50a8\u5668\u6280\u672f\u7528\u4e8e\u6307\u4ee4\uff08\u4f8b\u5982\u95ea\u5b58\uff09\u548c\u6570\u636e\uff08\u901a\u5e38\u662f\u8bfb/\u5199\u5b58\u50a8\u5668\uff09\u6b63\u53d8\u5f97\u6d41\u884c\uff09. The true distinction of a Harvard machine is that instruction and data memory occupy different address spaces . In other words, a memory address does not uniquely identify a storage location (as it does in a von Neumann machine); it is also necessary to know the memory space (instruction or data) to which the address belongs. Von Neumann architecture Main article: Von Neumann architecture A computer with a von Neumann architecture has the advantage over pure Harvard machines in that code can also be accessed and treated the same as data, and vice versa. This allows, for example, data to be read from disk storage into memory and then executed as code, or self-optimizing software systems using technologies such as just-in-time compilation to write machine code into their own memory and then later execute it. Another example is self-modifying code , which allows a program to modify itself. A disadvantage of these methods are issues with executable space protection , which increase the risks from malware and software defects. In addition, in these systems it is notoriously difficult to document code flow, and also can make debugging much more difficult.","title":"Modified-Harvard-architecture"},{"location":"Computer-architecture/Harvard-architecture/Modified-Harvard-architecture/#wikipedia#modified#harvard#architecture","text":"The modified Harvard architecture is a variation of the Harvard computer architecture that allows the contents of the instruction memory to be accessed as if it were data . Most modern computers that are documented as Harvard architecture are, in fact, modified Harvard architecture. SUMMARY : \u663e\u7136modified Harvard architeture\u5438\u6536\u4e86Von Neumann architecture\u7684\u5e76\u4e0d\u533a\u5206instruction\u548cdata\uff0c\u5c06\u5b83\u4eec\u90fd\u89c6\u4e3adata \u7684\u601d\u60f3\uff1b","title":"wikipedia Modified Harvard architecture"},{"location":"Computer-architecture/Harvard-architecture/Modified-Harvard-architecture/#harvard#architecture","text":"Main article: Harvard architecture The original Harvard architecture computer, the Harvard Mark I , employed entirely separate memory systems to store instructions and data. The CPU fetched the next instruction and loaded or stored data simultaneously and independently\uff08\u6709\u4e9bparallel\u7684\u601d\u60f3\uff09. This is in contrast to a von Neumann architecture computer, in which both instructions and data are stored in the same memory system and (without the complexity of a CPU cache ) must be accessed in turn . The physical separation of instruction and data memory is sometimes held to be the distinguishing feature of modern Harvard architecture computers. With microcontrollers (entire computer systems integrated onto single chips), the use of different memory technologies for instructions (e.g. flash memory ) and data (typically read/write memory ) in von Neumann machines is becoming popular\uff08\u5229\u7528\u5fae\u63a7\u5236\u5668\uff08\u6574\u4e2a\u8ba1\u7b97\u673a\u7cfb\u7edf\u96c6\u6210\u5230\u5355\u4e2a\u82af\u7247\u4e0a\uff09\uff0c\u5728\u51af\u00b7\u8bfa\u4f9d\u66fc\u673a\u5668\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684\u5b58\u50a8\u5668\u6280\u672f\u7528\u4e8e\u6307\u4ee4\uff08\u4f8b\u5982\u95ea\u5b58\uff09\u548c\u6570\u636e\uff08\u901a\u5e38\u662f\u8bfb/\u5199\u5b58\u50a8\u5668\uff09\u6b63\u53d8\u5f97\u6d41\u884c\uff09. The true distinction of a Harvard machine is that instruction and data memory occupy different address spaces . In other words, a memory address does not uniquely identify a storage location (as it does in a von Neumann machine); it is also necessary to know the memory space (instruction or data) to which the address belongs.","title":"Harvard architecture"},{"location":"Computer-architecture/Harvard-architecture/Modified-Harvard-architecture/#von#neumann#architecture","text":"Main article: Von Neumann architecture A computer with a von Neumann architecture has the advantage over pure Harvard machines in that code can also be accessed and treated the same as data, and vice versa. This allows, for example, data to be read from disk storage into memory and then executed as code, or self-optimizing software systems using technologies such as just-in-time compilation to write machine code into their own memory and then later execute it. Another example is self-modifying code , which allows a program to modify itself. A disadvantage of these methods are issues with executable space protection , which increase the risks from malware and software defects. In addition, in these systems it is notoriously difficult to document code flow, and also can make debugging much more difficult.","title":"Von Neumann architecture"},{"location":"Computer-architecture/Stored-program-computer/","text":"Stored-program computer stored-program computer\u662f\u975e\u5e38\u91cd\u8981\u7684\u601d\u60f3\u3002 wikipedia Stored-program computer A stored-program computer is one which stores program instructions in electronic memory.[ 1] Often the definition is extended with the requirement that the treatment of programs and data in memory be interchangeable or uniform.[ 2] [ 3] [ 4] A computer with a Von Neumann architecture stores program data and instruction data in the same memory; a computer with a Harvard architecture has separate memories for storing program and data.[ 5] [ 6] NOTE: von neumann architecture \u548c Harvard architecture\u7684\u5dee\u5f02 The stored-program computer idea can be traced back to the 1936 theoretical concept of a universal Turing machine .[ 11] Von Neumann was aware of this paper, and he impressed it on his collaborators as well.[ 12] NOTE: \u5173\u4e8e\u8c01\u9996\u5148\u63d0\u51fa\u8fd9\u4e2a\u6982\u5ff5\uff0c\u5386\u53f2\u5b66\u5bb6\u8ffd\u6eaf\u5230\u4e86Turing wikipedia Universal Turing machine \u00a7 Stored-program computer Function and data model Stored-program computer \u601d\u60f3\u544a\u8bc9\u6211\u4eec: \u5728memory\u4e2d\uff0c\u6709\u5982\u4e0b\u4e24\u7c7b: 1\u3001instruction 2\u3001data \u5728 ./Function-and-data-model \u4e2d\uff0c\u5c06\u57fa\u4e8e\u6b64\u6765\u6784\u5efa\u8d77\u4e00\u4e2a\u8fde\u63a5software\u548chardware\u7684uniform model\u3002","title":"Introduction"},{"location":"Computer-architecture/Stored-program-computer/#stored-program#computer","text":"stored-program computer\u662f\u975e\u5e38\u91cd\u8981\u7684\u601d\u60f3\u3002","title":"Stored-program computer"},{"location":"Computer-architecture/Stored-program-computer/#wikipedia#stored-program#computer","text":"A stored-program computer is one which stores program instructions in electronic memory.[ 1] Often the definition is extended with the requirement that the treatment of programs and data in memory be interchangeable or uniform.[ 2] [ 3] [ 4] A computer with a Von Neumann architecture stores program data and instruction data in the same memory; a computer with a Harvard architecture has separate memories for storing program and data.[ 5] [ 6] NOTE: von neumann architecture \u548c Harvard architecture\u7684\u5dee\u5f02 The stored-program computer idea can be traced back to the 1936 theoretical concept of a universal Turing machine .[ 11] Von Neumann was aware of this paper, and he impressed it on his collaborators as well.[ 12] NOTE: \u5173\u4e8e\u8c01\u9996\u5148\u63d0\u51fa\u8fd9\u4e2a\u6982\u5ff5\uff0c\u5386\u53f2\u5b66\u5bb6\u8ffd\u6eaf\u5230\u4e86Turing","title":"wikipedia Stored-program computer"},{"location":"Computer-architecture/Stored-program-computer/#wikipedia#universal#turing#machine#stored-program#computer","text":"","title":"wikipedia Universal Turing machine \u00a7 Stored-program computer"},{"location":"Computer-architecture/Stored-program-computer/#function#and#data#model","text":"Stored-program computer \u601d\u60f3\u544a\u8bc9\u6211\u4eec: \u5728memory\u4e2d\uff0c\u6709\u5982\u4e0b\u4e24\u7c7b: 1\u3001instruction 2\u3001data \u5728 ./Function-and-data-model \u4e2d\uff0c\u5c06\u57fa\u4e8e\u6b64\u6765\u6784\u5efa\u8d77\u4e00\u4e2a\u8fde\u63a5software\u548chardware\u7684uniform model\u3002","title":"Function and data model"},{"location":"Computer-architecture/Stored-program-computer/#_1","text":"","title":""},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/","text":"Function and data model \u5728\u73b0\u4ee3\uff0c\u4e0d\u65ad\u5730\u6d8c\u73b0\u7740\u65b0\u7684programming language\uff0c\u4e0d\u65ad\u5730\u6d8c\u73b0\u7740\u65b0\u7684programming paradigm\uff08\u6bd4\u5982\u4eceprocedural\u5230OOP\uff09\uff0cprogramming technique\u7684\u63d0\u9ad8\u80fd\u591f\u5927\u5927\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u662f\u65e0\u8bba\u5982\u4f55\u53d8\u5316\uff0c\u5b83\u4eec\u6700\u7ec8\u6267\u884c\u7684\u65f6\u5019\uff0c\u8fd8\u662f\u9075\u5faa\u201cstored-program computer\u201d\u7684\u601d\u60f3\uff0c\u5373\u5b83\u4eec\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u6700\u7ec8\u90fd\u4f1a\u88ab\u201c\u5403\u201d\u5230RAM\u4e2d\uff0c\u7136\u540e\u7531CPU\u8fdb\u884c\u6267\u884c\uff1b\u4e3a\u4e86\u4fbf\u4e8e\u7406\u89e3\u5404\u79cdprogramming language\u7684run mode\uff0c\u53ef\u4ee5\u5c06\u5404\u79cdprogramming language\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\uff08\u540e\u9762\u7b80\u79f0\u4e3aprogram\uff09\u7b80\u5316\u4e3a\u6709\u4e0b\u9762\u4e24\u90e8\u5206\u7ec4\u6210: \u7ec4\u6210\u90e8\u5206 \u8bf4\u660e hardware function \u51fd\u6570 \u4ece\u6c47\u7f16\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u51fd\u6570\u5c31\u662f\u4e00\u5806\u6307\u4ee4\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u662finstruction data \u6570\u636e \u5bf9\u5e94\u7684\u662fmemory \u6211\u4eec\u5c06\u5b83\u7b80\u79f0\u4e3a**Function and data model**\uff0c\u663e\u7136\u5b83\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\uff0c\u5efa\u7acb\u5728programming language\u548cCPU\u4e4b\u95f4\u3002 Software and hardware **Function and data model**\u8fde\u63a5software\u548chardware\u7684uniform model\uff0c\u5173\u4e8eSoftware and hardware\u7684\u8ba8\u8bba\uff0c\u53c2\u89c1\u6587\u7ae0\u300aSoftware-and-hardware\u300b \u4e0b\u9762\u7ed3\u5408\u4e00\u4e9b\u5177\u4f53\u95ee\u9898\uff0c\u6765\u5bf9\u5b83\u8fdb\u884c\u63cf\u8ff0\u3002 Process VMA wikipedia Memory address # Address space in application programming NOTE: \u5c06process\u7684VMA\u5206\u4e3a\u4e24\u90e8\u5206: 1) Machine code / instruction\uff0c\u5bf9\u5e94\u7684\u662ffunction 2) Data \u663e\u7136\uff0c\u6700\u7ec8\u6211\u4eec\u7684process\uff08run-time\u6982\u5ff5\uff09\u53ef\u4ee5\u770b\u505a\u662f\u6709\u4e0a\u8ff0\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u663e\u7136\u8fd9\u6837\u7684\u62bd\u8c61\u6709\u52a9\u4e8e\u6211\u4eec\u5bf9run mode\u7684\u7406\u89e3\uff1b Each memory location in a stored-program computer holds a binary number or decimal number of some sort . Its interpretation, as data of some data type or as an instruction, and use are determined by the instructions which retrieve and manipulate it. In modern multitasking environment, an application process usually has in its address space (or spaces) chunks of memory of following types: 1) Machine code , including: program's own code (historically known as code segment or text segment ); shared libraries . 2) Data , including: initialized data ( data segment ); uninitialized (but allocated) variables; run-time stack ; heap ; shared memory and memory mapped files . Some parts of address space may be not mapped at all. OOP \u8d8a\u6765\u8d8a\u591a\u7684programming language\u652f\u6301OOP\uff0c\u90a3\u4e48\u6211\u4eec\u4e0d\u4ec5\u8981\u95ee\uff1aFunction and data model\u80fd\u5426\u63cf\u8ff0\u4f7f\u7528 OOP language\u7f16\u5199\u7684\u7684program\u5462\uff1f\u7b54\u6848\u662f: \u53ef\u4ee5\u7684\u3002\u5206\u6790\u5982\u4e0b: OOP\u53ea\u662f\u4e00\u79cdprogramming paradigm\uff0c\u4e0d\u540c\u7684programming\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\uff0c\u4f46\u662f\u4e0d\u7ba1\u5982\u4f55\u5b9e\u73b0\uff0c\u5b83\u7684member method\u672c\u8d28\u4e0a\u662ffunction\uff0cmember variable\u672c\u8d28\u4e0a\u662fdata\uff0c\u4f9d\u7136\u53ef\u4ee5\u4f7f\u7528Function and data model\u6765\u8fdb\u884c\u63cf\u8ff0\u3002\u56e0\u6b64\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7684procedure programming language\u800c\u8a00\uff0c\u5f88\u591a\u6982\u5ff5\u90fd\u9700\u8981\u5411OOP\u6269\u5c55: \u7ec4\u6210\u90e8\u5206 \u8bf4\u660e function \u5305\u62ec: - \u666e\u901a\u51fd\u6570 - \u6210\u5458\u51fd\u6570; data \u5305\u62ec: - \u666e\u901adata - member data Pointer \u7531\u4e8e\u5b83\u4eec\u90fd\u4f4d\u4e8eRAM\u4e2d\uff0c\u56e0\u6b64\u90fd\u6709\u7740address\uff0c\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7pointer\u6765**\u8bbf\u95ee**/ \u5f15\u7528 : pointer explanation pointer to function Stored-program computer \u542f\u53d1\u4e86\u6211\u4eec stores program instructions in electronic memory. \u6240\u4ee5\u6211\u4eec\u7684\u6240\u7f16\u5199\u7684function\uff0c\u6700\u7ec8\u4e5f\u662f\u4f1a\u5b58\u5165\u5230 RAM\u4e2d\u7684\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u5c31\u548c\u6570\u636e\u4e00\u6837\uff0c\u53ef\u4ee5\u901a\u8fc7pointer\u5bf9\u5176\u8fdb\u884caccess\uff1b\u6240\u4ee5\u8fd9\u5c31\u662ffunction pointer\u7684\u672c\u8d28\u6240\u5728\uff1b pointer to data Memory access \u5728 CPU-memory-access \u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 Operation on memory 1\u3001read 2\u3001write","title":"Introduction"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#function#and#data#model","text":"\u5728\u73b0\u4ee3\uff0c\u4e0d\u65ad\u5730\u6d8c\u73b0\u7740\u65b0\u7684programming language\uff0c\u4e0d\u65ad\u5730\u6d8c\u73b0\u7740\u65b0\u7684programming paradigm\uff08\u6bd4\u5982\u4eceprocedural\u5230OOP\uff09\uff0cprogramming technique\u7684\u63d0\u9ad8\u80fd\u591f\u5927\u5927\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u662f\u65e0\u8bba\u5982\u4f55\u53d8\u5316\uff0c\u5b83\u4eec\u6700\u7ec8\u6267\u884c\u7684\u65f6\u5019\uff0c\u8fd8\u662f\u9075\u5faa\u201cstored-program computer\u201d\u7684\u601d\u60f3\uff0c\u5373\u5b83\u4eec\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u6700\u7ec8\u90fd\u4f1a\u88ab\u201c\u5403\u201d\u5230RAM\u4e2d\uff0c\u7136\u540e\u7531CPU\u8fdb\u884c\u6267\u884c\uff1b\u4e3a\u4e86\u4fbf\u4e8e\u7406\u89e3\u5404\u79cdprogramming language\u7684run mode\uff0c\u53ef\u4ee5\u5c06\u5404\u79cdprogramming language\u7684\u53ef\u6267\u884c\u7a0b\u5e8f\uff08\u540e\u9762\u7b80\u79f0\u4e3aprogram\uff09\u7b80\u5316\u4e3a\u6709\u4e0b\u9762\u4e24\u90e8\u5206\u7ec4\u6210: \u7ec4\u6210\u90e8\u5206 \u8bf4\u660e hardware function \u51fd\u6570 \u4ece\u6c47\u7f16\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u51fd\u6570\u5c31\u662f\u4e00\u5806\u6307\u4ee4\uff0c\u56e0\u6b64\u5bf9\u5e94\u7684\u662finstruction data \u6570\u636e \u5bf9\u5e94\u7684\u662fmemory \u6211\u4eec\u5c06\u5b83\u7b80\u79f0\u4e3a**Function and data model**\uff0c\u663e\u7136\u5b83\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\uff0c\u5efa\u7acb\u5728programming language\u548cCPU\u4e4b\u95f4\u3002","title":"Function and data model"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#software#and#hardware","text":"**Function and data model**\u8fde\u63a5software\u548chardware\u7684uniform model\uff0c\u5173\u4e8eSoftware and hardware\u7684\u8ba8\u8bba\uff0c\u53c2\u89c1\u6587\u7ae0\u300aSoftware-and-hardware\u300b \u4e0b\u9762\u7ed3\u5408\u4e00\u4e9b\u5177\u4f53\u95ee\u9898\uff0c\u6765\u5bf9\u5b83\u8fdb\u884c\u63cf\u8ff0\u3002","title":"Software and hardware"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#process#vma","text":"","title":"Process VMA"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#wikipedia#memory#address#address#space#in#application#programming","text":"NOTE: \u5c06process\u7684VMA\u5206\u4e3a\u4e24\u90e8\u5206: 1) Machine code / instruction\uff0c\u5bf9\u5e94\u7684\u662ffunction 2) Data \u663e\u7136\uff0c\u6700\u7ec8\u6211\u4eec\u7684process\uff08run-time\u6982\u5ff5\uff09\u53ef\u4ee5\u770b\u505a\u662f\u6709\u4e0a\u8ff0\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u663e\u7136\u8fd9\u6837\u7684\u62bd\u8c61\u6709\u52a9\u4e8e\u6211\u4eec\u5bf9run mode\u7684\u7406\u89e3\uff1b Each memory location in a stored-program computer holds a binary number or decimal number of some sort . Its interpretation, as data of some data type or as an instruction, and use are determined by the instructions which retrieve and manipulate it. In modern multitasking environment, an application process usually has in its address space (or spaces) chunks of memory of following types: 1) Machine code , including: program's own code (historically known as code segment or text segment ); shared libraries . 2) Data , including: initialized data ( data segment ); uninitialized (but allocated) variables; run-time stack ; heap ; shared memory and memory mapped files . Some parts of address space may be not mapped at all.","title":"wikipedia Memory address # Address space in application programming"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#oop","text":"\u8d8a\u6765\u8d8a\u591a\u7684programming language\u652f\u6301OOP\uff0c\u90a3\u4e48\u6211\u4eec\u4e0d\u4ec5\u8981\u95ee\uff1aFunction and data model\u80fd\u5426\u63cf\u8ff0\u4f7f\u7528 OOP language\u7f16\u5199\u7684\u7684program\u5462\uff1f\u7b54\u6848\u662f: \u53ef\u4ee5\u7684\u3002\u5206\u6790\u5982\u4e0b: OOP\u53ea\u662f\u4e00\u79cdprogramming paradigm\uff0c\u4e0d\u540c\u7684programming\u7684\u5b9e\u73b0\u65b9\u5f0f\u662f\u4e0d\u540c\u7684\uff0c\u4f46\u662f\u4e0d\u7ba1\u5982\u4f55\u5b9e\u73b0\uff0c\u5b83\u7684member method\u672c\u8d28\u4e0a\u662ffunction\uff0cmember variable\u672c\u8d28\u4e0a\u662fdata\uff0c\u4f9d\u7136\u53ef\u4ee5\u4f7f\u7528Function and data model\u6765\u8fdb\u884c\u63cf\u8ff0\u3002\u56e0\u6b64\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7684procedure programming language\u800c\u8a00\uff0c\u5f88\u591a\u6982\u5ff5\u90fd\u9700\u8981\u5411OOP\u6269\u5c55: \u7ec4\u6210\u90e8\u5206 \u8bf4\u660e function \u5305\u62ec: - \u666e\u901a\u51fd\u6570 - \u6210\u5458\u51fd\u6570; data \u5305\u62ec: - \u666e\u901adata - member data","title":"OOP"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#pointer","text":"\u7531\u4e8e\u5b83\u4eec\u90fd\u4f4d\u4e8eRAM\u4e2d\uff0c\u56e0\u6b64\u90fd\u6709\u7740address\uff0c\u56e0\u6b64\u53ef\u4ee5\u901a\u8fc7pointer\u6765**\u8bbf\u95ee**/ \u5f15\u7528 : pointer explanation pointer to function Stored-program computer \u542f\u53d1\u4e86\u6211\u4eec stores program instructions in electronic memory. \u6240\u4ee5\u6211\u4eec\u7684\u6240\u7f16\u5199\u7684function\uff0c\u6700\u7ec8\u4e5f\u662f\u4f1a\u5b58\u5165\u5230 RAM\u4e2d\u7684\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u5c31\u548c\u6570\u636e\u4e00\u6837\uff0c\u53ef\u4ee5\u901a\u8fc7pointer\u5bf9\u5176\u8fdb\u884caccess\uff1b\u6240\u4ee5\u8fd9\u5c31\u662ffunction pointer\u7684\u672c\u8d28\u6240\u5728\uff1b pointer to data","title":"Pointer"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#memory#access","text":"\u5728 CPU-memory-access \u7ae0\u8282\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002","title":"Memory access"},{"location":"Computer-architecture/Stored-program-computer/Function-and-data-model/#operation#on#memory","text":"1\u3001read 2\u3001write","title":"Operation on memory"},{"location":"Computer-architecture/Von-Neumann-architecture/","text":"Von Neumann architecture wikipedia Von Neumann architecture See also: Stored-program computer and Universal Turing machine \u00a7 Stored-program computer The von Neumann architecture \u2014also known as the von Neumann model or Princeton architecture \u2014is a computer architecture based on a 1945 description by the mathematician and physicist John von Neumann and others in the First Draft of a Report on the EDVAC .[ 1] That document describes a design architecture for an electronic digital computer with these components: A processing unit that contains an arithmetic logic unit and processor registers NOTE: \u4e0a\u9762\u6240\u8bf4\u7684processing unit\u6307\u7684\u5c31\u662fCPU A control unit that contains an instruction register and program counter Memory that stores data and instructions NOTE: \u8981\u5145\u5206\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u542b\u4e49\uff0c\u5c31\u9700\u8981\u9605\u8bfbSee also: Stored-program computer and Universal Turing machine \u00a7 Stored-program computer External mass storage Input and output mechanisms[ 1] [ 2] The term \"von Neumann architecture\" has evolved to mean any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus . This is referred to as the von Neumann bottleneck and often limits the performance of the system.[ 3] NOTE: \u672f\u8bed\u201c\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u201d\u5df2\u7ecf\u53d1\u5c55\u4e3a\u4efb\u4f55\u5b58\u50a8\u7a0b\u5e8f\u8ba1\u7b97\u673a\uff0c\u5176\u4e2d\u6307\u4ee4\u83b7\u53d6\u548c\u6570\u636e\u64cd\u4f5c\u4e0d\u80fd\u540c\u65f6\u53d1\u751f\uff0c\u56e0\u4e3a\u5b83\u4eec\u5171\u4eab\u516c\u5171\u603b\u7ebf\u3002 \u8fd9\u88ab\u79f0\u4e3a\u51af\u8bfa\u4f9d\u66fc\u74f6\u9888\uff0c\u5e76\u4e14\u7ecf\u5e38\u9650\u5236\u7cfb\u7edf\u7684\u6027\u80fd The design of a von Neumann architecture machine is simpler than a Harvard architecture machine\u2014which is also a stored-program system but has one dedicated set of address and data buses for reading and writing to memory, and another set of address and data buses to fetch instructions . A stored-program digital computer keeps both program instructions and data in read-write , random-access memory (RAM). Stored-program computers were an advancement over the program-controlled computers of the 1940s, such as the Colossus and the ENIAC . Those were programmed by setting switches and inserting patch cables to route data and control signals between various functional units. The vast majority of modern computers use the same memory for both data and program instructions. The von Neumann vs. Harvard distinction applies to the cache architecture, not the main memory ( split cache architecture ). Von Neumann architecture scheme History The earliest computing machines had fixed programs. Some very simple computers still use this design, either for simplicity or training purposes. For example, a desk calculator (in principle) is a fixed program computer. It can do basic mathematics , but it cannot run a word processor or games. Changing the program of a fixed-program machine requires rewiring, restructuring, or redesigning the machine. The earliest computers were not so much \"programmed\" as \"designed\" for a particular task. \"Reprogramming\"\u2014when possible at all\u2014was a laborious process that started with flowcharts and paper notes, followed by detailed engineering designs, and then the often-arduous process of physically rewiring and rebuilding the machine. It could take three weeks to set up and debug a program on ENIAC .[ 4] NOTE: \u6700\u65e9\u7684\u8ba1\u7b97\u673a\u6709\u56fa\u5b9a\u7684\u7a0b\u5e8f\u3002\u4e00\u4e9b\u975e\u5e38\u7b80\u5355\u7684\u8ba1\u7b97\u673a\u4ecd\u7136\u4f7f\u7528\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u65e0\u8bba\u662f\u4e3a\u4e86\u7b80\u5355\u8fd8\u662f\u57f9\u8bad\u3002\u4f8b\u5982\uff0c\u684c\u9762\u8ba1\u7b97\u5668\uff08\u539f\u5219\u4e0a\uff09\u662f\u56fa\u5b9a\u7a0b\u5e8f\u8ba1\u7b97\u673a\u3002\u5b83\u53ef\u4ee5\u505a\u57fa\u7840\u6570\u5b66\uff0c\u4f46\u5b83\u4e0d\u80fd\u8fd0\u884c\u6587\u5b57\u5904\u7406\u5668\u6216\u6e38\u620f\u3002\u66f4\u6539\u56fa\u5b9a\u7a0b\u5e8f\u673a\u5668\u7684\u7a0b\u5e8f\u9700\u8981\u91cd\u65b0\u5e03\u7ebf\uff0c\u91cd\u7ec4\u6216\u91cd\u65b0\u8bbe\u8ba1\u673a\u5668\u3002\u6700\u65e9\u7684\u8ba1\u7b97\u673a\u5e76\u6ca1\u6709\u4e3a\u7279\u5b9a\u4efb\u52a1\u201c\u7f16\u7a0b\u201d\u4e3a\u201c\u8bbe\u8ba1\u201d\u3002 \u201c\u91cd\u65b0\u7f16\u7a0b\u201d - \u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b - \u662f\u4e00\u4e2a\u8270\u82e6\u7684\u8fc7\u7a0b\uff0c\u4ece\u6d41\u7a0b\u56fe\u548c\u7eb8\u8d28\u7b14\u8bb0\u5f00\u59cb\uff0c\u7136\u540e\u662f\u8be6\u7ec6\u7684\u5de5\u7a0b\u8bbe\u8ba1\uff0c\u7136\u540e\u662f\u7ecf\u5e38\u8270\u82e6\u7684\u7269\u7406\u91cd\u65b0\u5e03\u7ebf\u548c\u91cd\u5efa\u673a\u5668\u7684\u8fc7\u7a0b\u3002\u5728ENIAC\u4e0a\u8bbe\u7f6e\u548c\u8c03\u8bd5\u7a0b\u5e8f\u53ef\u80fd\u9700\u8981\u4e09\u5468\u65f6\u95f4\u3002[4] With the proposal of the stored-program computer , this changed. A stored-program computer includes, by design, an instruction set , and can store in memory a set of instructions (a program ) that details the computation . NOTE: \u968f\u7740\u5b58\u50a8\u7a0b\u5e8f\u8ba1\u7b97\u673a\u7684\u63d0\u8bae\uff0c\u8fd9\u6539\u53d8\u4e86\u3002\u5b58\u50a8\u7a0b\u5e8f\u8ba1\u7b97\u673a\u901a\u8fc7\u8bbe\u8ba1\u5305\u62ec\u6307\u4ee4\u96c6\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5b58\u50a8\u5668\u4e2d\u5b58\u50a8\u8be6\u7ec6\u8bf4\u660e\u8ba1\u7b97\u7684\u4e00\u7ec4\u6307\u4ee4\uff08\u7a0b\u5e8f\uff09\u3002 A stored-program design also allows for self-modifying code . One early motivation for such a facility was the need for a program to increment or otherwise modify the address portion of instructions, which operators had to do manually in early designs. This became less important when index registers and indirect addressing became usual features of machine architecture. Another use was to embed frequently used data in the instruction stream using immediate addressing . Self-modifying code has largely fallen out of favor, since it is usually hard to understand and debug , as well as being inefficient under modern processor pipelining and caching schemes. NOTE: \u5b58\u50a8\u7a0b\u5e8f\u8bbe\u8ba1\u8fd8\u5141\u8bb8\u81ea\u4fee\u6539\u4ee3\u7801\u3002\u8fd9\u79cd\u8bbe\u65bd\u7684\u4e00\u4e2a\u65e9\u671f\u52a8\u673a\u662f\u9700\u8981\u4e00\u4e2a\u7a0b\u5e8f\u6765\u589e\u52a0\u6216\u4ee5\u5176\u4ed6\u65b9\u5f0f\u4fee\u6539\u6307\u4ee4\u7684\u5730\u5740\u90e8\u5206\uff0c\u64cd\u4f5c\u5458\u5fc5\u987b\u5728\u65e9\u671f\u8bbe\u8ba1\u4e2d\u624b\u52a8\u5b8c\u6210\u3002\u5f53\u7d22\u5f15\u5bc4\u5b58\u5668\u548c\u95f4\u63a5\u5bfb\u5740\u6210\u4e3a\u673a\u5668\u67b6\u6784\u7684\u5e38\u7528\u529f\u80fd\u65f6\uff0c\u8fd9\u53d8\u5f97\u4e0d\u90a3\u4e48\u91cd\u8981\u4e86\u3002\u53e6\u4e00\u79cd\u7528\u9014\u662f\u4f7f\u7528\u7acb\u5373\u5bfb\u5740\u5c06\u7ecf\u5e38\u4f7f\u7528\u7684\u6570\u636e\u5d4c\u5165\u6307\u4ee4\u6d41\u4e2d\u3002\u81ea\u4fee\u6539\u4ee3\u7801\u5df2\u5927\u90e8\u5206\u5931\u5ba0\uff0c\u56e0\u4e3a\u5b83\u901a\u5e38\u5f88\u96be\u7406\u89e3\u548c\u8c03\u8bd5\uff0c\u5e76\u4e14\u5728\u73b0\u4ee3\u5904\u7406\u5668\u6d41\u6c34\u7ebf\u548c\u7f13\u5b58\u65b9\u6848\u4e0b\u6548\u7387\u4f4e\u4e0b\u3002 Capabilities On a large scale, the ability to treat instructions as data is what makes assemblers , compilers , linkers , loaders , and other automated programming tools possible. It makes \"programs that write programs\" possible.[ 5] This has made a sophisticated self-hosting computing ecosystem flourish around von Neumann architecture machines. NOTE: \u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\uff0c\u5c06\u6307\u4ee4\u89c6\u4e3a\u6570\u636e\u7684\u80fd\u529b\u4f7f\u5f97\u6c47\u7f16\u5668\uff0c\u7f16\u8bd1\u5668\uff0c\u94fe\u63a5\u5668\uff0c\u52a0\u8f7d\u5668\u548c\u5176\u4ed6\u81ea\u52a8\u7f16\u7a0b\u5de5\u5177\u6210\u4e3a\u53ef\u80fd\u3002 \u5b83\u4f7f\u201c\u7f16\u5199\u7a0b\u5e8f\u7684\u7a0b\u5e8f\u201d\u6210\u4e3a\u53ef\u80fd\u3002[5] \u8fd9\u4f7f\u5f97\u590d\u6742\u7684\u81ea\u6258\u7ba1\u8ba1\u7b97\u751f\u6001\u7cfb\u7edf\u5728\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u673a\u5668\u4e2d\u84ec\u52c3\u53d1\u5c55\u3002 Some high level languages leverage the von Neumann architecture by providing an abstract, machine-independent way to manipulate executable code at runtime (e.g., LISP ), or by using runtime information to tune just-in-time compilation (e.g. languages hosted on the Java virtual machine , or languages embedded in web browsers ). On a smaller scale, some repetitive operations such as BITBLT or pixel and vertex shaders can be accelerated on general purpose processors with just-in-time compilation techniques. This is one use of self-modifying code that has remained popular. Development of the stored-program concept The mathematician Alan Turing , who had been alerted to a problem of mathematical logic by the lectures of Max Newman at the University of Cambridge , wrote a paper in 1936 entitled On Computable Numbers, with an Application to the Entscheidungs problem , which was published in the Proceedings of the London Mathematical Society .[ 6] In it he described a hypothetical machine he called a universal computing machine, now known as the \" Universal Turing machine \". The hypothetical machine had an infinite store (memory in today's terminology) that contained both instructions and data. John von Neumann became acquainted with Turing while he was a visiting professor at Cambridge in 1935, and also during Turing's PhD year at the Institute for Advanced Study in Princeton, New Jersey during 1936 \u2013 1937. Whether he knew of Turing's paper of 1936 at that time is not clear. Evolution Through the decades of the 1960s and 1970s computers generally became both smaller and faster, which led to evolutions in their architecture. For example, memory-mapped I/O lets input and output devices be treated the same as memory.[ 24] A single system bus could be used to provide a modular system with lower cost[ clarification needed ]. This is sometimes called a \"streamlining\" of the architecture.[ 25] In subsequent decades, simple microcontrollers would sometimes omit features of the model to lower cost and size. Larger computers added features for higher performance. Single system bus evolution of the architecture Design limitations Von Neumann bottleneck NOTE: \u8bb0\u5f97\u5927\u5b66\u65f6\u5728\u5b66\u4e60**\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406**\u8bfe\u7a0b\u7684\u65f6\u5019\uff0c\u8001\u5e08\u63d0\u51fa\u8fc7\u91cd\u8981\u7684\u89c2\u70b9\uff1a \u201c\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\u201d \u610f\u601d\u662fCPU\u7684ALU\u7684\u8fd0\u7b97\u901f\u5ea6\u662f\u975e\u5e38\u5feb\u7684\uff0c\u76f8\u6bd4\u4e4b\u4e0b\u4ece\u5185\u5b58\u4e2d\u8bfb\u53d6\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff0c\u6240\u4ee5ALU\u5e38\u5e38\u9700\u8981\u7b49\u5f85\uff0c\u8fd9\u5e94\u8be5\u662f\u5f53\u4ee3CPU\u8bbe\u8ba1\u65f6\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u77db\u76fe\u6240\u5728\uff0c\u5404\u79cd\u7f13\u89e3\u8fd9\u4e2a\u77db\u76fe\u7684\u6280\u672f\u4e0d\u65ad\u51fa\u73b0\uff0c\u6bd4\u5982: 1) \u5728Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76845.1.3CPU\u4e2d\u7684\u4e3b\u8981\u5bc4\u5b58\u5668\u7ae0\u8282\u4e2d\u6240\u63cf\u8ff0\u7684**\u6570\u636e\u7f13\u51b2\u5bc4\u5b58\u5668\uff08DR\uff09**\u7684\u4f5c\u7528\uff1a\u8865\u507fCPU\u548c\u5185\u5b58\u3001\u5916\u56f4\u8bbe\u5907\u4e4b\u95f4\u5728\u64cd\u4f5c\u901f\u5ea6\u4e0a\u7684\u5dee\u522b\u3002 \u4e0eVon Neumann bottleneck\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u662f: IO-bound\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u5de5\u7a0bsoftware-engineering\u7684 Software-analysis\\Performance\\Bound \u7ae0\u8282\u3002 The shared bus between the program memory and data memory leads to the von Neumann bottleneck , the limited throughput (data transfer rate) between the central processing unit (CPU) and memory compared to the amount of memory. Because the single bus can only access one of the two classes of memory at a time, throughput is lower than the rate at which the CPU can work. This seriously limits the effective processing speed when the CPU is required to perform minimal processing on large amounts of data. The CPU is continually forced to wait for needed data to move to or from memory. Since CPU speed and memory size have increased much faster than the throughput between them, the bottleneck has become more of a problem, a problem whose severity increases with every new generation of CPU. The von Neumann bottleneck was described by John Backus in his 1977 ACM Turing Award lecture. According to Backus: Surely there must be a less primitive way of making big changes in the store than by pushing vast numbers of words back and forth through the von Neumann bottleneck. Not only is this tube a literal bottleneck for the data traffic of a problem, but, more importantly, it is an intellectual bottleneck that has kept us tied to word-at-a-time thinking instead of encouraging us to think in terms of the larger conceptual units of the task at hand. Thus programming is basically planning and detailing the enormous traffic of words through the von Neumann bottleneck, and much of that traffic concerns not significant data itself, but where to find it.[ 26] [ 27] Mitigations There are several known methods for mitigating the Von Neumann performance bottleneck. For example, the following all can improve performance[ why? ]: Providing a cache between the CPU and the main memory providing separate caches or separate access paths for data and instructions (the so-called Modified Harvard architecture ) using branch predictor algorithms and logic providing a limited CPU stack or other on-chip scratchpad memory to reduce memory access Implementing the CPU and the memory hierarchy as a system on chip , providing greater locality of reference and thus reducing latency and increasing throughput between processor registers and main memory The problem can also be sidestepped somewhat by using parallel computing , using for example the non-uniform memory access (NUMA) architecture\u2014this approach is commonly employed by supercomputers. It is less clear whether the intellectual bottleneck that Backus criticized has changed much since 1977. Backus's proposed solution has not had a major influence.[ citation needed ] Modern functional programming and object-oriented programming are much less geared towards \"pushing vast numbers of words back and forth\" than earlier languages like FORTRAN were, but internally, that is still what computers spend much of their time doing, even highly parallel supercomputers. As of 1996, a database benchmark study found that three out of four CPU cycles were spent waiting for memory. Researchers expect that increasing the number of simultaneous instruction streams with multithreading or single-chip multiprocessing will make this bottleneck even worse.[ 28] In the context of multi-core processors , additional overhead is required to maintain cache coherence between processors and threads. Self-modifying code Aside from the von Neumann bottleneck, program modifications can be quite harmful, either by accident or design. In some simple stored-program computer designs, a malfunctioning program can damage itself, other programs, or the operating system , possibly leading to a computer crash . Memory protection and other forms of access control can usually protect against both accidental and malicious program modification.","title":"Introduction"},{"location":"Computer-architecture/Von-Neumann-architecture/#von#neumann#architecture","text":"","title":"Von Neumann architecture"},{"location":"Computer-architecture/Von-Neumann-architecture/#wikipedia#von#neumann#architecture","text":"See also: Stored-program computer and Universal Turing machine \u00a7 Stored-program computer The von Neumann architecture \u2014also known as the von Neumann model or Princeton architecture \u2014is a computer architecture based on a 1945 description by the mathematician and physicist John von Neumann and others in the First Draft of a Report on the EDVAC .[ 1] That document describes a design architecture for an electronic digital computer with these components: A processing unit that contains an arithmetic logic unit and processor registers NOTE: \u4e0a\u9762\u6240\u8bf4\u7684processing unit\u6307\u7684\u5c31\u662fCPU A control unit that contains an instruction register and program counter Memory that stores data and instructions NOTE: \u8981\u5145\u5206\u7406\u89e3\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u7684\u542b\u4e49\uff0c\u5c31\u9700\u8981\u9605\u8bfbSee also: Stored-program computer and Universal Turing machine \u00a7 Stored-program computer External mass storage Input and output mechanisms[ 1] [ 2] The term \"von Neumann architecture\" has evolved to mean any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus . This is referred to as the von Neumann bottleneck and often limits the performance of the system.[ 3] NOTE: \u672f\u8bed\u201c\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u201d\u5df2\u7ecf\u53d1\u5c55\u4e3a\u4efb\u4f55\u5b58\u50a8\u7a0b\u5e8f\u8ba1\u7b97\u673a\uff0c\u5176\u4e2d\u6307\u4ee4\u83b7\u53d6\u548c\u6570\u636e\u64cd\u4f5c\u4e0d\u80fd\u540c\u65f6\u53d1\u751f\uff0c\u56e0\u4e3a\u5b83\u4eec\u5171\u4eab\u516c\u5171\u603b\u7ebf\u3002 \u8fd9\u88ab\u79f0\u4e3a\u51af\u8bfa\u4f9d\u66fc\u74f6\u9888\uff0c\u5e76\u4e14\u7ecf\u5e38\u9650\u5236\u7cfb\u7edf\u7684\u6027\u80fd The design of a von Neumann architecture machine is simpler than a Harvard architecture machine\u2014which is also a stored-program system but has one dedicated set of address and data buses for reading and writing to memory, and another set of address and data buses to fetch instructions . A stored-program digital computer keeps both program instructions and data in read-write , random-access memory (RAM). Stored-program computers were an advancement over the program-controlled computers of the 1940s, such as the Colossus and the ENIAC . Those were programmed by setting switches and inserting patch cables to route data and control signals between various functional units. The vast majority of modern computers use the same memory for both data and program instructions. The von Neumann vs. Harvard distinction applies to the cache architecture, not the main memory ( split cache architecture ). Von Neumann architecture scheme","title":"wikipedia Von Neumann architecture"},{"location":"Computer-architecture/Von-Neumann-architecture/#history","text":"The earliest computing machines had fixed programs. Some very simple computers still use this design, either for simplicity or training purposes. For example, a desk calculator (in principle) is a fixed program computer. It can do basic mathematics , but it cannot run a word processor or games. Changing the program of a fixed-program machine requires rewiring, restructuring, or redesigning the machine. The earliest computers were not so much \"programmed\" as \"designed\" for a particular task. \"Reprogramming\"\u2014when possible at all\u2014was a laborious process that started with flowcharts and paper notes, followed by detailed engineering designs, and then the often-arduous process of physically rewiring and rebuilding the machine. It could take three weeks to set up and debug a program on ENIAC .[ 4] NOTE: \u6700\u65e9\u7684\u8ba1\u7b97\u673a\u6709\u56fa\u5b9a\u7684\u7a0b\u5e8f\u3002\u4e00\u4e9b\u975e\u5e38\u7b80\u5355\u7684\u8ba1\u7b97\u673a\u4ecd\u7136\u4f7f\u7528\u8fd9\u79cd\u8bbe\u8ba1\uff0c\u65e0\u8bba\u662f\u4e3a\u4e86\u7b80\u5355\u8fd8\u662f\u57f9\u8bad\u3002\u4f8b\u5982\uff0c\u684c\u9762\u8ba1\u7b97\u5668\uff08\u539f\u5219\u4e0a\uff09\u662f\u56fa\u5b9a\u7a0b\u5e8f\u8ba1\u7b97\u673a\u3002\u5b83\u53ef\u4ee5\u505a\u57fa\u7840\u6570\u5b66\uff0c\u4f46\u5b83\u4e0d\u80fd\u8fd0\u884c\u6587\u5b57\u5904\u7406\u5668\u6216\u6e38\u620f\u3002\u66f4\u6539\u56fa\u5b9a\u7a0b\u5e8f\u673a\u5668\u7684\u7a0b\u5e8f\u9700\u8981\u91cd\u65b0\u5e03\u7ebf\uff0c\u91cd\u7ec4\u6216\u91cd\u65b0\u8bbe\u8ba1\u673a\u5668\u3002\u6700\u65e9\u7684\u8ba1\u7b97\u673a\u5e76\u6ca1\u6709\u4e3a\u7279\u5b9a\u4efb\u52a1\u201c\u7f16\u7a0b\u201d\u4e3a\u201c\u8bbe\u8ba1\u201d\u3002 \u201c\u91cd\u65b0\u7f16\u7a0b\u201d - \u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b - \u662f\u4e00\u4e2a\u8270\u82e6\u7684\u8fc7\u7a0b\uff0c\u4ece\u6d41\u7a0b\u56fe\u548c\u7eb8\u8d28\u7b14\u8bb0\u5f00\u59cb\uff0c\u7136\u540e\u662f\u8be6\u7ec6\u7684\u5de5\u7a0b\u8bbe\u8ba1\uff0c\u7136\u540e\u662f\u7ecf\u5e38\u8270\u82e6\u7684\u7269\u7406\u91cd\u65b0\u5e03\u7ebf\u548c\u91cd\u5efa\u673a\u5668\u7684\u8fc7\u7a0b\u3002\u5728ENIAC\u4e0a\u8bbe\u7f6e\u548c\u8c03\u8bd5\u7a0b\u5e8f\u53ef\u80fd\u9700\u8981\u4e09\u5468\u65f6\u95f4\u3002[4] With the proposal of the stored-program computer , this changed. A stored-program computer includes, by design, an instruction set , and can store in memory a set of instructions (a program ) that details the computation . NOTE: \u968f\u7740\u5b58\u50a8\u7a0b\u5e8f\u8ba1\u7b97\u673a\u7684\u63d0\u8bae\uff0c\u8fd9\u6539\u53d8\u4e86\u3002\u5b58\u50a8\u7a0b\u5e8f\u8ba1\u7b97\u673a\u901a\u8fc7\u8bbe\u8ba1\u5305\u62ec\u6307\u4ee4\u96c6\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5b58\u50a8\u5668\u4e2d\u5b58\u50a8\u8be6\u7ec6\u8bf4\u660e\u8ba1\u7b97\u7684\u4e00\u7ec4\u6307\u4ee4\uff08\u7a0b\u5e8f\uff09\u3002 A stored-program design also allows for self-modifying code . One early motivation for such a facility was the need for a program to increment or otherwise modify the address portion of instructions, which operators had to do manually in early designs. This became less important when index registers and indirect addressing became usual features of machine architecture. Another use was to embed frequently used data in the instruction stream using immediate addressing . Self-modifying code has largely fallen out of favor, since it is usually hard to understand and debug , as well as being inefficient under modern processor pipelining and caching schemes. NOTE: \u5b58\u50a8\u7a0b\u5e8f\u8bbe\u8ba1\u8fd8\u5141\u8bb8\u81ea\u4fee\u6539\u4ee3\u7801\u3002\u8fd9\u79cd\u8bbe\u65bd\u7684\u4e00\u4e2a\u65e9\u671f\u52a8\u673a\u662f\u9700\u8981\u4e00\u4e2a\u7a0b\u5e8f\u6765\u589e\u52a0\u6216\u4ee5\u5176\u4ed6\u65b9\u5f0f\u4fee\u6539\u6307\u4ee4\u7684\u5730\u5740\u90e8\u5206\uff0c\u64cd\u4f5c\u5458\u5fc5\u987b\u5728\u65e9\u671f\u8bbe\u8ba1\u4e2d\u624b\u52a8\u5b8c\u6210\u3002\u5f53\u7d22\u5f15\u5bc4\u5b58\u5668\u548c\u95f4\u63a5\u5bfb\u5740\u6210\u4e3a\u673a\u5668\u67b6\u6784\u7684\u5e38\u7528\u529f\u80fd\u65f6\uff0c\u8fd9\u53d8\u5f97\u4e0d\u90a3\u4e48\u91cd\u8981\u4e86\u3002\u53e6\u4e00\u79cd\u7528\u9014\u662f\u4f7f\u7528\u7acb\u5373\u5bfb\u5740\u5c06\u7ecf\u5e38\u4f7f\u7528\u7684\u6570\u636e\u5d4c\u5165\u6307\u4ee4\u6d41\u4e2d\u3002\u81ea\u4fee\u6539\u4ee3\u7801\u5df2\u5927\u90e8\u5206\u5931\u5ba0\uff0c\u56e0\u4e3a\u5b83\u901a\u5e38\u5f88\u96be\u7406\u89e3\u548c\u8c03\u8bd5\uff0c\u5e76\u4e14\u5728\u73b0\u4ee3\u5904\u7406\u5668\u6d41\u6c34\u7ebf\u548c\u7f13\u5b58\u65b9\u6848\u4e0b\u6548\u7387\u4f4e\u4e0b\u3002","title":"History"},{"location":"Computer-architecture/Von-Neumann-architecture/#capabilities","text":"On a large scale, the ability to treat instructions as data is what makes assemblers , compilers , linkers , loaders , and other automated programming tools possible. It makes \"programs that write programs\" possible.[ 5] This has made a sophisticated self-hosting computing ecosystem flourish around von Neumann architecture machines. NOTE: \u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\uff0c\u5c06\u6307\u4ee4\u89c6\u4e3a\u6570\u636e\u7684\u80fd\u529b\u4f7f\u5f97\u6c47\u7f16\u5668\uff0c\u7f16\u8bd1\u5668\uff0c\u94fe\u63a5\u5668\uff0c\u52a0\u8f7d\u5668\u548c\u5176\u4ed6\u81ea\u52a8\u7f16\u7a0b\u5de5\u5177\u6210\u4e3a\u53ef\u80fd\u3002 \u5b83\u4f7f\u201c\u7f16\u5199\u7a0b\u5e8f\u7684\u7a0b\u5e8f\u201d\u6210\u4e3a\u53ef\u80fd\u3002[5] \u8fd9\u4f7f\u5f97\u590d\u6742\u7684\u81ea\u6258\u7ba1\u8ba1\u7b97\u751f\u6001\u7cfb\u7edf\u5728\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u673a\u5668\u4e2d\u84ec\u52c3\u53d1\u5c55\u3002 Some high level languages leverage the von Neumann architecture by providing an abstract, machine-independent way to manipulate executable code at runtime (e.g., LISP ), or by using runtime information to tune just-in-time compilation (e.g. languages hosted on the Java virtual machine , or languages embedded in web browsers ). On a smaller scale, some repetitive operations such as BITBLT or pixel and vertex shaders can be accelerated on general purpose processors with just-in-time compilation techniques. This is one use of self-modifying code that has remained popular.","title":"Capabilities"},{"location":"Computer-architecture/Von-Neumann-architecture/#development#of#the#stored-program#concept","text":"The mathematician Alan Turing , who had been alerted to a problem of mathematical logic by the lectures of Max Newman at the University of Cambridge , wrote a paper in 1936 entitled On Computable Numbers, with an Application to the Entscheidungs problem , which was published in the Proceedings of the London Mathematical Society .[ 6] In it he described a hypothetical machine he called a universal computing machine, now known as the \" Universal Turing machine \". The hypothetical machine had an infinite store (memory in today's terminology) that contained both instructions and data. John von Neumann became acquainted with Turing while he was a visiting professor at Cambridge in 1935, and also during Turing's PhD year at the Institute for Advanced Study in Princeton, New Jersey during 1936 \u2013 1937. Whether he knew of Turing's paper of 1936 at that time is not clear.","title":"Development of the stored-program concept"},{"location":"Computer-architecture/Von-Neumann-architecture/#evolution","text":"Through the decades of the 1960s and 1970s computers generally became both smaller and faster, which led to evolutions in their architecture. For example, memory-mapped I/O lets input and output devices be treated the same as memory.[ 24] A single system bus could be used to provide a modular system with lower cost[ clarification needed ]. This is sometimes called a \"streamlining\" of the architecture.[ 25] In subsequent decades, simple microcontrollers would sometimes omit features of the model to lower cost and size. Larger computers added features for higher performance. Single system bus evolution of the architecture","title":"Evolution"},{"location":"Computer-architecture/Von-Neumann-architecture/#design#limitations","text":"","title":"Design limitations"},{"location":"Computer-architecture/Von-Neumann-architecture/#von#neumann#bottleneck","text":"NOTE: \u8bb0\u5f97\u5927\u5b66\u65f6\u5728\u5b66\u4e60**\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406**\u8bfe\u7a0b\u7684\u65f6\u5019\uff0c\u8001\u5e08\u63d0\u51fa\u8fc7\u91cd\u8981\u7684\u89c2\u70b9\uff1a \u201c\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\u201d \u610f\u601d\u662fCPU\u7684ALU\u7684\u8fd0\u7b97\u901f\u5ea6\u662f\u975e\u5e38\u5feb\u7684\uff0c\u76f8\u6bd4\u4e4b\u4e0b\u4ece\u5185\u5b58\u4e2d\u8bfb\u53d6\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff0c\u6240\u4ee5ALU\u5e38\u5e38\u9700\u8981\u7b49\u5f85\uff0c\u8fd9\u5e94\u8be5\u662f\u5f53\u4ee3CPU\u8bbe\u8ba1\u65f6\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u77db\u76fe\u6240\u5728\uff0c\u5404\u79cd\u7f13\u89e3\u8fd9\u4e2a\u77db\u76fe\u7684\u6280\u672f\u4e0d\u65ad\u51fa\u73b0\uff0c\u6bd4\u5982: 1) \u5728Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76845.1.3CPU\u4e2d\u7684\u4e3b\u8981\u5bc4\u5b58\u5668\u7ae0\u8282\u4e2d\u6240\u63cf\u8ff0\u7684**\u6570\u636e\u7f13\u51b2\u5bc4\u5b58\u5668\uff08DR\uff09**\u7684\u4f5c\u7528\uff1a\u8865\u507fCPU\u548c\u5185\u5b58\u3001\u5916\u56f4\u8bbe\u5907\u4e4b\u95f4\u5728\u64cd\u4f5c\u901f\u5ea6\u4e0a\u7684\u5dee\u522b\u3002 \u4e0eVon Neumann bottleneck\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u662f: IO-bound\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u5de5\u7a0bsoftware-engineering\u7684 Software-analysis\\Performance\\Bound \u7ae0\u8282\u3002 The shared bus between the program memory and data memory leads to the von Neumann bottleneck , the limited throughput (data transfer rate) between the central processing unit (CPU) and memory compared to the amount of memory. Because the single bus can only access one of the two classes of memory at a time, throughput is lower than the rate at which the CPU can work. This seriously limits the effective processing speed when the CPU is required to perform minimal processing on large amounts of data. The CPU is continually forced to wait for needed data to move to or from memory. Since CPU speed and memory size have increased much faster than the throughput between them, the bottleneck has become more of a problem, a problem whose severity increases with every new generation of CPU. The von Neumann bottleneck was described by John Backus in his 1977 ACM Turing Award lecture. According to Backus: Surely there must be a less primitive way of making big changes in the store than by pushing vast numbers of words back and forth through the von Neumann bottleneck. Not only is this tube a literal bottleneck for the data traffic of a problem, but, more importantly, it is an intellectual bottleneck that has kept us tied to word-at-a-time thinking instead of encouraging us to think in terms of the larger conceptual units of the task at hand. Thus programming is basically planning and detailing the enormous traffic of words through the von Neumann bottleneck, and much of that traffic concerns not significant data itself, but where to find it.[ 26] [ 27]","title":"Von Neumann bottleneck"},{"location":"Computer-architecture/Von-Neumann-architecture/#mitigations","text":"There are several known methods for mitigating the Von Neumann performance bottleneck. For example, the following all can improve performance[ why? ]: Providing a cache between the CPU and the main memory providing separate caches or separate access paths for data and instructions (the so-called Modified Harvard architecture ) using branch predictor algorithms and logic providing a limited CPU stack or other on-chip scratchpad memory to reduce memory access Implementing the CPU and the memory hierarchy as a system on chip , providing greater locality of reference and thus reducing latency and increasing throughput between processor registers and main memory The problem can also be sidestepped somewhat by using parallel computing , using for example the non-uniform memory access (NUMA) architecture\u2014this approach is commonly employed by supercomputers. It is less clear whether the intellectual bottleneck that Backus criticized has changed much since 1977. Backus's proposed solution has not had a major influence.[ citation needed ] Modern functional programming and object-oriented programming are much less geared towards \"pushing vast numbers of words back and forth\" than earlier languages like FORTRAN were, but internally, that is still what computers spend much of their time doing, even highly parallel supercomputers. As of 1996, a database benchmark study found that three out of four CPU cycles were spent waiting for memory. Researchers expect that increasing the number of simultaneous instruction streams with multithreading or single-chip multiprocessing will make this bottleneck even worse.[ 28] In the context of multi-core processors , additional overhead is required to maintain cache coherence between processors and threads.","title":"Mitigations"},{"location":"Computer-architecture/Von-Neumann-architecture/#self-modifying#code","text":"Aside from the von Neumann bottleneck, program modifications can be quite harmful, either by accident or design. In some simple stored-program computer designs, a malfunctioning program can damage itself, other programs, or the operating system , possibly leading to a computer crash . Memory protection and other forms of access control can usually protect against both accidental and malicious program modification.","title":"Self-modifying code"},{"location":"Computer-architecture/Von-Neumann-architecture/Von-Neumann-bottleneck/","text":"Von Neumann bottleneck Von Neumann bottleneck: \"\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\" \u8bb0\u5f97\u5927\u5b66\u65f6\u5728\u5b66\u4e60**\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406**\u8bfe\u7a0b\u7684\u65f6\u5019\uff0c\u8001\u5e08\u63d0\u51fa\u8fc7\u91cd\u8981\u7684\u89c2\u70b9\uff1a \u201c\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\u201d \u610f\u601d\u662fCPU\u7684ALU\u7684\u8fd0\u7b97\u901f\u5ea6\u662f\u975e\u5e38\u5feb\u7684\uff0c\u76f8\u6bd4\u4e4b\u4e0b\u4ece\u5185\u5b58\u4e2d\u8bfb\u53d6\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff0c\u6240\u4ee5ALU\u5e38\u5e38\u9700\u8981\u7b49\u5f85\uff0c\u8fd9\u5e94\u8be5\u662f\u5f53\u4ee3CPU\u8bbe\u8ba1\u65f6\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u77db\u76fe\u6240\u5728\uff0c\u5404\u79cd\u7f13\u89e3\u8fd9\u4e2a\u77db\u76fe\u7684\u6280\u672f\u4e0d\u65ad\u51fa\u73b0\uff0c\u6bd4\u5982: 1) \u5728Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76845.1.3CPU\u4e2d\u7684\u4e3b\u8981\u5bc4\u5b58\u5668\u7ae0\u8282\u4e2d\u6240\u63cf\u8ff0\u7684**\u6570\u636e\u7f13\u51b2\u5bc4\u5b58\u5668\uff08DR\uff09**\u7684\u4f5c\u7528\uff1a\u8865\u507fCPU\u548c\u5185\u5b58\u3001\u5916\u56f4\u8bbe\u5907\u4e4b\u95f4\u5728\u64cd\u4f5c\u901f\u5ea6\u4e0a\u7684\u5dee\u522b\u3002 \u672c\u7ae0\u4e3b\u8981\u4ecb\u7ecdCPU\u8bbf\u95eememory\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u8bfb\u8005\u8bb0\u4f4f\u4e0a\u9762\u7684\u8fd9\u4e2a\u89c2\u70b9\uff0c\u5e94\u8be5\u80fd\u591f\u6709\u52a9\u4e8e\u7406\u89e3\u4f7f\u7528\u4e00\u4e9b\u6280\u672f\u7684\u76ee\u7684\u6240\u5728\u3002 NOTE: \u8fd9\u5176\u5b9e\u662f von Neumann bottleneck \uff0c\u53c2\u89c1 Computer-architecture\\Von-Neumann-architecture \u7ae0\u8282\u3002 wikipedia Von Neumann architecture # Design limitations # Von Neumann bottleneck NOTE: \u8bb0\u5f97\u5927\u5b66\u65f6\u5728\u5b66\u4e60**\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406**\u8bfe\u7a0b\u7684\u65f6\u5019\uff0c\u8001\u5e08\u63d0\u51fa\u8fc7\u91cd\u8981\u7684\u89c2\u70b9\uff1a \u201c\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\u201d \u610f\u601d\u662fCPU\u7684ALU\u7684\u8fd0\u7b97\u901f\u5ea6\u662f\u975e\u5e38\u5feb\u7684\uff0c\u76f8\u6bd4\u4e4b\u4e0b\u4ece\u5185\u5b58\u4e2d\u8bfb\u53d6\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff0c\u6240\u4ee5ALU\u5e38\u5e38\u9700\u8981\u7b49\u5f85\uff0c\u8fd9\u5e94\u8be5\u662f\u5f53\u4ee3CPU\u8bbe\u8ba1\u65f6\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u77db\u76fe\u6240\u5728\uff0c\u5404\u79cd\u7f13\u89e3\u8fd9\u4e2a\u77db\u76fe\u7684\u6280\u672f\u4e0d\u65ad\u51fa\u73b0\uff0c\u6bd4\u5982: 1) \u5728Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76845.1.3CPU\u4e2d\u7684\u4e3b\u8981\u5bc4\u5b58\u5668\u7ae0\u8282\u4e2d\u6240\u63cf\u8ff0\u7684**\u6570\u636e\u7f13\u51b2\u5bc4\u5b58\u5668\uff08DR\uff09**\u7684\u4f5c\u7528\uff1a\u8865\u507fCPU\u548c\u5185\u5b58\u3001\u5916\u56f4\u8bbe\u5907\u4e4b\u95f4\u5728\u64cd\u4f5c\u901f\u5ea6\u4e0a\u7684\u5dee\u522b\u3002 \u4e0eVon Neumann bottleneck\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u662f: IO-bound\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u5de5\u7a0bsoftware-engineering\u7684 Software-analysis\\Performance\\Bound \u7ae0\u8282\u3002 The shared bus between the program memory and data memory leads to the von Neumann bottleneck , the limited throughput (data transfer rate) between the central processing unit (CPU) and memory compared to the amount of memory. Because the single bus can only access one of the two classes of memory at a time, throughput is lower than the rate at which the CPU can work. This seriously limits the effective processing speed when the CPU is required to perform minimal processing on large amounts of data. The CPU is continually forced to wait for needed data to move to or from memory. Since CPU speed and memory size have increased much faster than the throughput between them, the bottleneck has become more of a problem, a problem whose severity increases with every new generation of CPU. The von Neumann bottleneck was described by John Backus in his 1977 ACM Turing Award lecture. According to Backus: Surely there must be a less primitive way of making big changes in the store than by pushing vast numbers of words back and forth through the von Neumann bottleneck. Not only is this tube a literal bottleneck for the data traffic of a problem, but, more importantly, it is an intellectual bottleneck that has kept us tied to word-at-a-time thinking instead of encouraging us to think in terms of the larger conceptual units of the task at hand. Thus programming is basically planning and detailing the enormous traffic of words through the von Neumann bottleneck, and much of that traffic concerns not significant data itself, but where to find it.[ 26] [ 27]","title":"Introduction"},{"location":"Computer-architecture/Von-Neumann-architecture/Von-Neumann-bottleneck/#von#neumann#bottleneck","text":"","title":"Von Neumann bottleneck"},{"location":"Computer-architecture/Von-Neumann-architecture/Von-Neumann-bottleneck/#von#neumann#bottleneck#cpu","text":"\u8bb0\u5f97\u5927\u5b66\u65f6\u5728\u5b66\u4e60**\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406**\u8bfe\u7a0b\u7684\u65f6\u5019\uff0c\u8001\u5e08\u63d0\u51fa\u8fc7\u91cd\u8981\u7684\u89c2\u70b9\uff1a \u201c\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\u201d \u610f\u601d\u662fCPU\u7684ALU\u7684\u8fd0\u7b97\u901f\u5ea6\u662f\u975e\u5e38\u5feb\u7684\uff0c\u76f8\u6bd4\u4e4b\u4e0b\u4ece\u5185\u5b58\u4e2d\u8bfb\u53d6\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff0c\u6240\u4ee5ALU\u5e38\u5e38\u9700\u8981\u7b49\u5f85\uff0c\u8fd9\u5e94\u8be5\u662f\u5f53\u4ee3CPU\u8bbe\u8ba1\u65f6\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u77db\u76fe\u6240\u5728\uff0c\u5404\u79cd\u7f13\u89e3\u8fd9\u4e2a\u77db\u76fe\u7684\u6280\u672f\u4e0d\u65ad\u51fa\u73b0\uff0c\u6bd4\u5982: 1) \u5728Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76845.1.3CPU\u4e2d\u7684\u4e3b\u8981\u5bc4\u5b58\u5668\u7ae0\u8282\u4e2d\u6240\u63cf\u8ff0\u7684**\u6570\u636e\u7f13\u51b2\u5bc4\u5b58\u5668\uff08DR\uff09**\u7684\u4f5c\u7528\uff1a\u8865\u507fCPU\u548c\u5185\u5b58\u3001\u5916\u56f4\u8bbe\u5907\u4e4b\u95f4\u5728\u64cd\u4f5c\u901f\u5ea6\u4e0a\u7684\u5dee\u522b\u3002 \u672c\u7ae0\u4e3b\u8981\u4ecb\u7ecdCPU\u8bbf\u95eememory\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u8bfb\u8005\u8bb0\u4f4f\u4e0a\u9762\u7684\u8fd9\u4e2a\u89c2\u70b9\uff0c\u5e94\u8be5\u80fd\u591f\u6709\u52a9\u4e8e\u7406\u89e3\u4f7f\u7528\u4e00\u4e9b\u6280\u672f\u7684\u76ee\u7684\u6240\u5728\u3002 NOTE: \u8fd9\u5176\u5b9e\u662f von Neumann bottleneck \uff0c\u53c2\u89c1 Computer-architecture\\Von-Neumann-architecture \u7ae0\u8282\u3002","title":"Von Neumann bottleneck: \"\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\""},{"location":"Computer-architecture/Von-Neumann-architecture/Von-Neumann-bottleneck/#wikipedia#von#neumann#architecture#design#limitations#von#neumann#bottleneck","text":"NOTE: \u8bb0\u5f97\u5927\u5b66\u65f6\u5728\u5b66\u4e60**\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406**\u8bfe\u7a0b\u7684\u65f6\u5019\uff0c\u8001\u5e08\u63d0\u51fa\u8fc7\u91cd\u8981\u7684\u89c2\u70b9\uff1a \u201c\u9650\u5236CPU\u901f\u5ea6\u7684\u662f\u4ece\u5185\u5b58\u4e2d\u8bfb\u5199\u6570\u636e\u201d \u610f\u601d\u662fCPU\u7684ALU\u7684\u8fd0\u7b97\u901f\u5ea6\u662f\u975e\u5e38\u5feb\u7684\uff0c\u76f8\u6bd4\u4e4b\u4e0b\u4ece\u5185\u5b58\u4e2d\u8bfb\u53d6\u662f\u6bd4\u8f83\u7f13\u6162\u7684\uff0c\u6240\u4ee5ALU\u5e38\u5e38\u9700\u8981\u7b49\u5f85\uff0c\u8fd9\u5e94\u8be5\u662f\u5f53\u4ee3CPU\u8bbe\u8ba1\u65f6\u9700\u8981\u8003\u8651\u7684\u4e00\u4e2a\u77db\u76fe\u6240\u5728\uff0c\u5404\u79cd\u7f13\u89e3\u8fd9\u4e2a\u77db\u76fe\u7684\u6280\u672f\u4e0d\u65ad\u51fa\u73b0\uff0c\u6bd4\u5982: 1) \u5728Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e\u76845.1.3CPU\u4e2d\u7684\u4e3b\u8981\u5bc4\u5b58\u5668\u7ae0\u8282\u4e2d\u6240\u63cf\u8ff0\u7684**\u6570\u636e\u7f13\u51b2\u5bc4\u5b58\u5668\uff08DR\uff09**\u7684\u4f5c\u7528\uff1a\u8865\u507fCPU\u548c\u5185\u5b58\u3001\u5916\u56f4\u8bbe\u5907\u4e4b\u95f4\u5728\u64cd\u4f5c\u901f\u5ea6\u4e0a\u7684\u5dee\u522b\u3002 \u4e0eVon Neumann bottleneck\u76f8\u5173\u7684\u4e00\u4e2a\u95ee\u9898\u662f: IO-bound\uff0c\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1\u5de5\u7a0bsoftware-engineering\u7684 Software-analysis\\Performance\\Bound \u7ae0\u8282\u3002 The shared bus between the program memory and data memory leads to the von Neumann bottleneck , the limited throughput (data transfer rate) between the central processing unit (CPU) and memory compared to the amount of memory. Because the single bus can only access one of the two classes of memory at a time, throughput is lower than the rate at which the CPU can work. This seriously limits the effective processing speed when the CPU is required to perform minimal processing on large amounts of data. The CPU is continually forced to wait for needed data to move to or from memory. Since CPU speed and memory size have increased much faster than the throughput between them, the bottleneck has become more of a problem, a problem whose severity increases with every new generation of CPU. The von Neumann bottleneck was described by John Backus in his 1977 ACM Turing Award lecture. According to Backus: Surely there must be a less primitive way of making big changes in the store than by pushing vast numbers of words back and forth through the von Neumann bottleneck. Not only is this tube a literal bottleneck for the data traffic of a problem, but, more importantly, it is an intellectual bottleneck that has kept us tied to word-at-a-time thinking instead of encouraging us to think in terms of the larger conceptual units of the task at hand. Thus programming is basically planning and detailing the enormous traffic of words through the von Neumann bottleneck, and much of that traffic concerns not significant data itself, but where to find it.[ 26] [ 27]","title":"wikipedia Von Neumann architecture # Design limitations # Von Neumann bottleneck"},{"location":"Expert-Ulrich-Drepper/","text":"Ulrich Drepper","title":"Introduction"},{"location":"Expert-Ulrich-Drepper/#ulrich#drepper","text":"","title":"Ulrich Drepper"},{"location":"Memory/Computer-memory/","text":"Computer memory \u672c\u6587\u9610\u8ff0computer\u7684memory\u7684\u67b6\u6784\u3001\u7ec4\u6210\u3002 \u7ef4\u57fa\u767e\u79d1 Computer memory Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e NOTE: 1\u3001Hierarchy of computer memory 2\u3001\u6784\u5efa\u8d77\u5b8c\u6574\u7684\u8ba4\u77e5","title":"Computer-memory"},{"location":"Memory/Computer-memory/#computer#memory","text":"\u672c\u6587\u9610\u8ff0computer\u7684memory\u7684\u67b6\u6784\u3001\u7ec4\u6210\u3002","title":"Computer memory"},{"location":"Memory/Computer-memory/#computer#memory_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Computer memory"},{"location":"Memory/Computer-memory/#book--","text":"NOTE: 1\u3001Hierarchy of computer memory 2\u3001\u6784\u5efa\u8d77\u5b8c\u6574\u7684\u8ba4\u77e5","title":"Book-\u8ba1\u7b97\u673a\u7ec4\u6210\u539f\u7406-\u79d1\u5b66\u51fa\u7248\u793e"},{"location":"Memory/Memory-address/","text":"wikipedia Memory address In computing , a memory address is a reference to a specific memory location used at various levels by software and hardware . Memory addresses are fixed-length sequences of digits conventionally displayed and manipulated as unsigned integers .[ 1] Such numerical semantic bases\uff08\u57fa\u4e8e\uff09 itself upon features of CPU (such as the instruction pointer and incremental address registers ), as well upon use of the memory like an array endorsed by various programming languages . Types of memory addresses Physical addresses A digital computer 's main memory consists of many memory locations. Each memory location has a physical address which is a code . The CPU (or other device) can use the code to access the corresponding memory location. Generally only system software , i.e. the BIOS , operating systems, and some specialized utility programs (e.g., memory testers ), address(\u5b9a\u4f4d) physical memory using machine code operands or processor registers , instructing the CPU to direct a hardware device , called the memory controller , to use the memory bus or system bus , or separate control , address and data busses , to execute the program's commands. The memory controllers' bus consists of a number of parallel lines, each represented by a binary digit (bit). The width of the bus, and thus the number of addressable storage units, and the number of bits in each unit, varies among computers. Logical addresses A computer program uses memory addresses to execute machine code , and to store and retrieve data . In early computers logical and physical addresses corresponded, but since the introduction of virtual memory most application programs do not have a knowledge of physical addresses. Rather, they address logical addresses , or virtual addresses , using the computer's memory management unit and operating system memory mapping; see below . \u200b Unit of address resolution See also: Word (computer architecture) and Binary prefix \u00a7 Main memory Most modern computers are byte-addressable . Each address identifies a single byte ( eight bits ) of storage. Data larger than a single byte may be stored in a sequence of consecutive addresses. There exist word-addressable computers, where the minimal addressable storage unit is exactly the processor's word . For example, the Data General Nova minicomputer , and the Texas Instruments TMS9900 and National Semiconductor IMP-16 microcomputers used 16 bit words , and there were many 36-bit mainframe computers (e.g., PDP-10 ) which used 18-bit word addressing , not byte addressing , giving an address space of 218 36-bit words, approximately 1 megabyte of storage. The efficiency of addressing of memory depends on the bit size of the bus used for addresses \u2013 the more bits used, the more addresses are available to the computer. For example, an 8-bit-byte-addressable machine with a 20-bit address bus (e.g. Intel 8086 ) can address 220 (1,048,576) memory locations, or one MiB of memory, while a 32-bit bus (e.g. Intel 80386 ) addresses 232 (4,294,967,296) locations, or a 4 GiB address space. In contrast, a 36-bit word-addressable machine with an 18-bit address bus addresses only 218 (262,144) 36-bit locations (9,437,184 bits), equivalent to 1,179,648 8-bit bytes, or 1152 KB, or 1.125 MiB \u2014slightly more than the 8086. Some older computers ( decimal computers ), were decimal digit-addressable . For example, each address in the IBM 1620 's magnetic-core memory identified a single six bit binary-coded decimal digit, consisting of a parity bit , flag bit and four numerical bits. The 1620 used 5-digit decimal addresses, so in theory the highest possible address was 99,999. In practice, the CPU supported 20,000 memory locations, and up to two optional external memory units could be added, each supporting 20,000 addresses, for a total of 60,000 (00000\u201359999). Contents of each memory location See also: binary data Each memory location in a stored-program computer holds a binary number or decimal number of some sort . Its interpretation, as data of some data type or as an instruction, and use are determined by the instructions which retrieve and manipulate it. Some early programmers combined instructions and data in words as a way to save memory, when it was expensive: The Manchester Mark 1 had space in its 40-bit words to store little bits of data \u2013 its processor ignored a small section in the middle of a word \u2013 and that was often exploited as extra data storage.[ citation needed ] Self-replicating programs such as viruses treat themselves sometimes as data and sometimes as instructions. Self-modifying code is generally deprecated nowadays, as it makes testing and maintenance disproportionally difficult to the saving of a few bytes, and can also give incorrect results because of the compiler or processor's assumptions about the machine's state , but is still sometimes used deliberately, with great care. Address space in application programming In modern multitasking environment, an application process usually has in its address space (or spaces) chunks of memory of following types: Machine code , including: program's own code (historically known as code segment or text segment ); shared libraries . Data , including: initialized data ( data segment ); uninitialized (but allocated) variables; run-time stack ; heap ; shared memory and memory mapped files . Some parts of address space may be not mapped at all. NOTE: address space in application\u5c06\u6307\u4ee4\uff08machine code\uff09\u548cdata\u5206\u5f00\u6765\u8fdb\u884c\u5b58\u50a8\uff0c\u4f46\u662f\u5728\u5e95\u5c42\uff0c\u662f\u4e0d\u533a\u5206\u7684\uff1b Addressing schemes Main article: Addressing mode A computer program can access an address given explicitly \u2013 in low-level programming this is usually called an absolute address , or sometimes a specific address , and is known as pointer data type in higher-level languages. But a program can also use relative address which specifies a location in relation to somewhere else (the base address ). There are many more indirect addressing modes . Mapping logical addresses to physical and virtual memory also adds several levels of indirection; see below.","title":"Memory-address"},{"location":"Memory/Memory-address/#wikipedia#memory#address","text":"In computing , a memory address is a reference to a specific memory location used at various levels by software and hardware . Memory addresses are fixed-length sequences of digits conventionally displayed and manipulated as unsigned integers .[ 1] Such numerical semantic bases\uff08\u57fa\u4e8e\uff09 itself upon features of CPU (such as the instruction pointer and incremental address registers ), as well upon use of the memory like an array endorsed by various programming languages .","title":"wikipedia Memory address"},{"location":"Memory/Memory-address/#types#of#memory#addresses","text":"","title":"Types of memory addresses"},{"location":"Memory/Memory-address/#physical#addresses","text":"A digital computer 's main memory consists of many memory locations. Each memory location has a physical address which is a code . The CPU (or other device) can use the code to access the corresponding memory location. Generally only system software , i.e. the BIOS , operating systems, and some specialized utility programs (e.g., memory testers ), address(\u5b9a\u4f4d) physical memory using machine code operands or processor registers , instructing the CPU to direct a hardware device , called the memory controller , to use the memory bus or system bus , or separate control , address and data busses , to execute the program's commands. The memory controllers' bus consists of a number of parallel lines, each represented by a binary digit (bit). The width of the bus, and thus the number of addressable storage units, and the number of bits in each unit, varies among computers.","title":"Physical addresses"},{"location":"Memory/Memory-address/#logical#addresses","text":"A computer program uses memory addresses to execute machine code , and to store and retrieve data . In early computers logical and physical addresses corresponded, but since the introduction of virtual memory most application programs do not have a knowledge of physical addresses. Rather, they address logical addresses , or virtual addresses , using the computer's memory management unit and operating system memory mapping; see below . \u200b","title":"Logical addresses"},{"location":"Memory/Memory-address/#unit#of#address#resolution","text":"See also: Word (computer architecture) and Binary prefix \u00a7 Main memory Most modern computers are byte-addressable . Each address identifies a single byte ( eight bits ) of storage. Data larger than a single byte may be stored in a sequence of consecutive addresses. There exist word-addressable computers, where the minimal addressable storage unit is exactly the processor's word . For example, the Data General Nova minicomputer , and the Texas Instruments TMS9900 and National Semiconductor IMP-16 microcomputers used 16 bit words , and there were many 36-bit mainframe computers (e.g., PDP-10 ) which used 18-bit word addressing , not byte addressing , giving an address space of 218 36-bit words, approximately 1 megabyte of storage. The efficiency of addressing of memory depends on the bit size of the bus used for addresses \u2013 the more bits used, the more addresses are available to the computer. For example, an 8-bit-byte-addressable machine with a 20-bit address bus (e.g. Intel 8086 ) can address 220 (1,048,576) memory locations, or one MiB of memory, while a 32-bit bus (e.g. Intel 80386 ) addresses 232 (4,294,967,296) locations, or a 4 GiB address space. In contrast, a 36-bit word-addressable machine with an 18-bit address bus addresses only 218 (262,144) 36-bit locations (9,437,184 bits), equivalent to 1,179,648 8-bit bytes, or 1152 KB, or 1.125 MiB \u2014slightly more than the 8086. Some older computers ( decimal computers ), were decimal digit-addressable . For example, each address in the IBM 1620 's magnetic-core memory identified a single six bit binary-coded decimal digit, consisting of a parity bit , flag bit and four numerical bits. The 1620 used 5-digit decimal addresses, so in theory the highest possible address was 99,999. In practice, the CPU supported 20,000 memory locations, and up to two optional external memory units could be added, each supporting 20,000 addresses, for a total of 60,000 (00000\u201359999).","title":"Unit of address resolution"},{"location":"Memory/Memory-address/#contents#of#each#memory#location","text":"See also: binary data Each memory location in a stored-program computer holds a binary number or decimal number of some sort . Its interpretation, as data of some data type or as an instruction, and use are determined by the instructions which retrieve and manipulate it. Some early programmers combined instructions and data in words as a way to save memory, when it was expensive: The Manchester Mark 1 had space in its 40-bit words to store little bits of data \u2013 its processor ignored a small section in the middle of a word \u2013 and that was often exploited as extra data storage.[ citation needed ] Self-replicating programs such as viruses treat themselves sometimes as data and sometimes as instructions. Self-modifying code is generally deprecated nowadays, as it makes testing and maintenance disproportionally difficult to the saving of a few bytes, and can also give incorrect results because of the compiler or processor's assumptions about the machine's state , but is still sometimes used deliberately, with great care.","title":"Contents of each memory location"},{"location":"Memory/Memory-address/#address#space#in#application#programming","text":"In modern multitasking environment, an application process usually has in its address space (or spaces) chunks of memory of following types: Machine code , including: program's own code (historically known as code segment or text segment ); shared libraries . Data , including: initialized data ( data segment ); uninitialized (but allocated) variables; run-time stack ; heap ; shared memory and memory mapped files . Some parts of address space may be not mapped at all. NOTE: address space in application\u5c06\u6307\u4ee4\uff08machine code\uff09\u548cdata\u5206\u5f00\u6765\u8fdb\u884c\u5b58\u50a8\uff0c\u4f46\u662f\u5728\u5e95\u5c42\uff0c\u662f\u4e0d\u533a\u5206\u7684\uff1b","title":"Address space in application programming"},{"location":"Memory/Memory-address/#addressing#schemes","text":"Main article: Addressing mode A computer program can access an address given explicitly \u2013 in low-level programming this is usually called an absolute address , or sometimes a specific address , and is known as pointer data type in higher-level languages. But a program can also use relative address which specifies a location in relation to somewhere else (the base address ). There are many more indirect addressing modes . Mapping logical addresses to physical and virtual memory also adds several levels of indirection; see below.","title":"Addressing schemes"},{"location":"Memory/Random-access-memory/","text":"Random-access memory \u7ef4\u57fa\u767e\u79d1 Random-access memory","title":"Introduction"},{"location":"Memory/Random-access-memory/#random-access#memory","text":"","title":"Random-access memory"},{"location":"Memory/Random-access-memory/#random-access#memory_1","text":"","title":"\u7ef4\u57fa\u767e\u79d1Random-access memory"},{"location":"Modern-CPU/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbamodern CPU\uff0c\u5176\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9\u662f: parallel computing\u3002 1\u3001\u5728 Tendency-toward-parallel-computing \u7ae0\u8282\uff0c\u603b\u7ed3 mordern CPU \u7684\u53d1\u5c55\u8d8b\u52bf\u3001\u8fd9\u6837\u53d1\u5c55\u7684\u539f\u56e0\u3001\u5f71\u54cd\u3002 2\u3001\u5728 Multiprocessing \u7ae0\u8282\uff0c\u603b\u7ed3mordern CPU\u7684architecture\uff0c\u5b83\u975e\u5e38\u91cd\u8981\uff0c\u662f\u540e\u7eed\u5f88\u591a\u5185\u5bb9\u7684\u57fa\u7840\u3002 3\u3001\u5728 Multicore-and-multiprocessor \u7ae0\u8282\uff0c\u5219\u5bf9Multicore\u3001multiprocessor \u8fdb\u884c\u4e00\u4e9b\u8bf4\u660e\u3002","title":"Introduction"},{"location":"Modern-CPU/#_1","text":"\u672c\u7ae0\u8ba8\u8bbamodern CPU\uff0c\u5176\u4e2d\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9\u662f: parallel computing\u3002 1\u3001\u5728 Tendency-toward-parallel-computing \u7ae0\u8282\uff0c\u603b\u7ed3 mordern CPU \u7684\u53d1\u5c55\u8d8b\u52bf\u3001\u8fd9\u6837\u53d1\u5c55\u7684\u539f\u56e0\u3001\u5f71\u54cd\u3002 2\u3001\u5728 Multiprocessing \u7ae0\u8282\uff0c\u603b\u7ed3mordern CPU\u7684architecture\uff0c\u5b83\u975e\u5e38\u91cd\u8981\uff0c\u662f\u540e\u7eed\u5f88\u591a\u5185\u5bb9\u7684\u57fa\u7840\u3002 3\u3001\u5728 Multicore-and-multiprocessor \u7ae0\u8282\uff0c\u5219\u5bf9Multicore\u3001multiprocessor \u8fdb\u884c\u4e00\u4e9b\u8bf4\u660e\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multi-core-processor/","text":"Multi-core processor wikipedia Multi-core processor A multi-core processor is a single computing component with two or more independent processing units called cores, which read and execute program instructions .[ 1] The instructions are ordinary CPU instructions (such as add, move data, and branch) but the single processor can run multiple instructions on separate cores at the same time, increasing overall speed for programs amenable to parallel computing .[ 2] Manufacturers typically integrate the cores onto a single integrated circuit die (known as a chip multiprocessor or CMP) or onto multiple dies\uff08\u7ba1\u82af\uff09 in a single chip package . The microprocessors currently used in almost all personal computers are multi-core. A multi-core processor implements multiprocessing in a single physical package. Designers may couple cores in a multi-core device tightly or loosely. For example, cores may or may not share caches , and they may implement message passing or shared-memory inter-core communication methods. Common network topologies to interconnect cores include bus , ring , two-dimensional mesh , and crossbar . Homogeneous multi-core systems include only identical cores; heterogeneous multi-core systems have cores that are not identical (e.g. big.LITTLE have heterogeneous cores that share the same instruction set, while AMD Accelerated Processing Units have cores that don't even share the same instruction set). Just as with single-processor systems, cores in multi-core systems may implement architectures such as VLIW , superscalar , vector , or multithreading . Multi-core processors are widely used across many application domains, including general-purpose , embedded , network , digital signal processing (DSP), and graphics (GPU). The improvement in performance gained by the use of a multi-core processor depends very much on the software algorithms used and their implementation . In particular, possible gains are limited by the fraction of the software that can run in parallel simultaneously on multiple cores; this effect is described by Amdahl's law . In the best case, so-called embarrassingly parallel problems may realize speedup factors \uff08\u52a0\u901f\u500d\u6570\uff09 near the number of cores, or even more if the problem is split up enough to fit within each core's cache(s), avoiding use of much slower main-system memory . Most applications, however, are not accelerated so much unless programmers invest a prohibitive amount of effort in re-factoring\uff08\u91cd\u6784\uff09 the whole problem.[ 3] The parallelization of software is a significant ongoing topic of research. Software effects Managing concurrency acquires a central role in developing parallel applications. The basic steps in designing parallel applications are: Partitioning \u5212\u5206 The partitioning stage of a design is intended to expose opportunities for parallel execution. Hence, the focus is on defining a large number of small tasks in order to yield what is termed a fine-grained decomposition \uff08\u7ec6\u7c92\u5ea6\u5206\u89e3\uff09 of a problem. Communication The tasks generated by a partition are intended to execute concurrently but cannot, in general, execute independently. The computation to be performed in one task will typically require data associated with another task. Data must then be transferred between tasks so as to allow computation to proceed. This information flow is specified in the communication phase of a design. Agglomeration \u96c6\u805a In the third stage, development moves from the abstract toward the concrete. Developers revisit decisions made in the partitioning and communication phases with a view to obtaining an algorithm that will execute efficiently on some class of parallel computer. In particular, developers consider whether it is useful to combine, or agglomerate, tasks identified by the partitioning phase, so as to provide a smaller number of tasks, each of greater size. They also determine whether it is worthwhile to replicate data and computation. Mapping In the fourth and final stage of the design of parallel algorithms, the developers specify where each task is to execute. This mapping problem does not arise on uniprocessors or on shared-memory computers that provide automatic task scheduling. On the other hand, on the server side , multi-core processors are ideal because they allow many users to connect to a site simultaneously and have independent threads of execution. This allows for Web servers and application servers that have much better throughput . 20181130 https://unix.stackexchange.com/questions/218074/how-to-know-number-of-cores-of-a-system-in-linux","title":"Multi-core-processor"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multi-core-processor/#multi-core#processor","text":"","title":"Multi-core processor"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multi-core-processor/#wikipedia#multi-core#processor","text":"A multi-core processor is a single computing component with two or more independent processing units called cores, which read and execute program instructions .[ 1] The instructions are ordinary CPU instructions (such as add, move data, and branch) but the single processor can run multiple instructions on separate cores at the same time, increasing overall speed for programs amenable to parallel computing .[ 2] Manufacturers typically integrate the cores onto a single integrated circuit die (known as a chip multiprocessor or CMP) or onto multiple dies\uff08\u7ba1\u82af\uff09 in a single chip package . The microprocessors currently used in almost all personal computers are multi-core. A multi-core processor implements multiprocessing in a single physical package. Designers may couple cores in a multi-core device tightly or loosely. For example, cores may or may not share caches , and they may implement message passing or shared-memory inter-core communication methods. Common network topologies to interconnect cores include bus , ring , two-dimensional mesh , and crossbar . Homogeneous multi-core systems include only identical cores; heterogeneous multi-core systems have cores that are not identical (e.g. big.LITTLE have heterogeneous cores that share the same instruction set, while AMD Accelerated Processing Units have cores that don't even share the same instruction set). Just as with single-processor systems, cores in multi-core systems may implement architectures such as VLIW , superscalar , vector , or multithreading . Multi-core processors are widely used across many application domains, including general-purpose , embedded , network , digital signal processing (DSP), and graphics (GPU). The improvement in performance gained by the use of a multi-core processor depends very much on the software algorithms used and their implementation . In particular, possible gains are limited by the fraction of the software that can run in parallel simultaneously on multiple cores; this effect is described by Amdahl's law . In the best case, so-called embarrassingly parallel problems may realize speedup factors \uff08\u52a0\u901f\u500d\u6570\uff09 near the number of cores, or even more if the problem is split up enough to fit within each core's cache(s), avoiding use of much slower main-system memory . Most applications, however, are not accelerated so much unless programmers invest a prohibitive amount of effort in re-factoring\uff08\u91cd\u6784\uff09 the whole problem.[ 3] The parallelization of software is a significant ongoing topic of research.","title":"wikipedia Multi-core processor"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multi-core-processor/#software#effects","text":"Managing concurrency acquires a central role in developing parallel applications. The basic steps in designing parallel applications are: Partitioning \u5212\u5206 The partitioning stage of a design is intended to expose opportunities for parallel execution. Hence, the focus is on defining a large number of small tasks in order to yield what is termed a fine-grained decomposition \uff08\u7ec6\u7c92\u5ea6\u5206\u89e3\uff09 of a problem. Communication The tasks generated by a partition are intended to execute concurrently but cannot, in general, execute independently. The computation to be performed in one task will typically require data associated with another task. Data must then be transferred between tasks so as to allow computation to proceed. This information flow is specified in the communication phase of a design. Agglomeration \u96c6\u805a In the third stage, development moves from the abstract toward the concrete. Developers revisit decisions made in the partitioning and communication phases with a view to obtaining an algorithm that will execute efficiently on some class of parallel computer. In particular, developers consider whether it is useful to combine, or agglomerate, tasks identified by the partitioning phase, so as to provide a smaller number of tasks, each of greater size. They also determine whether it is worthwhile to replicate data and computation. Mapping In the fourth and final stage of the design of parallel algorithms, the developers specify where each task is to execute. This mapping problem does not arise on uniprocessors or on shared-memory computers that provide automatic task scheduling. On the other hand, on the server side , multi-core processors are ideal because they allow many users to connect to a site simultaneously and have independent threads of execution. This allows for Web servers and application servers that have much better throughput .","title":"Software effects"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multi-core-processor/#20181130","text":"https://unix.stackexchange.com/questions/218074/how-to-know-number-of-cores-of-a-system-in-linux","title":"20181130"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multicore-VS-Multiprocessor/","text":"Difference Between Multicore and Multiprocessor What is the difference between MultiCore and MultiProcessor? What's the difference between multicore proc and multiproc system?","title":"Multicore-VS-Multiprocessor"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multicore-VS-Multiprocessor/#difference#between#multicore#and#multiprocessor","text":"","title":"Difference Between Multicore and Multiprocessor"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multicore-VS-Multiprocessor/#what#is#the#difference#between#multicore#and#multiprocessor","text":"","title":"What is the difference between MultiCore and MultiProcessor?"},{"location":"Modern-CPU/Multicore-and-multiprocessor/Multicore-VS-Multiprocessor/#whats#the#difference#between#multicore#proc#and#multiproc#system","text":"","title":"What's the difference between multicore proc and multiproc system?"},{"location":"Modern-CPU/Multiprocessing/","text":"Multiprocessing \u5728\u5f53\u4ecaCPU\u671d\u7740parallel scaling\u65b9\u5411\u7684\u53d1\u5c55\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u601d\u8003CPU\u7684\u7ed3\u6784\uff0c\u800cMultiprocessing\u5219\u662f\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u7684\u8ba8\u8bba\u3002 Wikipedia Multiprocessing NOTE: \u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u63cf\u8ff0Multiprocessing \uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Multiple-model \u7ae0\u8282\uff0c\u56e0\u6b64\uff0c\u5728Multiprocessing\uff0c\u5c31\u4f1a\u9762\u4e34multiple model\u4e2d\u63cf\u8ff0\u7684\u6240\u6709\u7684\u95ee\u9898\u3002 Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system.[ 1] [ 2] The term also refers to the ability of a system to support more than one processor and/or the ability to allocate tasks between them.[ 3] There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined ( multiple cores on one die , multiple dies in one package , multiple packages in one system unit , etc.). Key topics NOTE: \u8fd9\u4e00\u6bb5\u5bf9Multiprocessing\u4e2d\u5173\u952etopic\u7684\u603b\u7ed3\u662f\u975e\u5e38\u597d\u7684\u3002 Processor symmetry NOTE: \"symmetry\"\u7684\u610f\u601d\u662f\"\u5bf9\u79f0\"\u3002 Systems that treat all CPUs equally are called symmetric multiprocessing (SMP) systems. In systems where all CPUs are not equal, system resources may be divided in a number of ways, including asymmetric multiprocessing (ASMP), non-uniform memory access (NUMA) multiprocessing, and clustered multiprocessing. NOTE: SMP\u3001NUMA\u662f\u6211\u4eec\u7ecf\u5e38\u9047\u5230\u7684\u3002 Instruction and data streams NOTE: \u6682\u65f6\u6ca1\u6709\u9047\u5230\u76f8\u5173\u7684\u5185\u5bb9 Processor coupling Tightly coupled multiprocessor system Loosely coupled multiprocessor system Main article: shared nothing architecture Multiprocessor Communication Architecture NOTE: multiple model\u4e2d\u7684\u5178\u578b\u95ee\u9898 Message passing Separate address space for each processor. processors communicate via message passing. processors provide local message queue memories. focus attention on costly non-local operations. Shared memory Processors communicate with shared address space Processors communicate by memory read/write Easy on small-scale machines Lower latency SMP or NUMA architecture Flynn's taxonomy Single instruction stream Multiple instruction streams Single program Multiple programs Single data stream SISD MISD Multiple data streams SIMD MIMD SPMD MPMD NOTE: data stream**\u548c**instruction stream \u7d20\u6750 csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd","title":"Introduction"},{"location":"Modern-CPU/Multiprocessing/#multiprocessing","text":"\u5728\u5f53\u4ecaCPU\u671d\u7740parallel scaling\u65b9\u5411\u7684\u53d1\u5c55\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u601d\u8003CPU\u7684\u7ed3\u6784\uff0c\u800cMultiprocessing\u5219\u662f\u5bf9\u8fd9\u4e2a\u8bdd\u9898\u7684\u8ba8\u8bba\u3002","title":"Multiprocessing"},{"location":"Modern-CPU/Multiprocessing/#wikipedia#multiprocessing","text":"NOTE: \u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u63cf\u8ff0Multiprocessing \uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Multiple-model \u7ae0\u8282\uff0c\u56e0\u6b64\uff0c\u5728Multiprocessing\uff0c\u5c31\u4f1a\u9762\u4e34multiple model\u4e2d\u63cf\u8ff0\u7684\u6240\u6709\u7684\u95ee\u9898\u3002 Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system.[ 1] [ 2] The term also refers to the ability of a system to support more than one processor and/or the ability to allocate tasks between them.[ 3] There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined ( multiple cores on one die , multiple dies in one package , multiple packages in one system unit , etc.).","title":"Wikipedia Multiprocessing"},{"location":"Modern-CPU/Multiprocessing/#key#topics","text":"NOTE: \u8fd9\u4e00\u6bb5\u5bf9Multiprocessing\u4e2d\u5173\u952etopic\u7684\u603b\u7ed3\u662f\u975e\u5e38\u597d\u7684\u3002","title":"Key topics"},{"location":"Modern-CPU/Multiprocessing/#processor#symmetry","text":"NOTE: \"symmetry\"\u7684\u610f\u601d\u662f\"\u5bf9\u79f0\"\u3002 Systems that treat all CPUs equally are called symmetric multiprocessing (SMP) systems. In systems where all CPUs are not equal, system resources may be divided in a number of ways, including asymmetric multiprocessing (ASMP), non-uniform memory access (NUMA) multiprocessing, and clustered multiprocessing. NOTE: SMP\u3001NUMA\u662f\u6211\u4eec\u7ecf\u5e38\u9047\u5230\u7684\u3002","title":"Processor symmetry"},{"location":"Modern-CPU/Multiprocessing/#instruction#and#data#streams","text":"NOTE: \u6682\u65f6\u6ca1\u6709\u9047\u5230\u76f8\u5173\u7684\u5185\u5bb9","title":"Instruction and data streams"},{"location":"Modern-CPU/Multiprocessing/#processor#coupling","text":"","title":"Processor coupling"},{"location":"Modern-CPU/Multiprocessing/#tightly#coupled#multiprocessor#system","text":"","title":"Tightly coupled multiprocessor system"},{"location":"Modern-CPU/Multiprocessing/#loosely#coupled#multiprocessor#system","text":"Main article: shared nothing architecture","title":"Loosely coupled multiprocessor system"},{"location":"Modern-CPU/Multiprocessing/#multiprocessor#communication#architecture","text":"NOTE: multiple model\u4e2d\u7684\u5178\u578b\u95ee\u9898","title":"Multiprocessor Communication Architecture"},{"location":"Modern-CPU/Multiprocessing/#message#passing","text":"Separate address space for each processor. processors communicate via message passing. processors provide local message queue memories. focus attention on costly non-local operations.","title":"Message passing"},{"location":"Modern-CPU/Multiprocessing/#shared#memory","text":"Processors communicate with shared address space Processors communicate by memory read/write Easy on small-scale machines Lower latency SMP or NUMA architecture","title":"Shared memory"},{"location":"Modern-CPU/Multiprocessing/#flynns#taxonomy","text":"Single instruction stream Multiple instruction streams Single program Multiple programs Single data stream SISD MISD Multiple data streams SIMD MIMD SPMD MPMD NOTE: data stream**\u548c**instruction stream","title":"Flynn's taxonomy"},{"location":"Modern-CPU/Multiprocessing/#_1","text":"","title":"\u7d20\u6750"},{"location":"Modern-CPU/Multiprocessing/#csdn","text":"","title":"csdn \u804a\u804a\u9ad8\u5e76\u53d1\uff08\u4e94\uff09\u7406\u89e3\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u4ee5\u53ca\u5bf9\u5e76\u53d1\u7f16\u7a0b\u7684\u5f71\u54cd"},{"location":"Modern-CPU/Multiprocessing/Symmetric-multiprocessing/","text":"SMP: Symmetric multiprocessing \"symmetric\"\u7684\u542b\u4e49\u662f\"\u5bf9\u79f0\"\u3002 wikipedia Symmetric multiprocessing Symmetric multiprocessing ( SMP ) involves a multiprocessor computer hardware and software architecture where two or more identical processors are connected to a single, shared main memory , have full access to all input and output devices, and are controlled by a single operating system instance that treats all processors equally, reserving none for special purposes. Most multiprocessor systems today use an SMP architecture . In the case of multi-core processors , the SMP architecture applies to the cores, treating them as separate processors. NOTE: \u8fd9\u662f\u5f53\u4eca\u4e3b\u6d41\u7684architecture\u3002\u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u63cf\u8ff0SMP architecture\uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Multiple-model \u7ae0\u8282\uff0c\u56e0\u6b64\uff0c\u5f53\u91c7\u7528SMP architecture\u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u9762\u4e34multiple model\u4e2d\u63cf\u8ff0\u7684\u6240\u6709\u7684\u95ee\u9898\u3002 SMP systems are tightly coupled multiprocessor systems with a pool of homogeneous(\u540c\u6784\u7684) processors running independently of each other. Each processor, executing different programs and working on different sets of data, has the capability of sharing common resources ( memory , I/O device, interrupt system and so on) that are connected using a system bus or a crossbar . Diagram of a symmetric multiprocessing system NOTE: \u8fd9\u5f20\u56fe\u975e\u5e38\u91cd\u8981\u3002 Design SMP systems have centralized shared memory called main memory (MM) operating under a single operating system with two or more homogeneous processors. Usually each processor has an associated private high-speed memory known as cache memory (or cache) to speed up the main memory data access and to reduce the system bus traffic. NOTE: \u4ece\u4e0a\u9762\u7684\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u6bcf\u4e2aprocessor\u90fd\u6709\u81ea\u5df1\u7684cache\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4ecb\u7ecd\u4e86\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662fspeed up main memory data access\u3002 Processors may be interconnected(\u4e92\u8054) using buses, crossbar switches or on-chip mesh networks. The bottleneck in the scalability of SMP using buses or crossbar switches is the bandwidth and power consumption of the interconnect among the various processors, the memory, and the disk arrays. Mesh(\u7f51\u683c) architectures avoid these bottlenecks, and provide nearly linear scalability to much higher processor counts at the sacrifice of programmability: Serious programming challenges remain with this kind of architecture because it requires two distinct modes of programming; one for the CPUs themselves and one for the interconnect between the CPUs. A single programming language would have to be able to not only partition the workload, but also comprehend the memory locality, which is severe in a mesh-based architecture.[ 3] SMP systems allow any processor to work on any task no matter where the data for that task is located in memory, provided that each task in the system is not in execution on two or more processors at the same time. With proper operating system support, SMP systems can easily move tasks between processors to balance the workload\uff08\u5de5\u4f5c\u8d1f\u8f7d\uff09 efficiently. Performance When more than one program executes at the same time, an SMP system has considerably better performance than a uni-processor, because different programs can run on different CPUs simultaneously. In cases where an SMP environment processes many jobs, administrators often experience a loss of hardware efficiency. Software programs have been developed to schedule jobs so that the processor utilization reaches its maximum potential. Good software packages can achieve this maximum potential by scheduling each CPU separately, as well as being able to integrate multiple SMP machines and clusters. Access to RAM is serialized ; this and cache coherency issues causes performance to lag(\u843d\u540e) slightly behind the number of additional processors in the system. NOTE: \u8fd9\u79cd\u7ed3\u6784\u7684\u74f6\u9888\u6240\u5728\u3002\u7ed3\u6784\u51b3\u5b9a***\u3002 Consistency model of SMP \u8fd9\u4e2a\u95ee\u9898\u5728Wikipedia Memory ordering \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba: In symmetric multiprocessing (SMP) microprocessor systems There are several memory-consistency models for SMP systems: Sequential consistency (all reads and all writes are in-order) Relaxed consistency (some types of reordering are allowed) Loads can be reordered after loads (for better working of cache coherency, better scaling) Loads can be reordered after stores Stores can be reordered after stores Stores can be reordered after loads Weak consistency (reads and writes are arbitrarily reordered, limited only by explicit memory barriers )","title":"Introduction"},{"location":"Modern-CPU/Multiprocessing/Symmetric-multiprocessing/#smp#symmetric#multiprocessing","text":"\"symmetric\"\u7684\u542b\u4e49\u662f\"\u5bf9\u79f0\"\u3002","title":"SMP: Symmetric multiprocessing"},{"location":"Modern-CPU/Multiprocessing/Symmetric-multiprocessing/#wikipedia#symmetric#multiprocessing","text":"Symmetric multiprocessing ( SMP ) involves a multiprocessor computer hardware and software architecture where two or more identical processors are connected to a single, shared main memory , have full access to all input and output devices, and are controlled by a single operating system instance that treats all processors equally, reserving none for special purposes. Most multiprocessor systems today use an SMP architecture . In the case of multi-core processors , the SMP architecture applies to the cores, treating them as separate processors. NOTE: \u8fd9\u662f\u5f53\u4eca\u4e3b\u6d41\u7684architecture\u3002\u53ef\u4ee5\u4f7f\u7528multiple model\u6765\u63cf\u8ff0SMP architecture\uff0c\u53c2\u89c1\u5de5\u7a0bparallel-computing\u7684 Model\\Multiple-model \u7ae0\u8282\uff0c\u56e0\u6b64\uff0c\u5f53\u91c7\u7528SMP architecture\u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u9762\u4e34multiple model\u4e2d\u63cf\u8ff0\u7684\u6240\u6709\u7684\u95ee\u9898\u3002 SMP systems are tightly coupled multiprocessor systems with a pool of homogeneous(\u540c\u6784\u7684) processors running independently of each other. Each processor, executing different programs and working on different sets of data, has the capability of sharing common resources ( memory , I/O device, interrupt system and so on) that are connected using a system bus or a crossbar . Diagram of a symmetric multiprocessing system NOTE: \u8fd9\u5f20\u56fe\u975e\u5e38\u91cd\u8981\u3002","title":"wikipedia Symmetric multiprocessing"},{"location":"Modern-CPU/Multiprocessing/Symmetric-multiprocessing/#design","text":"SMP systems have centralized shared memory called main memory (MM) operating under a single operating system with two or more homogeneous processors. Usually each processor has an associated private high-speed memory known as cache memory (or cache) to speed up the main memory data access and to reduce the system bus traffic. NOTE: \u4ece\u4e0a\u9762\u7684\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u6bcf\u4e2aprocessor\u90fd\u6709\u81ea\u5df1\u7684cache\uff0c\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4ecb\u7ecd\u4e86\uff0c\u8fd9\u6837\u505a\u7684\u76ee\u7684\u662fspeed up main memory data access\u3002 Processors may be interconnected(\u4e92\u8054) using buses, crossbar switches or on-chip mesh networks. The bottleneck in the scalability of SMP using buses or crossbar switches is the bandwidth and power consumption of the interconnect among the various processors, the memory, and the disk arrays. Mesh(\u7f51\u683c) architectures avoid these bottlenecks, and provide nearly linear scalability to much higher processor counts at the sacrifice of programmability: Serious programming challenges remain with this kind of architecture because it requires two distinct modes of programming; one for the CPUs themselves and one for the interconnect between the CPUs. A single programming language would have to be able to not only partition the workload, but also comprehend the memory locality, which is severe in a mesh-based architecture.[ 3] SMP systems allow any processor to work on any task no matter where the data for that task is located in memory, provided that each task in the system is not in execution on two or more processors at the same time. With proper operating system support, SMP systems can easily move tasks between processors to balance the workload\uff08\u5de5\u4f5c\u8d1f\u8f7d\uff09 efficiently.","title":"Design"},{"location":"Modern-CPU/Multiprocessing/Symmetric-multiprocessing/#performance","text":"When more than one program executes at the same time, an SMP system has considerably better performance than a uni-processor, because different programs can run on different CPUs simultaneously. In cases where an SMP environment processes many jobs, administrators often experience a loss of hardware efficiency. Software programs have been developed to schedule jobs so that the processor utilization reaches its maximum potential. Good software packages can achieve this maximum potential by scheduling each CPU separately, as well as being able to integrate multiple SMP machines and clusters. Access to RAM is serialized ; this and cache coherency issues causes performance to lag(\u843d\u540e) slightly behind the number of additional processors in the system. NOTE: \u8fd9\u79cd\u7ed3\u6784\u7684\u74f6\u9888\u6240\u5728\u3002\u7ed3\u6784\u51b3\u5b9a***\u3002","title":"Performance"},{"location":"Modern-CPU/Multiprocessing/Symmetric-multiprocessing/#consistency#model#of#smp","text":"\u8fd9\u4e2a\u95ee\u9898\u5728Wikipedia Memory ordering \u4e2d\u8fdb\u884c\u4e86\u8ba8\u8bba:","title":"Consistency model of SMP"},{"location":"Modern-CPU/Multiprocessing/Symmetric-multiprocessing/#in#symmetric#multiprocessing#smp#microprocessor#systems","text":"There are several memory-consistency models for SMP systems: Sequential consistency (all reads and all writes are in-order) Relaxed consistency (some types of reordering are allowed) Loads can be reordered after loads (for better working of cache coherency, better scaling) Loads can be reordered after stores Stores can be reordered after stores Stores can be reordered after loads Weak consistency (reads and writes are arbitrarily reordered, limited only by explicit memory barriers )","title":"In symmetric multiprocessing (SMP) microprocessor systems"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u8ba8\u8bbaCPU\u7684\u53d1\u5c55\u3002 \u53d1\u5c55\u6982\u8ff0: \u4ece frequency scaling \u5230 parallel scaling \u5728Wikipedia Parallel computing \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3: Parallelism has been employed for many years, mainly in high-performance computing , but interest in it has grown lately due to the physical constraints preventing frequency scaling .[ 2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[ 3] parallel computing has become the dominant paradigm in computer architecture , mainly in the form of multi-core processors .[ 4] \u5728Wikipedia Frequency scaling \u4e2d\u4e5f\u8be6\u7ec6\u7684\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u95ee\u9898: Frequency ramping was the dominant force in commodity processor performance increases from the mid-1980s until roughly the end of 2004. Increasing processor power consumption led ultimately to Intel 's May 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[ 3] NOTE: \u6807\u5fd7\u7740frequency scaling\u7684\u9000\u51fa Moore's Law , despite predictions of its demise, is still in effect. Despite power issues, transistor densities are still doubling every 18 to 24 months. With the end of frequency scaling , these new transistors (which are no longer needed to facilitate frequency scaling) can be used to add extra hardware, such as additional cores, to facilitate parallel computing - a technique that is being referred to as parallel scaling . NOTE: \u53ef\u4ee5\u9884\u89c1: \u53ea\u8981 Moore's Law \u4f9d\u7136\u6709\u6548\uff0c multicore processors \u7684number of core\u5c06\u4f1a\u8fdb\u4e00\u6b65\u589e\u52a0\uff0c\u8fd9\u5c06\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86computer\u7684parallel computing\u80fd\u529b\u3002 \u6700\u540e\u4e00\u6bb5\uff0c\u4f5c\u8005\u6240\u603b\u7ed3\u7684**parallel scaling**\u662f\u975e\u5e38\u597d\u7684\u3002 The end of frequency scaling as the dominant cause of processor performance gains has caused an industry-wide shift to parallel computing in the form of multicore processors . \u5728 gotw The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \u4e2d\uff0c\u6709\u5982\u4e0b\u63cf\u8ff0: The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse to hyperthreading and multicore architectures. Both of these features are already available on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors, and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall Processor Forum was multicore devices, as many companies showed new or updated multicore processors. Looking back, it\u2019s not much of a stretch to call 2004 the year of multicore. And that puts us at a fundamental turning point in software development, at least for the next few years and for applications targeting general-purpose desktop computers and low-end servers (which happens to account for the vast bulk of the dollar value of software sold today). In this article, I\u2019ll describe the changing face of hardware, why it suddenly does matter to software, and how specifically the concurrency revolution matters to you and is going to change the way you will likely be writing software in the future. \u5728 preshing A Look Back at Single-Threaded CPU Performance \u4e2d\uff0c\u4e5f\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 \u663e\u7136\uff0c\u4e3b\u8981\u95ee\u9898\u662f\u65e0\u6cd5\u65e0\u9650\u5236\u5730\u8fdb\u884cfrequency scaling\uff0c\u56e0\u6b64\u6700\u7ec8\u7684\u7ed3\u679c\u662f: \"parallel computing has become the dominant paradigm in computer architecture \"\u3002 \u5f71\u54cd \u5f53\u4ee3CPU\u671d\u7740parallel computing\u7684\u65b9\u5411\u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u7531\u4e8e\u5b83\u662f\u5e95\u5c42\u7684(hardware\u5c42)\uff0c\u56e0\u6b64\u5b83\u5bf9computer science\u7684\u5176\u4ed6\u5404\u4e2a\u9886\u57df\u90fd\u4ea7\u751f\u4e86\u91cd\u5927\u7684\u5f71\u54cd\uff0c\u5b83\u4fc3\u4f7f\u4e86computer science\u4e2d\u7684\u5176\u4ed6\u9886\u57df\u90fd\u671d\u7740\u8fd9\u4e2a\u65b9\u5411\u6f14\u8fdb: \u5bf9programming language\u7684\u5f71\u54cd Programming language\u9700\u8981\u4e0d\u65ad\u5730\u5f15\u5165\u65b0\u7684\u7279\u6027\u6765\u4fc3\u8fdb\u5bf9parallel computing\u7684\u5145\u5206\u5229\u7528\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50: 1) C++11\u5f15\u5165\u4e86thread\u3001C++20\u5f15\u5165\u4e86coroutine NOTE: \u5728 gotw The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \u4e2d \u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u63a2\u8ba8 2) Python\u5f15\u5165\u4e86coroutine 3) concurrent programming language\u7684\u6d8c\u73b0: golang \u603b\u7684\u6765\u8bf4\uff0c\u5728programming language\u5c42\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u878d\u5165\u65b0\u7684\u8bed\u8a00\u7279\u6027\u6765\u9002\u5e94\u8fd9\u79cd\u6280\u672f\u7684\u6f14\u8fdb\u3002 \u5bf9OS\u7684\u5f71\u54cd OS\u9700\u8981\u8003\u8651\u5982\u4f55\u6765\u5145\u5206\u53d1\u6325CPU\u7684parallel computing\u7279\u6027\uff0c\u6bd4\u5982: 1) IO multiplex \u5bf9\u5e94\u7528\u5c42\u7684\u5f71\u54cd \u5728\u5e94\u7528\u5c42\uff0c\u9700\u8981\u8003\u8651\u5982\u4f55\u91cd\u590d\u53d1\u6325CPU\u7684parallel computing\u7279\u6027\u3002 \u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u5174\u8d77\u3002 \u7d20\u6750 \u672c\u7ae0\u6536\u5f55\u4e86\u4e00\u4e9b\u8ba8\u8bbaparallel computing\u53d1\u5c55\u8d8b\u52bf\u7684\u6587\u7ae0: 1\u3001preshing A Look Back at Single-Threaded CPU Performance 2\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A \u7b80\u5355\u6765\u8bf4\uff0c\u4f60\u53ef\u4ee5\u7406\u89e3\u5f53\u4ee3CPU\u4e0d\u4ec5\u662f\u591a\u6838\u5fc3\uff0c\u800c\u4e14\u6bcf\u4e2a\u6838\u5fc3\u8fd8\u662f\u591a\u4efb\u52a1\uff08\u591a\u6307\u4ee4\uff09\u5e76\u884c\u7684\u3002\u8ba1\u7b97\u673a\u8bfe\u672c\u4e0a\u7684\u90a3\u79cd\u4e00\u4e2a\u65f6\u949f\u4e00\u6761\u6307\u4ee4\u7684\uff0c\u65e9\u5c31\u662f\u8001\u9ec4\u5386\u4e86 \uff08\u5f53\u7136\uff0c\u5b8f\u89c2\u6765\u770b\u57fa\u672c\u539f\u7406\u5e76\u6ca1\u6709\u6539\u53d8\uff09","title":"Introduction"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/#_1","text":"\u672c\u7ae0\u8ba8\u8bbaCPU\u7684\u53d1\u5c55\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/#frequency#scaling#parallel#scaling","text":"\u5728Wikipedia Parallel computing \u4e2d\u5bf9\u6b64\u8fdb\u884c\u4e86\u603b\u7ed3: Parallelism has been employed for many years, mainly in high-performance computing , but interest in it has grown lately due to the physical constraints preventing frequency scaling .[ 2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[ 3] parallel computing has become the dominant paradigm in computer architecture , mainly in the form of multi-core processors .[ 4] \u5728Wikipedia Frequency scaling \u4e2d\u4e5f\u8be6\u7ec6\u7684\u8ba8\u8bba\u4e86\u8fd9\u4e2a\u95ee\u9898: Frequency ramping was the dominant force in commodity processor performance increases from the mid-1980s until roughly the end of 2004. Increasing processor power consumption led ultimately to Intel 's May 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[ 3] NOTE: \u6807\u5fd7\u7740frequency scaling\u7684\u9000\u51fa Moore's Law , despite predictions of its demise, is still in effect. Despite power issues, transistor densities are still doubling every 18 to 24 months. With the end of frequency scaling , these new transistors (which are no longer needed to facilitate frequency scaling) can be used to add extra hardware, such as additional cores, to facilitate parallel computing - a technique that is being referred to as parallel scaling . NOTE: \u53ef\u4ee5\u9884\u89c1: \u53ea\u8981 Moore's Law \u4f9d\u7136\u6709\u6548\uff0c multicore processors \u7684number of core\u5c06\u4f1a\u8fdb\u4e00\u6b65\u589e\u52a0\uff0c\u8fd9\u5c06\u8fdb\u4e00\u6b65\u589e\u52a0\u4e86computer\u7684parallel computing\u80fd\u529b\u3002 \u6700\u540e\u4e00\u6bb5\uff0c\u4f5c\u8005\u6240\u603b\u7ed3\u7684**parallel scaling**\u662f\u975e\u5e38\u597d\u7684\u3002 The end of frequency scaling as the dominant cause of processor performance gains has caused an industry-wide shift to parallel computing in the form of multicore processors . \u5728 gotw The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \u4e2d\uff0c\u6709\u5982\u4e0b\u63cf\u8ff0: The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse to hyperthreading and multicore architectures. Both of these features are already available on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors, and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall Processor Forum was multicore devices, as many companies showed new or updated multicore processors. Looking back, it\u2019s not much of a stretch to call 2004 the year of multicore. And that puts us at a fundamental turning point in software development, at least for the next few years and for applications targeting general-purpose desktop computers and low-end servers (which happens to account for the vast bulk of the dollar value of software sold today). In this article, I\u2019ll describe the changing face of hardware, why it suddenly does matter to software, and how specifically the concurrency revolution matters to you and is going to change the way you will likely be writing software in the future. \u5728 preshing A Look Back at Single-Threaded CPU Performance \u4e2d\uff0c\u4e5f\u8fdb\u884c\u4e86\u4ecb\u7ecd\u3002 \u663e\u7136\uff0c\u4e3b\u8981\u95ee\u9898\u662f\u65e0\u6cd5\u65e0\u9650\u5236\u5730\u8fdb\u884cfrequency scaling\uff0c\u56e0\u6b64\u6700\u7ec8\u7684\u7ed3\u679c\u662f: \"parallel computing has become the dominant paradigm in computer architecture \"\u3002","title":"\u53d1\u5c55\u6982\u8ff0: \u4ecefrequency scaling \u5230 parallel scaling"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/#_2","text":"\u5f53\u4ee3CPU\u671d\u7740parallel computing\u7684\u65b9\u5411\u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u7531\u4e8e\u5b83\u662f\u5e95\u5c42\u7684(hardware\u5c42)\uff0c\u56e0\u6b64\u5b83\u5bf9computer science\u7684\u5176\u4ed6\u5404\u4e2a\u9886\u57df\u90fd\u4ea7\u751f\u4e86\u91cd\u5927\u7684\u5f71\u54cd\uff0c\u5b83\u4fc3\u4f7f\u4e86computer science\u4e2d\u7684\u5176\u4ed6\u9886\u57df\u90fd\u671d\u7740\u8fd9\u4e2a\u65b9\u5411\u6f14\u8fdb: \u5bf9programming language\u7684\u5f71\u54cd Programming language\u9700\u8981\u4e0d\u65ad\u5730\u5f15\u5165\u65b0\u7684\u7279\u6027\u6765\u4fc3\u8fdb\u5bf9parallel computing\u7684\u5145\u5206\u5229\u7528\uff0c\u4e0b\u9762\u662f\u4e00\u4e9b\u4f8b\u5b50: 1) C++11\u5f15\u5165\u4e86thread\u3001C++20\u5f15\u5165\u4e86coroutine NOTE: \u5728 gotw The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software \u4e2d \u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u63a2\u8ba8 2) Python\u5f15\u5165\u4e86coroutine 3) concurrent programming language\u7684\u6d8c\u73b0: golang \u603b\u7684\u6765\u8bf4\uff0c\u5728programming language\u5c42\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u878d\u5165\u65b0\u7684\u8bed\u8a00\u7279\u6027\u6765\u9002\u5e94\u8fd9\u79cd\u6280\u672f\u7684\u6f14\u8fdb\u3002 \u5bf9OS\u7684\u5f71\u54cd OS\u9700\u8981\u8003\u8651\u5982\u4f55\u6765\u5145\u5206\u53d1\u6325CPU\u7684parallel computing\u7279\u6027\uff0c\u6bd4\u5982: 1) IO multiplex \u5bf9\u5e94\u7528\u5c42\u7684\u5f71\u54cd \u5728\u5e94\u7528\u5c42\uff0c\u9700\u8981\u8003\u8651\u5982\u4f55\u91cd\u590d\u53d1\u6325CPU\u7684parallel computing\u7279\u6027\u3002 \u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u5174\u8d77\u3002","title":"\u5f71\u54cd"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/#_3","text":"\u672c\u7ae0\u6536\u5f55\u4e86\u4e00\u4e9b\u8ba8\u8bbaparallel computing\u53d1\u5c55\u8d8b\u52bf\u7684\u6587\u7ae0: 1\u3001preshing A Look Back at Single-Threaded CPU Performance 2\u3001zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A \u7b80\u5355\u6765\u8bf4\uff0c\u4f60\u53ef\u4ee5\u7406\u89e3\u5f53\u4ee3CPU\u4e0d\u4ec5\u662f\u591a\u6838\u5fc3\uff0c\u800c\u4e14\u6bcf\u4e2a\u6838\u5fc3\u8fd8\u662f\u591a\u4efb\u52a1\uff08\u591a\u6307\u4ee4\uff09\u5e76\u884c\u7684\u3002\u8ba1\u7b97\u673a\u8bfe\u672c\u4e0a\u7684\u90a3\u79cd\u4e00\u4e2a\u65f6\u949f\u4e00\u6761\u6307\u4ee4\u7684\uff0c\u65e9\u5c31\u662f\u8001\u9ec4\u5386\u4e86 \uff08\u5f53\u7136\uff0c\u5b8f\u89c2\u6765\u770b\u57fa\u672c\u539f\u7406\u5e76\u6ca1\u6709\u6539\u53d8\uff09","title":"\u7d20\u6750"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/","text":"gotw The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software NOTE: 1\u3001\u662f\u5728\u9605\u8bfb preshing A Look Back at Single-Threaded CPU Performance \u65f6\uff0c\u53d1\u73b0\u7684\u8fd9\u7bc7\u6587\u7ae0 2\u3001\u5728 \u66f4\u597d\u7684\u5185\u5b58\u7ba1\u7406-jemalloc (redis \u9ed8\u8ba4\u4f7f\u7528\u7684) \u4e2d\uff0c\u5bf9\u8fd9\u7bc7\u6587\u7ae0\u7684\u6838\u5fc3\u89c2\u70b9\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u89e3\u8bfb: 2005\u5e74\u53d1\u8868\u4e86\u4e00\u7bc7\u6587\u7ae0\u201c \u514d\u8d39\u5348\u9910\u7684\u65f6\u4ee3\u7ed3\u675f\u4e86 \u201d\u3002\u5728\u4e4b\u524d\uff0c\u7a0b\u5e8f\u5c31\u7b97\u4e0d\u7528\u8d39\u8111\u5b50\uff0c\u968f\u7740cpu\u65f6\u949f\u901f\u5ea6\u589e\u52a0\uff0c\u7a0b\u5e8f\u6027\u80fd\u81ea\u5df1\u5c31\u4f1a\u4e0a\u53bb\u3002\u4f46\u73b0\u5728\u4e0d\u540c\uff0c\u73b0\u5728cpu\u65f6\u949f\u8d8b\u4e8e\u7a33\u5b9a\uff0c\u800c\u6838\u6570\u4e0d\u65ad\u5730\u589e\u52a0\u3002\u7a0b\u5e8f\u5458\u9700\u8981\u9002\u5e94\u8fd9\u6837\u7684\u591a\u7ebf\u7a0b\u591a\u8fdb\u7a0b\u7684\u73af\u5883\uff0c\u5e76\u8981\u5f00\u53d1\u51fa\u9002\u5408\u7684\u7a0b\u5e8f\u3002\u6587\u7ae0\u8bb2\u7684\u5927\u6982\u662f\u8fd9\u6837\u7684\u5185\u5bb9\u3002 6\u5e74\u4e4b\u540e\u7684\u5982\u4eca\uff0c\u8fd9\u7bc7\u6587\u7ae0\u5b8c\u5168\u53d8\u6210\u73b0\u5b9e\u4e86\u3002\u4e8b\u5b9e\u4e0acpu\u65f6\u949f\u505c\u7559\u57283GHz\uff0c\u800c\u6838\u4e0d\u65ad\u4e0a\u5347\u3002\u73b0\u5728\u7a0b\u5e8f\u8981\u9002\u5e94\u591a\u7ebf\u7a0b\u591a\u8fdb\u7a0b\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u901f\u5ea6\u624d\u80fd\u4e0a\u5347\u3002\u4f46\u662f\u8fd9\u6837\u7684\u7a0b\u5e8f\u5f88\u96be\u3002 3\u3001\u975e\u5e38\u80fd\u591f\u4f53\u73b0\"hardware and software-Andy and Bill's law\" 4\u3001\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u5730\u9ad8\u77bb\u8fdc\u77a9\uff0c\u5199\u4e8e 2005 \u5e74\uff0c\u65f6\u81f3\u4eca\u65e5\uff0c\u4f9d\u7136\u6709\u6548\u3002 \u5e8f\u8a00 By Herb Sutter The biggest sea change in software development since the OO revolution is knocking at the door, and its name is Concurrency . This article appeared in * Dr. Dobb's Journal , 30(3), March 2005**. A much briefer version under the title \"The Concurrency Revolution\" appeared in C/C++ Users Journal , 23(2), February 2005 .* Update note: The CPU trends graph last updated August 2009 to include current data and show the trend continues as predicted. The rest of this article including all text is still original as first posted here in December 2004. Your free lunch will soon be over. What can you do about it? What are you doing about it? The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving(\u9a71\u52a8) clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse (\u4e00\u540c\u5730) to hyperthreading(\u8d85\u7ebf\u7a0b) and multicore architectures. Both of these features are already available on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors, and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall Processor Forum was multicore devices, as many companies showed new or updated multicore processors. Looking back, it\u2019s not much of a stretch to call 2004 the year of multicore. And that puts us at a fundamental turning point in software development, at least for the next few years and for applications targeting general-purpose desktop computers and low-end servers (which happens to account for the vast bulk of the dollar value of software sold today). In this article, I\u2019ll describe the changing face of hardware, why it suddenly does matter to software, and how specifically the concurrency revolution matters to you and is going to change the way you will likely be writing software in the future. NOTE: \u9ad8\u77bb\u8fdc\u77a9 Arguably, the free lunch has already been over for a year or two, only we\u2019re just now noticing. The Free Performance Lunch There\u2019s an interesting phenomenon that\u2019s known as \u201cAndy giveth, and Bill taketh away.\u201d No matter how fast processors get, software consistently finds new ways to eat up the extra speed. Make a CPU ten times as fast, and software will usually find ten times as much to do (or, in some cases, will feel at liberty to do it ten times less efficiently). Most classes of applications have enjoyed free and regular performance gains for several decades, even without releasing new versions or doing anything special, because the CPU manufacturers (primarily) and memory and disk manufacturers (secondarily) have reliably enabled ever-newer and ever-faster mainstream systems. Clock speed isn\u2019t the only measure of performance, or even necessarily a good one, but it\u2019s an instructive one: We\u2019re used to seeing 500MHz CPUs give way to 1GHz CPUs give way to 2GHz CPUs, and so on. Today we\u2019re in the 3GHz range on mainstream computers. Obstacles, and Why You Don\u2019t Have 10GHz Today TANSTAAFL: Moore\u2019s Law and the Next Generation(s) What This Means For Software: The Next Revolution Concurrency is the next major revolution in how we write software In the 1990s, we learned to grok objects. The revolution in mainstream software development from structured programming to object-oriented programming was the greatest such change in the past 20 years, and arguably in the past 30 years. There have been other changes, including the most recent (and genuinely interesting) naissance of web services, but nothing that most of us have seen during our careers has been as fundamental and as far-reaching a change in the way we write software as the object revolution. Until now. Starting today, the performance lunch isn\u2019t free any more. Sure, there will continue to be generally applicable performance gains that everyone can pick up, thanks mainly to cache size improvements. But if you want your application to benefit from the continued exponential throughput advances in new processors, it will need to be a well-written concurrent (usually multithreaded) application. And that\u2019s easier said than done, because not all problems are inherently parallelizable and because concurrent programming is hard. I can hear the howls of protest: \u201cConcurrency? That\u2019s not news! People are already writing concurrent applications.\u201d That\u2019s true. Of a small fraction of developers. Remember that people have been doing object-oriented programming since at least the days of Simula in the late 1960s. But OO didn\u2019t become a revolution, and dominant in the mainstream, until the 1990s. Why then? The reason the revolution happened was primarily that our industry was driven by requirements to write larger and larger systems that solved larger and larger problems and exploited the greater and greater CPU and storage resources that were becoming available. OOP\u2019s strengths in abstraction and dependency management made it a necessity for achieving large-scale software development that is economical, reliable, and repeatable. Similarly, we\u2019ve been doing concurrent programming since those same dark ages, writing coroutines and monitors and similar jazzy stuff. And for the past decade or so we\u2019ve witnessed incrementally more and more programmers writing concurrent (multi-threaded, multi-process) systems. But an actual revolution marked by a major turning point toward concurrency has been slow to materialize. Today the vast majority of applications are single-threaded, and for good reasons that I\u2019ll summarize in the next section. By the way, on the matter of hype: People have always been quick to announce \u201cthe next software development revolution,\u201d usually about their own brand-new technology. Don\u2019t believe it. New technologies are often genuinely interesting and sometimes beneficial, but the biggest revolutions in the way we write software generally come from technologies that have already been around for some years and have already experienced gradual growth before they transition to explosive growth. This is necessary: You can only base a software development revolution on a technology that\u2019s mature enough to build on (including having solid vendor and tool support), and it generally takes any new software technology at least seven years before it\u2019s solid enough to be broadly usable without performance cliffs and other gotchas. As a result, true software development revolutions like OO happen around technologies that have already been undergoing refinement for years, often decades. Even in Hollywood, most genuine \u201covernight successes\u201d have really been performing for many years before their big break. Concurrency is the next major revolution in how we write software. Different experts still have different opinions on whether it will be bigger than OO, but that kind of conversation is best left to pundits. For technologists, the interesting thing is that concurrency is of the same order as OO both in the (expected) scale of the revolution and in the complexity and learning curve of the technology. What It Means For Us Applications will increasingly need to be concurrent if they want to fully exploit continuing exponential CPU throughput gains Efficiency and performance optimization will get more, not less, important Conclusion","title":"Introduction"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#gotw#the#free#lunch#is#over#a#fundamental#turn#toward#concurrency#in#software","text":"NOTE: 1\u3001\u662f\u5728\u9605\u8bfb preshing A Look Back at Single-Threaded CPU Performance \u65f6\uff0c\u53d1\u73b0\u7684\u8fd9\u7bc7\u6587\u7ae0 2\u3001\u5728 \u66f4\u597d\u7684\u5185\u5b58\u7ba1\u7406-jemalloc (redis \u9ed8\u8ba4\u4f7f\u7528\u7684) \u4e2d\uff0c\u5bf9\u8fd9\u7bc7\u6587\u7ae0\u7684\u6838\u5fc3\u89c2\u70b9\u8fdb\u884c\u4e86\u975e\u5e38\u597d\u7684\u89e3\u8bfb: 2005\u5e74\u53d1\u8868\u4e86\u4e00\u7bc7\u6587\u7ae0\u201c \u514d\u8d39\u5348\u9910\u7684\u65f6\u4ee3\u7ed3\u675f\u4e86 \u201d\u3002\u5728\u4e4b\u524d\uff0c\u7a0b\u5e8f\u5c31\u7b97\u4e0d\u7528\u8d39\u8111\u5b50\uff0c\u968f\u7740cpu\u65f6\u949f\u901f\u5ea6\u589e\u52a0\uff0c\u7a0b\u5e8f\u6027\u80fd\u81ea\u5df1\u5c31\u4f1a\u4e0a\u53bb\u3002\u4f46\u73b0\u5728\u4e0d\u540c\uff0c\u73b0\u5728cpu\u65f6\u949f\u8d8b\u4e8e\u7a33\u5b9a\uff0c\u800c\u6838\u6570\u4e0d\u65ad\u5730\u589e\u52a0\u3002\u7a0b\u5e8f\u5458\u9700\u8981\u9002\u5e94\u8fd9\u6837\u7684\u591a\u7ebf\u7a0b\u591a\u8fdb\u7a0b\u7684\u73af\u5883\uff0c\u5e76\u8981\u5f00\u53d1\u51fa\u9002\u5408\u7684\u7a0b\u5e8f\u3002\u6587\u7ae0\u8bb2\u7684\u5927\u6982\u662f\u8fd9\u6837\u7684\u5185\u5bb9\u3002 6\u5e74\u4e4b\u540e\u7684\u5982\u4eca\uff0c\u8fd9\u7bc7\u6587\u7ae0\u5b8c\u5168\u53d8\u6210\u73b0\u5b9e\u4e86\u3002\u4e8b\u5b9e\u4e0acpu\u65f6\u949f\u505c\u7559\u57283GHz\uff0c\u800c\u6838\u4e0d\u65ad\u4e0a\u5347\u3002\u73b0\u5728\u7a0b\u5e8f\u8981\u9002\u5e94\u591a\u7ebf\u7a0b\u591a\u8fdb\u7a0b\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u901f\u5ea6\u624d\u80fd\u4e0a\u5347\u3002\u4f46\u662f\u8fd9\u6837\u7684\u7a0b\u5e8f\u5f88\u96be\u3002 3\u3001\u975e\u5e38\u80fd\u591f\u4f53\u73b0\"hardware and software-Andy and Bill's law\" 4\u3001\u8fd9\u7bc7\u6587\u7ae0\u975e\u5e38\u5730\u9ad8\u77bb\u8fdc\u77a9\uff0c\u5199\u4e8e 2005 \u5e74\uff0c\u65f6\u81f3\u4eca\u65e5\uff0c\u4f9d\u7136\u6709\u6548\u3002","title":"gotw The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#_1","text":"By Herb Sutter The biggest sea change in software development since the OO revolution is knocking at the door, and its name is Concurrency . This article appeared in * Dr. Dobb's Journal , 30(3), March 2005**. A much briefer version under the title \"The Concurrency Revolution\" appeared in C/C++ Users Journal , 23(2), February 2005 .* Update note: The CPU trends graph last updated August 2009 to include current data and show the trend continues as predicted. The rest of this article including all text is still original as first posted here in December 2004. Your free lunch will soon be over. What can you do about it? What are you doing about it? The major processor manufacturers and architectures, from Intel and AMD to Sparc and PowerPC, have run out of room with most of their traditional approaches to boosting CPU performance. Instead of driving(\u9a71\u52a8) clock speeds and straight-line instruction throughput ever higher, they are instead turning en masse (\u4e00\u540c\u5730) to hyperthreading(\u8d85\u7ebf\u7a0b) and multicore architectures. Both of these features are already available on chips today; in particular, multicore is available on current PowerPC and Sparc IV processors, and is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004 In-Stat/MDR Fall Processor Forum was multicore devices, as many companies showed new or updated multicore processors. Looking back, it\u2019s not much of a stretch to call 2004 the year of multicore. And that puts us at a fundamental turning point in software development, at least for the next few years and for applications targeting general-purpose desktop computers and low-end servers (which happens to account for the vast bulk of the dollar value of software sold today). In this article, I\u2019ll describe the changing face of hardware, why it suddenly does matter to software, and how specifically the concurrency revolution matters to you and is going to change the way you will likely be writing software in the future. NOTE: \u9ad8\u77bb\u8fdc\u77a9 Arguably, the free lunch has already been over for a year or two, only we\u2019re just now noticing.","title":"\u5e8f\u8a00"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#the#free#performance#lunch","text":"There\u2019s an interesting phenomenon that\u2019s known as \u201cAndy giveth, and Bill taketh away.\u201d No matter how fast processors get, software consistently finds new ways to eat up the extra speed. Make a CPU ten times as fast, and software will usually find ten times as much to do (or, in some cases, will feel at liberty to do it ten times less efficiently). Most classes of applications have enjoyed free and regular performance gains for several decades, even without releasing new versions or doing anything special, because the CPU manufacturers (primarily) and memory and disk manufacturers (secondarily) have reliably enabled ever-newer and ever-faster mainstream systems. Clock speed isn\u2019t the only measure of performance, or even necessarily a good one, but it\u2019s an instructive one: We\u2019re used to seeing 500MHz CPUs give way to 1GHz CPUs give way to 2GHz CPUs, and so on. Today we\u2019re in the 3GHz range on mainstream computers.","title":"The Free Performance Lunch"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#obstacles#and#why#you#dont#have#10ghz#today","text":"","title":"Obstacles, and Why You Don\u2019t Have 10GHz Today"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#tanstaafl#moores#law#and#the#next#generations","text":"","title":"TANSTAAFL: Moore\u2019s Law and the Next Generation(s)"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#what#this#means#for#software#the#next#revolution","text":"Concurrency is the next major revolution in how we write software In the 1990s, we learned to grok objects. The revolution in mainstream software development from structured programming to object-oriented programming was the greatest such change in the past 20 years, and arguably in the past 30 years. There have been other changes, including the most recent (and genuinely interesting) naissance of web services, but nothing that most of us have seen during our careers has been as fundamental and as far-reaching a change in the way we write software as the object revolution. Until now. Starting today, the performance lunch isn\u2019t free any more. Sure, there will continue to be generally applicable performance gains that everyone can pick up, thanks mainly to cache size improvements. But if you want your application to benefit from the continued exponential throughput advances in new processors, it will need to be a well-written concurrent (usually multithreaded) application. And that\u2019s easier said than done, because not all problems are inherently parallelizable and because concurrent programming is hard. I can hear the howls of protest: \u201cConcurrency? That\u2019s not news! People are already writing concurrent applications.\u201d That\u2019s true. Of a small fraction of developers. Remember that people have been doing object-oriented programming since at least the days of Simula in the late 1960s. But OO didn\u2019t become a revolution, and dominant in the mainstream, until the 1990s. Why then? The reason the revolution happened was primarily that our industry was driven by requirements to write larger and larger systems that solved larger and larger problems and exploited the greater and greater CPU and storage resources that were becoming available. OOP\u2019s strengths in abstraction and dependency management made it a necessity for achieving large-scale software development that is economical, reliable, and repeatable. Similarly, we\u2019ve been doing concurrent programming since those same dark ages, writing coroutines and monitors and similar jazzy stuff. And for the past decade or so we\u2019ve witnessed incrementally more and more programmers writing concurrent (multi-threaded, multi-process) systems. But an actual revolution marked by a major turning point toward concurrency has been slow to materialize. Today the vast majority of applications are single-threaded, and for good reasons that I\u2019ll summarize in the next section. By the way, on the matter of hype: People have always been quick to announce \u201cthe next software development revolution,\u201d usually about their own brand-new technology. Don\u2019t believe it. New technologies are often genuinely interesting and sometimes beneficial, but the biggest revolutions in the way we write software generally come from technologies that have already been around for some years and have already experienced gradual growth before they transition to explosive growth. This is necessary: You can only base a software development revolution on a technology that\u2019s mature enough to build on (including having solid vendor and tool support), and it generally takes any new software technology at least seven years before it\u2019s solid enough to be broadly usable without performance cliffs and other gotchas. As a result, true software development revolutions like OO happen around technologies that have already been undergoing refinement for years, often decades. Even in Hollywood, most genuine \u201covernight successes\u201d have really been performing for many years before their big break. Concurrency is the next major revolution in how we write software. Different experts still have different opinions on whether it will be bigger than OO, but that kind of conversation is best left to pundits. For technologists, the interesting thing is that concurrency is of the same order as OO both in the (expected) scale of the revolution and in the complexity and learning curve of the technology.","title":"What This Means For Software: The Next Revolution"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#what#it#means#for#us","text":"Applications will increasingly need to be concurrent if they want to fully exploit continuing exponential CPU throughput gains Efficiency and performance optimization will get more, not less, important","title":"What It Means For Us"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/gotw-The-Free-Lunch-Is-Over-A-Fundamental-Turn-Toward-Concurrency-in-Software/#conclusion","text":"","title":"Conclusion"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/theregister-The-New-C%2B%2BLay-down-your-guns-knives-and-clubs/","text":"The New C++: Lay down your guns, knives, and clubs","title":"Introduction"},{"location":"Modern-CPU/Tendency-toward-parallel-computing/theregister-The-New-C%2B%2BLay-down-your-guns-knives-and-clubs/#the#new#c#lay#down#your#guns#knives#and#clubs","text":"","title":"The New C++: Lay down your guns, knives, and clubs"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/","text":"CPU\u6d41\u6c34\u7ebf\u3001\u591a\u53d1\u5c04\u3001\u8d85\u6807\u91cf\u3001CPU\u5fae\u7801 \u8fd9\u4e9b\u672f\u8bed\uff0c\u65f6\u957f\u78b0\u5230\uff0c\u6709\u5fc5\u8981\u68b3\u7406\u4e00\u4e0b\u3002 zhihu \u5173\u4e8eCPU\u6d41\u6c34\u7ebf \u591a\u53d1\u5c04 \u8d85\u6807\u91cf CPU\u5fae\u7801 \u4e4b\u95f4 \u7684\u5173\u7cfb\u548c\u539f\u7406? A \u4f5c\u8005\uff1aSinaean Dean \u94fe\u63a5\uff1a https://www.zhihu.com/question/66374524/answer/243527000 \u6765\u6e90\uff1a\u77e5\u4e4e \u8457\u4f5c\u6743\u5f52\u4f5c\u8005\u6240\u6709\u3002\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u8054\u7cfb\u4f5c\u8005\u83b7\u5f97\u6388\u6743\uff0c\u975e\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904\u3002 \u6d41\u6c34\u7ebf\u662f\u6307\u4e00\u6761\u6307\u4ee4\u7684\u6267\u884c\u88ab\u5207\u5206\u6210\u591a\u4e2a\u9636\u6bb5\uff0c\u4ea4\u7531\u4e0d\u540c\u7684\u6709\u72ec\u7acb\u529f\u80fd\u7684\u903b\u8f91\u90e8\u4ef6\u53bb\u4f9d\u6b21\u6267\u884c\u3002\u5c31\u597d\u6bd4\u4e00\u4e2a\u4eba\u4e5f\u53ef\u4ee5\u505a\u4e00\u9053\u83dc\uff0c\u4f60\u4e5f\u53ef\u4ee5\u628a\u505a\u83dc\u5206\u4e3a\u6d17\u83dc\u3001\u5207\u83dc\u3001\u7092\u83dc\u4e09\u9053\u5de5\u5e8f\uff0c\u4ea4\u7ed9\u4e09\u4e2a\u4eba\u4f9d\u6b21\u6267\u884c\u3002 \u591a\u53d1\u5c04\u5c31\u662f\u8bf4\u4f60\u6709\u591a\u6761\u6d41\u6c34\u7ebf\uff0c\u8fd9\u6837\u4f60\u539f\u6765\u53ef\u4ee5\u4ea4\u7ed9CPU\u4e00\u6761\u6307\u4ee4\uff0c\u73b0\u5728\u53ef\u4ee5\u540c\u65f6\u4ea4\u7ed9\u4ed6\u4e24\u6761\u6216\u662f\u4e09\u6761\u6307\u4ee4\u3002 \u8d85\u6807\u91cf\u662f\u8bf4\u4f60\u7684CPU\u6838\u5fc3\u4e2d\u6709\u4e24\u4e2a\u6216\u66f4\u591a\u7684ALU\uff0c\u751a\u81f3FPU\u7528\u6765\u505a\u8ba1\u7b97\u3002\u5728\u6ca1\u6709\u591a\u53d1\u5c04\u7684\u60c5\u51b5\u4e0b\u8d85\u6807\u91cf\u4e5f\u662f\u53ef\u4ee5\u7528\u7684\uff0c\u6bd4\u5982\u8bf4\u4e00\u4e2aALU\u5728\u6267\u884c\u4e00\u4e2a\u591a\u65f6\u949f\u5468\u671f\u7684\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u628a\u4e0b\u4e00\u6761\u6307\u4ee4\u4ea4\u7ed9\u4ee4\u4e00\u4e2aALU\u3002\u8fd9\u5c31\u597d\u6bd4\u4f60\u6709\u4f60\u7684\u83dc\u6bcf\u6837\u83dc\u7092\u51fa\u6765\u6240\u9700\u65f6\u95f4\u4e0d\u4e00\u6837\uff0c\u6709\u7684\u5757\u6709\u7684\u6162\uff0c\u4f46\u6d17\u83dc\u548c\u5207\u83dc\u7684\u901f\u5ea6\u662f\u4e00\u6837\u7684\uff0c\u90a3\u6709\u65f6\u53ef\u80fd\u914d\u4e24\u4e2a\u7092\u83dc\u7684\u5e08\u5085\u66f4\u597d\u4e00\u4e0b\u3002 nyu.edu Lecture 17: Multiple Issue: SIMD, EPIC, and superscalar \u5206\u522b\u4ecb\u7ecd wikipedia Superscalar processor A superscalar processor is a CPU that implements a form of parallelism called instruction-level parallelism within a single processor. NOTE: \u5355\u4e2a processor \u5185\u7684 \u7684\u5e76\u884c wikipedia Instruction pipelining In computer science , instruction pipelining is a technique for implementing instruction-level parallelism within a single processor. Pipelining attempts to keep every part of the processor busy with some instruction by dividing incoming instructions into a series of sequential steps (the eponymous \" pipeline \") performed by different processor units with different parts of instructions processed in parallel. NOTE: 1\u3001\u8fd9\u4e00\u6bb5\u8bfb\u5b8c\u540e\uff0c\u5c31\u89c9\u5f97\u5b83\u548c\u524d\u9762\u7684 Superscalar processor \u8bf4\u7684\u662f\u540c\u4e00\u4e2a\u610f\u601d wikipedia Wide-issue A wide-issue architecture is a computer processor that issues more than one instruction per clock cycle . NOTE: 1\u3001\u4e00\u4e2a\u65f6\u949f\u5468\u671f\u5185\u53d1\u5c04\u591a\u6761\u6307\u4ee4 2\u3001\u8fd9\u5e94\u8be5\u662fmultiple issue umd.edu Multiple Issue Processors I NOTE: 1\u3001\u4ecb\u7ecd\u5730\u6bd4\u8f83\u8be6\u7ec6 Types of Multiple Issue Processors: There are basically two variations in multiple issue processors \u2013 Superscalar processors and VLIW (Very Long Instruction Word) processors. There are two types of superscalar processors that issue varying numbers of instructions per clock. They are statically scheduled superscalars that use in-order execution dynamically scheduled superscalars that use out-of-order execution Superscalar vs Instruction pipelining \u5728 Superscalar processor \u4e2d\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd: While a superscalar CPU is typically also pipelined , superscalar and pipelining execution are considered different performance enhancement techniques. The former executes multiple instructions in parallel by using multiple execution units , whereas the latter executes multiple instructions in the same execution unit in parallel by dividing the execution unit into different phases. 1\u3001Instruction pipelining \u662f \u4f7f\u7528\u591a\u4e2a \" execution units \" 2\u3001Superscalar \u662f \u6bcf\u4e2a \" execution units \" \u5185\u90e8\u5c06instruction\u5206\u4e3a\u591a\u4e2a\u9636\u6bb5\uff0c\u7136\u540e\u5e76\u884c\u6267\u884c \u5173\u4e8e\u6b64\uff0c\u5728 zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A \u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u603b\u7ed3: \u7b80\u5355\u6765\u8bf4\uff0c\u4f60\u53ef\u4ee5\u7406\u89e3\u5f53\u4ee3CPU\u4e0d\u4ec5\u662f\u591a\u6838\u5fc3\uff0c\u800c\u4e14\u6bcf\u4e2a\u6838\u5fc3\u8fd8\u662f\u591a\u4efb\u52a1\uff08\u591a\u6307\u4ee4\uff09\u5e76\u884c\u7684\u3002\u8ba1\u7b97\u673a\u8bfe\u672c\u4e0a\u7684\u90a3\u79cd\u4e00\u4e2a\u65f6\u949f\u4e00\u6761\u6307\u4ee4\u7684\uff0c\u65e9\u5c31\u662f\u8001\u9ec4\u5386\u4e86 \uff08\u5f53\u7136\uff0c\u5b8f\u89c2\u6765\u770b\u57fa\u672c\u539f\u7406\u5e76\u6ca1\u6709\u6539\u53d8\uff09","title":"Introduction"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#cpucpu","text":"\u8fd9\u4e9b\u672f\u8bed\uff0c\u65f6\u957f\u78b0\u5230\uff0c\u6709\u5fc5\u8981\u68b3\u7406\u4e00\u4e0b\u3002","title":"CPU\u6d41\u6c34\u7ebf\u3001\u591a\u53d1\u5c04\u3001\u8d85\u6807\u91cf\u3001CPU\u5fae\u7801"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#zhihu#cpu#cpu","text":"","title":"zhihu \u5173\u4e8eCPU\u6d41\u6c34\u7ebf \u591a\u53d1\u5c04 \u8d85\u6807\u91cf CPU\u5fae\u7801 \u4e4b\u95f4 \u7684\u5173\u7cfb\u548c\u539f\u7406?"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#a","text":"\u4f5c\u8005\uff1aSinaean Dean \u94fe\u63a5\uff1a https://www.zhihu.com/question/66374524/answer/243527000 \u6765\u6e90\uff1a\u77e5\u4e4e \u8457\u4f5c\u6743\u5f52\u4f5c\u8005\u6240\u6709\u3002\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u8054\u7cfb\u4f5c\u8005\u83b7\u5f97\u6388\u6743\uff0c\u975e\u5546\u4e1a\u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904\u3002 \u6d41\u6c34\u7ebf\u662f\u6307\u4e00\u6761\u6307\u4ee4\u7684\u6267\u884c\u88ab\u5207\u5206\u6210\u591a\u4e2a\u9636\u6bb5\uff0c\u4ea4\u7531\u4e0d\u540c\u7684\u6709\u72ec\u7acb\u529f\u80fd\u7684\u903b\u8f91\u90e8\u4ef6\u53bb\u4f9d\u6b21\u6267\u884c\u3002\u5c31\u597d\u6bd4\u4e00\u4e2a\u4eba\u4e5f\u53ef\u4ee5\u505a\u4e00\u9053\u83dc\uff0c\u4f60\u4e5f\u53ef\u4ee5\u628a\u505a\u83dc\u5206\u4e3a\u6d17\u83dc\u3001\u5207\u83dc\u3001\u7092\u83dc\u4e09\u9053\u5de5\u5e8f\uff0c\u4ea4\u7ed9\u4e09\u4e2a\u4eba\u4f9d\u6b21\u6267\u884c\u3002 \u591a\u53d1\u5c04\u5c31\u662f\u8bf4\u4f60\u6709\u591a\u6761\u6d41\u6c34\u7ebf\uff0c\u8fd9\u6837\u4f60\u539f\u6765\u53ef\u4ee5\u4ea4\u7ed9CPU\u4e00\u6761\u6307\u4ee4\uff0c\u73b0\u5728\u53ef\u4ee5\u540c\u65f6\u4ea4\u7ed9\u4ed6\u4e24\u6761\u6216\u662f\u4e09\u6761\u6307\u4ee4\u3002 \u8d85\u6807\u91cf\u662f\u8bf4\u4f60\u7684CPU\u6838\u5fc3\u4e2d\u6709\u4e24\u4e2a\u6216\u66f4\u591a\u7684ALU\uff0c\u751a\u81f3FPU\u7528\u6765\u505a\u8ba1\u7b97\u3002\u5728\u6ca1\u6709\u591a\u53d1\u5c04\u7684\u60c5\u51b5\u4e0b\u8d85\u6807\u91cf\u4e5f\u662f\u53ef\u4ee5\u7528\u7684\uff0c\u6bd4\u5982\u8bf4\u4e00\u4e2aALU\u5728\u6267\u884c\u4e00\u4e2a\u591a\u65f6\u949f\u5468\u671f\u7684\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u628a\u4e0b\u4e00\u6761\u6307\u4ee4\u4ea4\u7ed9\u4ee4\u4e00\u4e2aALU\u3002\u8fd9\u5c31\u597d\u6bd4\u4f60\u6709\u4f60\u7684\u83dc\u6bcf\u6837\u83dc\u7092\u51fa\u6765\u6240\u9700\u65f6\u95f4\u4e0d\u4e00\u6837\uff0c\u6709\u7684\u5757\u6709\u7684\u6162\uff0c\u4f46\u6d17\u83dc\u548c\u5207\u83dc\u7684\u901f\u5ea6\u662f\u4e00\u6837\u7684\uff0c\u90a3\u6709\u65f6\u53ef\u80fd\u914d\u4e24\u4e2a\u7092\u83dc\u7684\u5e08\u5085\u66f4\u597d\u4e00\u4e0b\u3002","title":"A"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#nyuedu#lecture#17#multiple#issue#simd#epic#and#superscalar","text":"","title":"nyu.edu Lecture 17: Multiple Issue: SIMD, EPIC, and superscalar"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#_1","text":"","title":"\u5206\u522b\u4ecb\u7ecd"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#wikipedia#superscalar#processor","text":"A superscalar processor is a CPU that implements a form of parallelism called instruction-level parallelism within a single processor. NOTE: \u5355\u4e2a processor \u5185\u7684 \u7684\u5e76\u884c","title":"wikipedia Superscalar processor"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#wikipedia#instruction#pipelining","text":"In computer science , instruction pipelining is a technique for implementing instruction-level parallelism within a single processor. Pipelining attempts to keep every part of the processor busy with some instruction by dividing incoming instructions into a series of sequential steps (the eponymous \" pipeline \") performed by different processor units with different parts of instructions processed in parallel. NOTE: 1\u3001\u8fd9\u4e00\u6bb5\u8bfb\u5b8c\u540e\uff0c\u5c31\u89c9\u5f97\u5b83\u548c\u524d\u9762\u7684 Superscalar processor \u8bf4\u7684\u662f\u540c\u4e00\u4e2a\u610f\u601d","title":"wikipedia Instruction pipelining"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#wikipedia#wide-issue","text":"A wide-issue architecture is a computer processor that issues more than one instruction per clock cycle . NOTE: 1\u3001\u4e00\u4e2a\u65f6\u949f\u5468\u671f\u5185\u53d1\u5c04\u591a\u6761\u6307\u4ee4 2\u3001\u8fd9\u5e94\u8be5\u662fmultiple issue","title":"wikipedia Wide-issue"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#umdedu#multiple#issue#processors#i","text":"NOTE: 1\u3001\u4ecb\u7ecd\u5730\u6bd4\u8f83\u8be6\u7ec6 Types of Multiple Issue Processors: There are basically two variations in multiple issue processors \u2013 Superscalar processors and VLIW (Very Long Instruction Word) processors. There are two types of superscalar processors that issue varying numbers of instructions per clock. They are statically scheduled superscalars that use in-order execution dynamically scheduled superscalars that use out-of-order execution","title":"umd.edu Multiple Issue Processors I"},{"location":"Modern-CPU/%E6%B5%81%E6%B0%B4%E7%BA%BF-%E8%B6%85%E6%A0%87%E9%87%8F-%E5%BE%AE%E7%A0%81-%E5%A4%9A%E5%8F%91%E5%B0%84%E7%AE%80%E4%BB%8B/#superscalar#vs#instruction#pipelining","text":"\u5728 Superscalar processor \u4e2d\u6709\u8fd9\u6837\u7684\u4ecb\u7ecd: While a superscalar CPU is typically also pipelined , superscalar and pipelining execution are considered different performance enhancement techniques. The former executes multiple instructions in parallel by using multiple execution units , whereas the latter executes multiple instructions in the same execution unit in parallel by dividing the execution unit into different phases. 1\u3001Instruction pipelining \u662f \u4f7f\u7528\u591a\u4e2a \" execution units \" 2\u3001Superscalar \u662f \u6bcf\u4e2a \" execution units \" \u5185\u90e8\u5c06instruction\u5206\u4e3a\u591a\u4e2a\u9636\u6bb5\uff0c\u7136\u540e\u5e76\u884c\u6267\u884c \u5173\u4e8e\u6b64\uff0c\u5728 zhihu \u5982\u4f55\u7406\u89e3 C++11 \u7684\u516d\u79cd memory order\uff1f # A \u4e2d\uff0c\u6709\u8fd9\u6837\u7684\u603b\u7ed3: \u7b80\u5355\u6765\u8bf4\uff0c\u4f60\u53ef\u4ee5\u7406\u89e3\u5f53\u4ee3CPU\u4e0d\u4ec5\u662f\u591a\u6838\u5fc3\uff0c\u800c\u4e14\u6bcf\u4e2a\u6838\u5fc3\u8fd8\u662f\u591a\u4efb\u52a1\uff08\u591a\u6307\u4ee4\uff09\u5e76\u884c\u7684\u3002\u8ba1\u7b97\u673a\u8bfe\u672c\u4e0a\u7684\u90a3\u79cd\u4e00\u4e2a\u65f6\u949f\u4e00\u6761\u6307\u4ee4\u7684\uff0c\u65e9\u5c31\u662f\u8001\u9ec4\u5386\u4e86 \uff08\u5f53\u7136\uff0c\u5b8f\u89c2\u6765\u770b\u57fa\u672c\u539f\u7406\u5e76\u6ca1\u6709\u6539\u53d8\uff09","title":"Superscalar vs Instruction pipelining"},{"location":"TODO/x-86/","text":"x-86 register and their application \u719f\u6089x-86\u7684register\u548c\u5b83\u4eec\u7684application\uff0c\u5bf9\u4e8e\u7406\u89e3\u5f88\u591a\u95ee\u9898\u90fd\u975e\u5e38\u6709\u76ca\uff0c\u89e6\u53d1\u6211\u4ea7\u751f\u8fd9\u4e2a\u60f3\u6cd5\u7684\u662f\u5728\u9605\u8bfbgdb 10.13 Registers \u7ae0\u8282\u65f6\uff0c\u5176\u4e2d\u63cf\u8ff0\u4e86register\u7684\u4e00\u4e9b\u77e5\u8bc6\uff0c\u521a\u5f00\u59cb\u6211\u5e76\u4e0d\u6e05\u695a\u3002 stack register wikipedia Stack register frame pointer stackoverflow What is the purpose of the EBP frame pointer register? stackoverflow why to use ebp in function prologue/epilogue? stackoverflow What is the purpose of the EBP frame pointer register? stackoverflow What is exactly the base pointer and stack pointer? To what do they point? stackoverflow Doesn't the frame pointer make the stack pointer redundant? stackoverflow What are the ESP and the EBP registers? embeddedrelated Difference Between Stack Pointer and Frame Pointer i-harness c++ - register - stack pointer vs frame pointer Intel x86 Function-call Conventions call conversion\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9 Steve Friedl's Unixwiz.net Tech Tips Intel x86 Function-call Conventions - Assembly View wikipedia Calling convention osdev Calling Conventions stackoverflow Behind the scenes of returning value from function c++ wikibooks X86 Assembly x86_64 Assembly Cheat Sheet https://www.cs.uaf.edu/2017/fall/cs301/reference/x86_64.html x86 Assembly Guide https://www.cs.virginia.edu/~evans/cs216/guides/x86.html https://wiki.skullsecurity.org/Registers https://wiki.skullsecurity.org/index.php?title=Fundamentals","title":"x-86"},{"location":"TODO/x-86/#x-86","text":"","title":"x-86"},{"location":"TODO/x-86/#register#and#their#application","text":"\u719f\u6089x-86\u7684register\u548c\u5b83\u4eec\u7684application\uff0c\u5bf9\u4e8e\u7406\u89e3\u5f88\u591a\u95ee\u9898\u90fd\u975e\u5e38\u6709\u76ca\uff0c\u89e6\u53d1\u6211\u4ea7\u751f\u8fd9\u4e2a\u60f3\u6cd5\u7684\u662f\u5728\u9605\u8bfbgdb 10.13 Registers \u7ae0\u8282\u65f6\uff0c\u5176\u4e2d\u63cf\u8ff0\u4e86register\u7684\u4e00\u4e9b\u77e5\u8bc6\uff0c\u521a\u5f00\u59cb\u6211\u5e76\u4e0d\u6e05\u695a\u3002","title":"register and their application"},{"location":"TODO/x-86/#stack#register","text":"wikipedia Stack register","title":"stack register"},{"location":"TODO/x-86/#frame#pointer","text":"stackoverflow What is the purpose of the EBP frame pointer register? stackoverflow why to use ebp in function prologue/epilogue? stackoverflow What is the purpose of the EBP frame pointer register? stackoverflow What is exactly the base pointer and stack pointer? To what do they point? stackoverflow Doesn't the frame pointer make the stack pointer redundant? stackoverflow What are the ESP and the EBP registers? embeddedrelated Difference Between Stack Pointer and Frame Pointer i-harness c++ - register - stack pointer vs frame pointer","title":"frame pointer"},{"location":"TODO/x-86/#intel#x86#function-call#conventions","text":"call conversion\u662f\u975e\u5e38\u91cd\u8981\u7684\u4e00\u4e2a\u5185\u5bb9 Steve Friedl's Unixwiz.net Tech Tips Intel x86 Function-call Conventions - Assembly View wikipedia Calling convention osdev Calling Conventions stackoverflow Behind the scenes of returning value from function c++","title":"Intel x86 Function-call Conventions"},{"location":"TODO/x-86/#wikibooks#x86#assembly","text":"","title":"wikibooks X86 Assembly"},{"location":"TODO/x-86/#x86_64#assembly#cheat#sheet","text":"https://www.cs.uaf.edu/2017/fall/cs301/reference/x86_64.html","title":"x86_64 Assembly Cheat Sheet"},{"location":"TODO/x-86/#x86#assembly#guide","text":"https://www.cs.virginia.edu/~evans/cs216/guides/x86.html https://wiki.skullsecurity.org/Registers https://wiki.skullsecurity.org/index.php?title=Fundamentals","title":"x86 Assembly Guide"}]}